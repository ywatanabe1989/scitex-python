{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated MCP Server Workflow\n",
    "\n",
    "This notebook demonstrates how multiple SciTeX MCP servers work together in a complete research workflow.\n",
    "\n",
    "## Scenario: Neuroscience Data Analysis Pipeline\n",
    "\n",
    "We'll analyze EEG data from a cognitive experiment:\n",
    "1. **IO Translator**: Convert legacy analysis code to SciTeX\n",
    "2. **Config Server**: Extract and manage experimental parameters\n",
    "3. **DSP Server**: Process neural signals\n",
    "4. **Stats Server**: Perform statistical analysis\n",
    "5. **PLT Server**: Create publication figures\n",
    "6. **PD Server**: Manage and export results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Legacy Code Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original legacy analysis script\n",
    "legacy_code = \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, stats\n",
    "import mne\n",
    "\n",
    "# Load EEG data\n",
    "subjects = ['S01', 'S02', 'S03', 'S04', 'S05']\n",
    "results = []\n",
    "\n",
    "for subj in subjects:\n",
    "    # Load raw data\n",
    "    raw = mne.io.read_raw_fif(f'/data/eeg/{subj}_raw.fif', preload=True)\n",
    "    events = mne.read_events(f'/data/eeg/{subj}_events.eve')\n",
    "    \n",
    "    # Preprocessing\n",
    "    raw.filter(1, 40)  # Bandpass filter\n",
    "    raw.notch_filter(50)  # Remove line noise\n",
    "    \n",
    "    # Epoch data\n",
    "    epochs = mne.Epochs(raw, events, event_id={'target': 1, 'standard': 2},\n",
    "                       tmin=-0.2, tmax=0.8, baseline=(-0.2, 0))\n",
    "    \n",
    "    # Compute ERPs\n",
    "    erp_target = epochs['target'].average()\n",
    "    erp_standard = epochs['standard'].average()\n",
    "    \n",
    "    # Extract P300 amplitude\n",
    "    p300_window = (0.3, 0.5)\n",
    "    p300_amp = erp_target.data[:, int(p300_window[0]*1000):int(p300_window[1]*1000)].max()\n",
    "    \n",
    "    results.append({\n",
    "        'subject': subj,\n",
    "        'p300_amplitude': p300_amp,\n",
    "        'n_trials': len(epochs)\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('/results/p300_analysis.csv', index=False)\n",
    "\n",
    "# Plot grand average\n",
    "plt.figure(figsize=(10, 6))\n",
    "# ... plotting code ...\n",
    "plt.savefig('/figures/grand_average_erp.png')\n",
    "\"\"\"\n",
    "\n",
    "print(\"LEGACY NEUROSCIENCE ANALYSIS CODE\")\n",
    "print(\"=\" * 50)\n",
    "print(legacy_code[:800] + \"\\n... [truncated]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Use IO Translator to convert to SciTeX\n",
    "print(\"STEP 1: TRANSLATING TO SCITEX\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# In practice, this would be called through MCP:\n",
    "# result = mcp.call_tool(\"scitex-io-translator\", \"translate_to_scitex\", {\n",
    "#     \"source_code\": legacy_code,\n",
    "#     \"target_modules\": [\"io\", \"dsp\", \"plt\", \"pd\"],\n",
    "#     \"add_config_support\": True\n",
    "# })\n",
    "\n",
    "print(\"✓ Code translated to SciTeX format\")\n",
    "print(\"✓ Paths converted to relative\")\n",
    "print(\"✓ Configuration extracted to CONFIG/\")\n",
    "print(\"✓ Added reproducibility features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated SciTeX code with config management\n",
    "scitex_code = '''\n",
    "import scitex as stx\n",
    "\n",
    "def main(CONFIG):\n",
    "    \"\"\"EEG P300 analysis pipeline.\"\"\"\n",
    "    # Initialize experiment\n",
    "    exp = stx.config.load_experiment(\n",
    "        config_dir='./CONFIG',\n",
    "        validate=True,\n",
    "        schema='neuroscience'\n",
    "    )\n",
    "    \n",
    "    # Process all subjects\n",
    "    results = []\n",
    "    \n",
    "    for subj_id in CONFIG.subjects:\n",
    "        # Load data with automatic format detection\n",
    "        raw_data = stx.io.load(CONFIG.paths.raw_data_pattern.format(subj_id))\n",
    "        events = stx.io.load(CONFIG.paths.events_pattern.format(subj_id))\n",
    "        \n",
    "        # Apply preprocessing pipeline\n",
    "        processed = stx.dsp.preprocess_eeg(\n",
    "            raw_data,\n",
    "            sampling_rate=CONFIG.eeg.sampling_rate,\n",
    "            filters=CONFIG.eeg.preprocessing.filters,\n",
    "            reference=CONFIG.eeg.preprocessing.reference,\n",
    "            bad_channels='auto',\n",
    "            artifact_rejection=CONFIG.eeg.preprocessing.artifact_rejection\n",
    "        )\n",
    "        \n",
    "        # Extract epochs\n",
    "        epochs = stx.dsp.create_epochs(\n",
    "            processed,\n",
    "            events,\n",
    "            event_mapping=CONFIG.experiment.event_codes,\n",
    "            time_window=CONFIG.analysis.epoch_window,\n",
    "            baseline=CONFIG.analysis.baseline,\n",
    "            reject_criteria=CONFIG.eeg.rejection_thresholds\n",
    "        )\n",
    "        \n",
    "        # Compute ERPs and extract features\n",
    "        erp_results = stx.dsp.compute_erp_features(\n",
    "            epochs,\n",
    "            components=CONFIG.analysis.erp_components,\n",
    "            methods=CONFIG.analysis.extraction_methods\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'subject_id': subj_id,\n",
    "            **erp_results,\n",
    "            'n_trials': len(epochs),\n",
    "            'quality_metrics': stx.dsp.assess_data_quality(epochs)\n",
    "        })\n",
    "    \n",
    "    # Create results dataframe\n",
    "    df_results = stx.pd.force_df(results)\n",
    "    \n",
    "    return df_results\n",
    "'''\n",
    "\n",
    "print(\"SCITEX CODE WITH CONFIGURATION MANAGEMENT\")\n",
    "print(\"=\" * 50)\n",
    "print(scitex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated configuration files\n",
    "config_files = {\n",
    "    \"CONFIG/experiment.yaml\": \"\"\"\n",
    "experiment:\n",
    "  name: \"P300 Oddball Paradigm\"\n",
    "  type: \"event-related-potential\"\n",
    "  date: \"2024-01-15\"\n",
    "  \n",
    "  event_codes:\n",
    "    target: 1\n",
    "    standard: 2\n",
    "    response: 3\n",
    "    \n",
    "  paradigm:\n",
    "    n_blocks: 4\n",
    "    trials_per_block: 100\n",
    "    target_probability: 0.2\n",
    "    isi: [1000, 1500]  # Inter-stimulus interval range (ms)\n",
    "\"\"\",\n",
    "    \"CONFIG/eeg.yaml\": \"\"\"\n",
    "eeg:\n",
    "  sampling_rate: 1000\n",
    "  n_channels: 64\n",
    "  reference: \"average\"\n",
    "  \n",
    "  preprocessing:\n",
    "    filters:\n",
    "      highpass: 0.1\n",
    "      lowpass: 40\n",
    "      notch: [50, 100]  # Line noise and harmonics\n",
    "    \n",
    "    artifact_rejection:\n",
    "      method: \"ica\"\n",
    "      n_components: 20\n",
    "      eog_channels: [\"EOG1\", \"EOG2\"]\n",
    "    \n",
    "  rejection_thresholds:\n",
    "    peak_to_peak: 150  # microvolts\n",
    "    flat: 5            # microvolts\n",
    "    \n",
    "  channels_of_interest:\n",
    "    - \"Fz\"\n",
    "    - \"Cz\"\n",
    "    - \"Pz\"\n",
    "\"\"\",\n",
    "    \"CONFIG/analysis.yaml\": \"\"\"\n",
    "analysis:\n",
    "  epoch_window: [-0.2, 1.0]  # seconds\n",
    "  baseline: [-0.2, 0.0]\n",
    "  \n",
    "  erp_components:\n",
    "    N100:\n",
    "      window: [0.08, 0.12]\n",
    "      channels: [\"Fz\", \"Cz\"]\n",
    "      polarity: \"negative\"\n",
    "      \n",
    "    P300:\n",
    "      window: [0.25, 0.50]\n",
    "      channels: [\"Cz\", \"Pz\"]\n",
    "      polarity: \"positive\"\n",
    "      \n",
    "    N400:\n",
    "      window: [0.35, 0.45]\n",
    "      channels: [\"Cz\", \"Pz\"]\n",
    "      polarity: \"negative\"\n",
    "  \n",
    "  extraction_methods:\n",
    "    - \"peak_amplitude\"\n",
    "    - \"mean_amplitude\"\n",
    "    - \"peak_latency\"\n",
    "    - \"area_under_curve\"\n",
    "  \n",
    "  statistical:\n",
    "    alpha: 0.05\n",
    "    correction: \"fdr_bh\"\n",
    "    bootstrap_iterations: 1000\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "print(\"GENERATED CONFIGURATION FILES\")\n",
    "print(\"=\" * 50)\n",
    "for filename, content in config_files.items():\n",
    "    print(f\"\\n{filename}:\")\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Signal Processing with DSP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSP server processing pipeline\n",
    "dsp_pipeline = '''\n",
    "# Step 3: Advanced signal processing\n",
    "def process_eeg_signals(df_epochs, CONFIG):\n",
    "    \"\"\"Apply advanced DSP techniques to EEG data.\"\"\"\n",
    "    \n",
    "    # Time-frequency analysis\n",
    "    tf_results = stx.dsp.time_frequency_analysis(\n",
    "        df_epochs,\n",
    "        method='morlet',\n",
    "        frequencies=np.logspace(0, 1.7, 30),  # 1-50 Hz\n",
    "        n_cycles=7,\n",
    "        output='complex',  # For phase analysis\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Phase-amplitude coupling\n",
    "    pac_results = stx.dsp.compute_pac(\n",
    "        df_epochs,\n",
    "        phase_freqs=CONFIG.analysis.pac.phase_frequencies,\n",
    "        amp_freqs=CONFIG.analysis.pac.amplitude_frequencies,\n",
    "        method='tort',\n",
    "        n_surrogates=200\n",
    "    )\n",
    "    \n",
    "    # Connectivity analysis\n",
    "    connectivity = stx.dsp.compute_connectivity(\n",
    "        df_epochs,\n",
    "        method=['coherence', 'plv', 'wpli'],\n",
    "        frequencies=CONFIG.analysis.frequency_bands,\n",
    "        n_cycles=5,\n",
    "        time_resolved=True\n",
    "    )\n",
    "    \n",
    "    # Source localization (if MRI available)\n",
    "    if CONFIG.paths.get('mri_template'):\n",
    "        sources = stx.dsp.estimate_sources(\n",
    "            df_epochs,\n",
    "            forward_model=CONFIG.paths.forward_model,\n",
    "            method='mne',\n",
    "            regularization='auto'\n",
    "        )\n",
    "    \n",
    "    # Feature extraction for machine learning\n",
    "    ml_features = stx.dsp.extract_eeg_features(\n",
    "        df_epochs,\n",
    "        feature_sets=[\n",
    "            'spectral_power',\n",
    "            'hjorth_parameters',\n",
    "            'fractal_dimension',\n",
    "            'sample_entropy',\n",
    "            'wavelet_energy'\n",
    "        ],\n",
    "        standardize=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'time_frequency': tf_results,\n",
    "        'pac': pac_results,\n",
    "        'connectivity': connectivity,\n",
    "        'ml_features': ml_features\n",
    "    }\n",
    "'''\n",
    "\n",
    "print(\"DSP SERVER PROCESSING PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "print(dsp_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Statistical Analysis with Stats Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis pipeline\n",
    "stats_pipeline = '''\n",
    "# Step 4: Comprehensive statistical analysis\n",
    "def analyze_erp_statistics(df_results, dsp_results, CONFIG):\n",
    "    \"\"\"Perform statistical analysis of ERP data.\"\"\"\n",
    "    \n",
    "    # Between-group comparisons (if applicable)\n",
    "    if 'group' in df_results.columns:\n",
    "        group_stats = stx.stats.compare_groups(\n",
    "            df_results,\n",
    "            dependent_vars=['p300_amplitude', 'p300_latency'],\n",
    "            grouping_var='group',\n",
    "            covariates=['age', 'gender'],\n",
    "            tests=['ancova', 'permutation'],\n",
    "            n_permutations=5000,\n",
    "            effect_sizes=['cohens_d', 'eta_squared'],\n",
    "            confidence_level=0.95\n",
    "        )\n",
    "    \n",
    "    # Within-subject comparisons\n",
    "    condition_stats = stx.stats.repeated_measures_analysis(\n",
    "        data=df_results,\n",
    "        within_factors=['condition', 'time'],\n",
    "        dependent_var='amplitude',\n",
    "        subject_var='subject_id',\n",
    "        sphericity_correction='greenhouse-geisser',\n",
    "        post_hoc='bonferroni',\n",
    "        plot_interactions=True\n",
    "    )\n",
    "    \n",
    "    # Time-frequency statistics\n",
    "    tf_stats = stx.stats.cluster_permutation_test(\n",
    "        dsp_results['time_frequency'],\n",
    "        contrast='target-standard',\n",
    "        n_permutations=1000,\n",
    "        threshold='tfce',  # Threshold-free cluster enhancement\n",
    "        tail='two-sided',\n",
    "        adjacency='temporal-spectral',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Connectivity statistics\n",
    "    conn_stats = stx.stats.network_based_statistic(\n",
    "        dsp_results['connectivity'],\n",
    "        contrast='condition',\n",
    "        threshold=3.0,\n",
    "        n_permutations=5000,\n",
    "        method='nbs'\n",
    "    )\n",
    "    \n",
    "    # Machine learning classification\n",
    "    ml_results = stx.stats.classification_analysis(\n",
    "        features=dsp_results['ml_features'],\n",
    "        labels=df_results['condition'],\n",
    "        models=['svm', 'random_forest', 'lda'],\n",
    "        cv_strategy='stratified_kfold',\n",
    "        n_splits=5,\n",
    "        scoring=['accuracy', 'f1', 'roc_auc'],\n",
    "        feature_selection='mutual_info',\n",
    "        n_features=20,\n",
    "        permutation_test=True\n",
    "    )\n",
    "    \n",
    "    # Effect size and power analysis\n",
    "    power_analysis = stx.stats.compute_achieved_power(\n",
    "        data=df_results,\n",
    "        effect_sizes=group_stats['effect_sizes'],\n",
    "        alpha=CONFIG.analysis.statistical.alpha,\n",
    "        design='mixed',\n",
    "        include_sensitivity=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'group_comparisons': group_stats,\n",
    "        'repeated_measures': condition_stats,\n",
    "        'time_frequency_stats': tf_stats,\n",
    "        'connectivity_stats': conn_stats,\n",
    "        'classification': ml_results,\n",
    "        'power_analysis': power_analysis\n",
    "    }\n",
    "'''\n",
    "\n",
    "print(\"STATISTICAL ANALYSIS PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "print(stats_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualization with PLT Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication-ready visualization\n",
    "visualization_pipeline = '''\n",
    "# Step 5: Create publication figures\n",
    "def create_publication_figures(df_results, dsp_results, stats_results, CONFIG):\n",
    "    \"\"\"Generate publication-ready figures.\"\"\"\n",
    "    \n",
    "    # Set publication style\n",
    "    stx.plt.set_publication_style(\n",
    "        journal='neuroimage',\n",
    "        column_width='double',\n",
    "        color_palette='colorblind_safe'\n",
    "    )\n",
    "    \n",
    "    # Figure 1: Grand average ERPs\n",
    "    fig1 = stx.plt.create_erp_figure(\n",
    "        layout='multi_channel',\n",
    "        figsize='auto'\n",
    "    )\n",
    "    \n",
    "    fig1.plot_grand_average_erp(\n",
    "        df_results,\n",
    "        conditions=['target', 'standard'],\n",
    "        channels=CONFIG.eeg.channels_of_interest,\n",
    "        time_window=[-0.2, 0.8],\n",
    "        baseline=CONFIG.analysis.baseline,\n",
    "        show_sem=True,\n",
    "        show_topo_maps=[0.1, 0.3, 0.4],  # Time points for topographies\n",
    "        mark_components=CONFIG.analysis.erp_components,\n",
    "        add_difference_wave=True\n",
    "    )\n",
    "    \n",
    "    # Add statistical markers\n",
    "    fig1.add_statistical_markers(\n",
    "        stats_results['repeated_measures'],\n",
    "        method='cluster',\n",
    "        alpha=0.05\n",
    "    )\n",
    "    \n",
    "    # Figure 2: Time-frequency results\n",
    "    fig2 = stx.plt.create_tf_figure(\n",
    "        n_conditions=2,\n",
    "        n_channels=3,\n",
    "        figsize=(12, 8)\n",
    "    )\n",
    "    \n",
    "    fig2.plot_time_frequency_maps(\n",
    "        dsp_results['time_frequency'],\n",
    "        baseline=CONFIG.analysis.baseline,\n",
    "        vmin=-3, vmax=3,  # Z-scored power\n",
    "        cmap='RdBu_r',\n",
    "        contour=stats_results['time_frequency_stats']['clusters'],\n",
    "        colorbar_label='Power (z-score)'\n",
    "    )\n",
    "    \n",
    "    # Figure 3: Connectivity results\n",
    "    fig3 = stx.plt.create_connectivity_figure(\n",
    "        style='circular',\n",
    "        figsize=(10, 10)\n",
    "    )\n",
    "    \n",
    "    fig3.plot_connectivity_matrix(\n",
    "        dsp_results['connectivity']['wpli'],\n",
    "        frequency_band='alpha',\n",
    "        threshold='significant',\n",
    "        node_colors='lobe',\n",
    "        edge_cmap='viridis',\n",
    "        show_labels=True\n",
    "    )\n",
    "    \n",
    "    # Figure 4: Statistical summary\n",
    "    fig4 = stx.plt.create_results_figure(\n",
    "        layout='dashboard',\n",
    "        figsize=(15, 10)\n",
    "    )\n",
    "    \n",
    "    # Panel A: Component amplitudes\n",
    "    fig4.panels['A'].plot_component_comparison(\n",
    "        df_results,\n",
    "        components=['N100', 'P300'],\n",
    "        show_individual=True,\n",
    "        show_stats=True,\n",
    "        violin=True\n",
    "    )\n",
    "    \n",
    "    # Panel B: Classification results\n",
    "    fig4.panels['B'].plot_classification_results(\n",
    "        stats_results['classification'],\n",
    "        show_confusion_matrix=True,\n",
    "        show_roc_curves=True,\n",
    "        show_feature_importance=True\n",
    "    )\n",
    "    \n",
    "    # Panel C: Effect sizes\n",
    "    fig4.panels['C'].plot_effect_sizes(\n",
    "        stats_results['group_comparisons']['effect_sizes'],\n",
    "        show_ci=True,\n",
    "        reference_lines=[0.2, 0.5, 0.8],  # Small, medium, large\n",
    "        sort_by_magnitude=True\n",
    "    )\n",
    "    \n",
    "    # Panel D: Power analysis\n",
    "    fig4.panels['D'].plot_power_curves(\n",
    "        stats_results['power_analysis'],\n",
    "        show_achieved=True,\n",
    "        show_required_n=True\n",
    "    )\n",
    "    \n",
    "    # Save all figures\n",
    "    figures = {\n",
    "        'figure_1_erp': fig1,\n",
    "        'figure_2_timefreq': fig2,\n",
    "        'figure_3_connectivity': fig3,\n",
    "        'figure_4_summary': fig4\n",
    "    }\n",
    "    \n",
    "    for name, fig in figures.items():\n",
    "        stx.io.save(\n",
    "            fig,\n",
    "            f'./figures/{name}',\n",
    "            formats=['png', 'pdf', 'svg'],\n",
    "            dpi=300,\n",
    "            metadata={\n",
    "                'experiment': CONFIG.experiment.name,\n",
    "                'n_subjects': len(CONFIG.subjects),\n",
    "                'analysis_date': stx.gen.timestamp()\n",
    "            },\n",
    "            export_data=True,\n",
    "            symlink_from_cwd=True\n",
    "        )\n",
    "    \n",
    "    return figures\n",
    "'''\n",
    "\n",
    "print(\"VISUALIZATION PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "print(visualization_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Results Management with PD Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results management and export\n",
    "results_management = '''\n",
    "# Step 6: Organize and export results\n",
    "def manage_results(all_results, CONFIG):\n",
    "    \"\"\"Organize, validate, and export all results.\"\"\"\n",
    "    \n",
    "    # Combine all results into structured format\n",
    "    results_db = stx.pd.create_results_database(\n",
    "        erp_data=all_results['erp'],\n",
    "        dsp_results=all_results['dsp'],\n",
    "        stats_results=all_results['stats'],\n",
    "        metadata=CONFIG.experiment,\n",
    "        schema='neuroscience_erp'\n",
    "    )\n",
    "    \n",
    "    # Quality control\n",
    "    qc_report = stx.pd.quality_control(\n",
    "        results_db,\n",
    "        checks=[\n",
    "            'completeness',      # All subjects processed\n",
    "            'outlier_detection', # Statistical outliers\n",
    "            'consistency',       # Cross-measure consistency\n",
    "            'replication'        # Internal replication\n",
    "        ],\n",
    "        generate_report=True\n",
    "    )\n",
    "    \n",
    "    # Create results summary table\n",
    "    summary_table = stx.pd.create_summary_table(\n",
    "        results_db,\n",
    "        group_by=['condition', 'component'],\n",
    "        metrics=['mean', 'std', 'ci95', 'effect_size', 'p_value'],\n",
    "        format_numbers=True,\n",
    "        highlight_significant=True\n",
    "    )\n",
    "    \n",
    "    # Export to multiple formats\n",
    "    export_manager = stx.pd.ResultsExporter(\n",
    "        results_db,\n",
    "        output_dir='./results/final/'\n",
    "    )\n",
    "    \n",
    "    # Scientific data formats\n",
    "    export_manager.to_hdf5(\n",
    "        'complete_results.h5',\n",
    "        compression='gzip',\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    export_manager.to_mat(\n",
    "        'results_for_matlab.mat',\n",
    "        scipy_compatible=True\n",
    "    )\n",
    "    \n",
    "    # Publication formats\n",
    "    export_manager.to_excel(\n",
    "        'results_tables.xlsx',\n",
    "        sheets={\n",
    "            'Summary': summary_table,\n",
    "            'Individual_ERPs': results_db['erp_data'],\n",
    "            'Statistics': results_db['stats_summary'],\n",
    "            'QC_Report': qc_report\n",
    "        },\n",
    "        formatting='publication'\n",
    "    )\n",
    "    \n",
    "    # Generate manuscript tables\n",
    "    manuscript_tables = stx.pd.create_manuscript_tables(\n",
    "        summary_table,\n",
    "        table_format='apa',  # APA style\n",
    "        caption_template='professional',\n",
    "        number_format='3.2f',\n",
    "        p_value_format='exact',\n",
    "        save_as=['latex', 'docx', 'rtf']\n",
    "    )\n",
    "    \n",
    "    # Create data package for sharing\n",
    "    data_package = stx.pd.create_bids_package(\n",
    "        results_db,\n",
    "        package_name='erp_p300_study',\n",
    "        include_raw=False,  # Only derivatives\n",
    "        include_code=True,\n",
    "        include_config=True,\n",
    "        readme_template='comprehensive',\n",
    "        license='CC-BY-4.0'\n",
    "    )\n",
    "    \n",
    "    # Generate citations and methods\n",
    "    methods_text = stx.pd.generate_methods_section(\n",
    "        pipeline_config=CONFIG,\n",
    "        software_versions=stx.gen.get_environment_info(),\n",
    "        include_equations=True,\n",
    "        citation_style='apa'\n",
    "    )\n",
    "    \n",
    "    stx.io.save(methods_text, './manuscript/methods_section.md',\n",
    "                symlink_from_cwd=True)\n",
    "    \n",
    "    return {\n",
    "        'summary': summary_table,\n",
    "        'qc_report': qc_report,\n",
    "        'data_package': data_package\n",
    "    }\n",
    "'''\n",
    "\n",
    "print(\"RESULTS MANAGEMENT PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "print(results_management)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Integrated Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete integrated workflow\n",
    "complete_workflow = '''\n",
    "import scitex as stx\n",
    "\n",
    "def run_complete_analysis():\n",
    "    \"\"\"Execute complete EEG analysis workflow using MCP servers.\"\"\"\n",
    "    \n",
    "    # Step 1: Initialize project with orchestrator\n",
    "    project = stx.orchestrator.initialize_project(\n",
    "        name=\"P300_ERP_Analysis\",\n",
    "        type=\"neuroscience\",\n",
    "        create_structure=True\n",
    "    )\n",
    "    \n",
    "    # Step 2: Translate and validate code\n",
    "    translated = stx.io_translator.process_codebase(\n",
    "        source_dir=\"./legacy_scripts/\",\n",
    "        target_modules=[\"all\"],\n",
    "        validate=True\n",
    "    )\n",
    "    \n",
    "    # Step 3: Load configuration\n",
    "    CONFIG = stx.config.load_validated(\n",
    "        config_dir=\"./CONFIG/\",\n",
    "        schema=\"neuroscience_erp\"\n",
    "    )\n",
    "    \n",
    "    # Step 4: Run main analysis\n",
    "    with stx.gen.progress_monitor(\"EEG Analysis Pipeline\"):\n",
    "        \n",
    "        # Data loading and preprocessing\n",
    "        erp_results = main(CONFIG)\n",
    "        \n",
    "        # Signal processing\n",
    "        dsp_results = process_eeg_signals(erp_results, CONFIG)\n",
    "        \n",
    "        # Statistical analysis\n",
    "        stats_results = analyze_erp_statistics(\n",
    "            erp_results, dsp_results, CONFIG\n",
    "        )\n",
    "        \n",
    "        # Visualization\n",
    "        figures = create_publication_figures(\n",
    "            erp_results, dsp_results, stats_results, CONFIG\n",
    "        )\n",
    "        \n",
    "        # Results management\n",
    "        final_results = manage_results(\n",
    "            {\n",
    "                'erp': erp_results,\n",
    "                'dsp': dsp_results,\n",
    "                'stats': stats_results,\n",
    "                'figures': figures\n",
    "            },\n",
    "            CONFIG\n",
    "        )\n",
    "    \n",
    "    # Step 5: Generate final report\n",
    "    report = stx.orchestrator.generate_project_report(\n",
    "        project,\n",
    "        include_figures=True,\n",
    "        include_stats=True,\n",
    "        include_methods=True,\n",
    "        format='html'\n",
    "    )\n",
    "    \n",
    "    # Step 6: Package for reproducibility\n",
    "    stx.orchestrator.create_reproducibility_package(\n",
    "        project,\n",
    "        include_data=True,\n",
    "        include_environment=True,\n",
    "        create_docker=True,\n",
    "        create_singularity=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Results saved to: {project.results_dir}\")\n",
    "    print(f\"Figures saved to: {project.figures_dir}\")\n",
    "    print(f\"Report available at: {report.path}\")\n",
    "    print(f\"Reproducibility package: {project.package_path}\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run with SciTeX initialization\n",
    "    stx.gen.run_main(run_complete_analysis)\n",
    "'''\n",
    "\n",
    "print(\"COMPLETE INTEGRATED WORKFLOW\")\n",
    "print(\"=\" * 50)\n",
    "print(complete_workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Summary\n",
    "\n",
    "This integrated workflow demonstrates how SciTeX MCP servers work together:\n",
    "\n",
    "### 1. **Code Translation (IO Translator)**\n",
    "   - Converted legacy analysis scripts to SciTeX format\n",
    "   - Extracted configuration to separate files\n",
    "   - Added reproducibility features\n",
    "\n",
    "### 2. **Configuration Management (Config Server)**\n",
    "   - Centralized experimental parameters\n",
    "   - Validated configuration schema\n",
    "   - Enabled parameter sweeps and optimization\n",
    "\n",
    "### 3. **Signal Processing (DSP Server)**\n",
    "   - Advanced filtering and preprocessing\n",
    "   - Time-frequency analysis\n",
    "   - Connectivity and source estimation\n",
    "   - Feature extraction for ML\n",
    "\n",
    "### 4. **Statistical Analysis (Stats Server)**\n",
    "   - Appropriate test selection\n",
    "   - Multiple comparison corrections\n",
    "   - Effect size calculations\n",
    "   - Machine learning classification\n",
    "\n",
    "### 5. **Visualization (PLT Server)**\n",
    "   - Publication-ready figures\n",
    "   - Multi-panel layouts\n",
    "   - Statistical annotations\n",
    "   - Automatic data export\n",
    "\n",
    "### 6. **Data Management (PD Server)**\n",
    "   - Results organization\n",
    "   - Quality control\n",
    "   - Multi-format export\n",
    "   - BIDS compliance\n",
    "\n",
    "### 7. **Project Orchestration (Orchestrator)**\n",
    "   - Workflow coordination\n",
    "   - Progress monitoring\n",
    "   - Report generation\n",
    "   - Reproducibility packaging\n",
    "\n",
    "## Benefits of Integration\n",
    "\n",
    "1. **Seamless Workflow**: Each server handles its specialized domain\n",
    "2. **Consistency**: Shared configuration and data formats\n",
    "3. **Reproducibility**: Every step tracked and documented\n",
    "4. **Scalability**: Parallel processing where applicable\n",
    "5. **Quality Assurance**: Built-in validation at each step\n",
    "6. **Publication Ready**: Outputs formatted for journals\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Customize configuration for your specific experiment\n",
    "- Add domain-specific analysis modules\n",
    "- Integrate with compute clusters for large datasets\n",
    "- Share reproducibility packages with collaborators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}