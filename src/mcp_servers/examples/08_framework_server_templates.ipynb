{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Framework Server - Code Templates and Structure\n",
    "\n",
    "This notebook demonstrates the Framework Server's capabilities for converting scripts to SciTeX framework patterns, managing boilerplate code, and ensuring proper project structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of Framework Server\n",
    "\n",
    "The Framework Server helps transform standard Python scripts into properly structured SciTeX projects with:\n",
    "- Standard headers and imports\n",
    "- Main function patterns\n",
    "- Argument parsing\n",
    "- Error handling\n",
    "- Reproducibility features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a typical research script before SciTeX\n",
    "basic_script = \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "# Load and process data\n",
    "data = pd.read_csv('experiment_data.csv')\n",
    "filtered = signal.savgol_filter(data['signal'], 51, 3)\n",
    "\n",
    "# Calculate metrics\n",
    "mean_val = np.mean(filtered)\n",
    "std_val = np.std(filtered)\n",
    "print(f\"Mean: {mean_val}, Std: {std_val}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data['time'], data['signal'], alpha=0.5, label='Raw')\n",
    "plt.plot(data['time'], filtered, label='Filtered')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Signal')\n",
    "plt.legend()\n",
    "plt.savefig('results.png')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original script:\")\n",
    "print(basic_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Framework Conversion\n",
    "\n",
    "The Framework Server converts this to a properly structured SciTeX script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework Server output\n",
    "scitex_script = '''\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# Timestamp: \"2025-07-02 10:00:00 (username)\"\n",
    "# File: ./analyze_experiment.py\n",
    "# ========================================\n",
    "\"\"\"Analyze experimental data with filtering and visualization.\n",
    "\n",
    "This script processes experimental time series data by:\n",
    "1. Loading data from CSV\n",
    "2. Applying Savitzky-Golay filtering\n",
    "3. Computing statistics\n",
    "4. Generating visualizations\n",
    "\"\"\"\n",
    "# ========================================\n",
    "import os\n",
    "import sys\n",
    "from scitex import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "__FILE__ = \"./analyze_experiment.py\"\n",
    "__DIR__ = os.path.dirname(__FILE__)\n",
    "sys.path.append(os.path.dirname(__DIR__))\n",
    "# ========================================\n",
    "\n",
    "import scitex as stx\n",
    "from scipy import signal\n",
    "\n",
    "# ========================================\n",
    "# CONFIG\n",
    "# ========================================\n",
    "CONFIG = stx.io.load_config(f\"{__DIR__}/config/PATH.yaml\")\n",
    "PARAMS = stx.io.load_config(f\"{__DIR__}/config/PARAMS.yaml\")\n",
    "\n",
    "# ========================================\n",
    "# FUNCTIONS\n",
    "# ========================================\n",
    "def process_signal(data: pd.DataFrame, params: Dict[str, Any]) -> np.ndarray:\n",
    "    \"\"\"Apply Savitzky-Golay filter to signal data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Input data with 'signal' column\n",
    "    params : dict\n",
    "        Filter parameters\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Filtered signal\n",
    "    \"\"\"\n",
    "    return signal.savgol_filter(\n",
    "        data['signal'], \n",
    "        params['window_length'], \n",
    "        params['polyorder']\n",
    "    )\n",
    "\n",
    "def compute_statistics(signal: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute signal statistics.\"\"\"\n",
    "    return {\n",
    "        'mean': np.mean(signal),\n",
    "        'std': np.std(signal),\n",
    "        'min': np.min(signal),\n",
    "        'max': np.max(signal)\n",
    "    }\n",
    "\n",
    "# ========================================\n",
    "# MAIN\n",
    "# ========================================\n",
    "def main(args: argparse.Namespace) -> int:\n",
    "    \"\"\"Main analysis pipeline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    args : argparse.Namespace\n",
    "        Command line arguments\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Exit code (0 for success)\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    stx.utils.setup_logging(args.log_level)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        logger.info(f\"Loading data from {CONFIG['data']['input']}\")\n",
    "        data = stx.io.load(CONFIG['data']['input'])\n",
    "        \n",
    "        # Process signal\n",
    "        logger.info(\"Applying filter...\")\n",
    "        filtered = process_signal(data, PARAMS['filter'])\n",
    "        \n",
    "        # Compute statistics\n",
    "        stats = compute_statistics(filtered)\n",
    "        logger.info(f\"Statistics: {stats}\")\n",
    "        \n",
    "        # Visualization\n",
    "        logger.info(\"Creating visualization...\")\n",
    "        fig, ax = stx.plt.subplots(figsize=PARAMS['plot']['figure_size'])\n",
    "        \n",
    "        # Plot raw and filtered signals\n",
    "        ax.plot(data['time'], data['signal'], \n",
    "                alpha=0.5, label='Raw', color=COLORS['raw'])\n",
    "        ax.plot(data['time'], filtered, \n",
    "                label='Filtered', color=COLORS['filtered'])\n",
    "        \n",
    "        ax.set_xyt('Time (s)', 'Signal', \n",
    "                   f'Experimental Data (mean={stats[\"mean\"]:.2f})')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Save outputs\n",
    "        output_path = CONFIG['output']['figures'] / f\"results_{args.suffix}.png\"\n",
    "        stx.io.save(fig, output_path, dpi=PARAMS['plot']['dpi'])\n",
    "        \n",
    "        # Save statistics\n",
    "        stats_path = CONFIG['output']['results'] / f\"stats_{args.suffix}.yaml\"\n",
    "        stx.io.save(stats, stats_path)\n",
    "        \n",
    "        logger.info(\"Analysis complete!\")\n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Analysis failed: {e}\")\n",
    "        return 1\n",
    "\n",
    "# ========================================\n",
    "# ARGUMENT PARSER\n",
    "# ========================================\n",
    "def get_parser() -> argparse.ArgumentParser:\n",
    "    \"\"\"Create command line argument parser.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=__doc__,\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--suffix', \n",
    "        type=str, \n",
    "        default='',\n",
    "        help='Suffix for output files'\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--log-level',\n",
    "        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],\n",
    "        default='INFO',\n",
    "        help='Logging level'\n",
    "    )\n",
    "    \n",
    "    return parser\n",
    "\n",
    "# ========================================\n",
    "# ENTRY POINT\n",
    "# ========================================\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "    sys.exit(main(args))\n",
    "\n",
    "# EOF\n",
    "'''\n",
    "\n",
    "print(\"SciTeX framework script:\")\n",
    "print(scitex_script[:2000] + \"\\n... (truncated for display)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Framework Components\n",
    "\n",
    "The Framework Server manages several key components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standard Header\n",
    "header_template = '''\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# Timestamp: \"{timestamp} ({username})\"\n",
    "# File: {filepath}\n",
    "# ========================================\n",
    "\"\"\"{docstring}\"\"\"\n",
    "# ========================================\n",
    "'''\n",
    "\n",
    "print(\"Standard header template:\")\n",
    "print(header_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import Organization\n",
    "import_structure = \"\"\"\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from scitex import logging\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Path setup\n",
    "__FILE__ = \"{filepath}\"\n",
    "__DIR__ = os.path.dirname(__FILE__)\n",
    "sys.path.append(os.path.dirname(__DIR__))\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# SciTeX imports\n",
    "import scitex as stx\n",
    "\n",
    "# Local imports\n",
    "from .utils import helper_function\n",
    "\"\"\"\n",
    "\n",
    "print(\"Import organization pattern:\")\n",
    "print(import_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configuration Loading Pattern\n",
    "config_pattern = \"\"\"\n",
    "# ========================================\n",
    "# CONFIG\n",
    "# ========================================\n",
    "CONFIG = stx.io.load_config(f\"{__DIR__}/config/PATH.yaml\")\n",
    "PARAMS = stx.io.load_config(f\"{__DIR__}/config/PARAMS.yaml\")\n",
    "COLORS = stx.io.load_config(f\"{__DIR__}/config/COLORS.yaml\")\n",
    "\n",
    "# Optional: Environment-specific overrides\n",
    "env = os.getenv('SCITEX_ENV', 'development')\n",
    "if os.path.exists(f\"{__DIR__}/config/PARAMS.{env}.yaml\"):\n",
    "    env_params = stx.io.load_config(f\"{__DIR__}/config/PARAMS.{env}.yaml\")\n",
    "    PARAMS = stx.utils.merge_dicts(PARAMS, env_params)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Configuration loading pattern:\")\n",
    "print(config_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Function Templates\n",
    "\n",
    "The Framework Server provides templates for common function patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing function template\n",
    "function_templates = {\n",
    "    \"data_processor\": '''\n",
    "def process_data(\n",
    "    data: pd.DataFrame,\n",
    "    params: Dict[str, Any],\n",
    "    verbose: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Process experimental data according to parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Input data with required columns\n",
    "    params : dict\n",
    "        Processing parameters\n",
    "    verbose : bool, optional\n",
    "        Print progress information\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Processed data\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If required columns are missing\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    required_cols = params.get('required_columns', [])\n",
    "    missing = set(required_cols) - set(data.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "    \n",
    "    # Process data\n",
    "    result = data.copy()\n",
    "    \n",
    "    # Your processing logic here\n",
    "    \n",
    "    return result\n",
    "''',\n",
    "    \"analysis_function\": '''\n",
    "@stx.decorators.timed\n",
    "@stx.decorators.log_function\n",
    "def analyze_results(\n",
    "    results: Dict[str, np.ndarray],\n",
    "    method: str = 'standard'\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Analyze experimental results.\n",
    "    \n",
    "    This function is decorated to:\n",
    "    - Log execution time\n",
    "    - Log function calls\n",
    "    \"\"\"\n",
    "    analysis = {}\n",
    "    \n",
    "    # Analysis logic\n",
    "    \n",
    "    return analysis\n",
    "''',\n",
    "    \"visualization_function\": '''\n",
    "def create_figure(\n",
    "    data: pd.DataFrame,\n",
    "    style: str = 'publication'\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"Create publication-ready figure.\"\"\"\n",
    "    # Set style\n",
    "    stx.plt.set_style(style)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = stx.plt.subplots(\n",
    "        nrows=2, \n",
    "        ncols=2,\n",
    "        figsize=PARAMS['figure']['size'],\n",
    "        constrained_layout=True\n",
    "    )\n",
    "    \n",
    "    # Plotting logic\n",
    "    \n",
    "    return fig, axes\n",
    "''',\n",
    "}\n",
    "\n",
    "print(\"Function templates:\")\n",
    "for name, template in function_templates.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Function Patterns\n",
    "\n",
    "Different main function patterns for different types of scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis script main function\n",
    "analysis_main = '''\n",
    "def main(args: argparse.Namespace) -> int:\n",
    "    \"\"\"Main analysis pipeline.\"\"\"\n",
    "    # Setup\n",
    "    stx.utils.setup_logging(args.log_level)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = stx.path.Path(CONFIG['output']['base']) / args.experiment_id\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save command for reproducibility\n",
    "    stx.repro.save_command(output_dir / 'command.txt')\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        logger.info(\"Loading data...\")\n",
    "        data = stx.io.load(args.input_file)\n",
    "        \n",
    "        # Process\n",
    "        logger.info(\"Processing...\")\n",
    "        results = process_data(data, PARAMS)\n",
    "        \n",
    "        # Save results\n",
    "        logger.info(\"Saving results...\")\n",
    "        stx.io.save(results, output_dir / 'results.pkl')\n",
    "        \n",
    "        # Generate report\n",
    "        if args.generate_report:\n",
    "            generate_report(results, output_dir)\n",
    "            \n",
    "        logger.info(\"Analysis complete!\")\n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Analysis failed: {e}\", exc_info=True)\n",
    "        return 1\n",
    "'''\n",
    "\n",
    "print(\"Analysis script main function:\")\n",
    "print(analysis_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment script with multiple runs\n",
    "experiment_main = '''\n",
    "def run_single_experiment(\n",
    "    params: Dict[str, Any],\n",
    "    run_id: int\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Run a single experiment iteration.\"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    stx.utils.set_random_seed(params['seed'] + run_id)\n",
    "    \n",
    "    # Run experiment\n",
    "    results = {\n",
    "        'run_id': run_id,\n",
    "        'params': params,\n",
    "        'timestamp': stx.dt.now(),\n",
    "    }\n",
    "    \n",
    "    # Your experiment logic here\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main(args: argparse.Namespace) -> int:\n",
    "    \"\"\"Run multiple experiments with parameter sweeps.\"\"\"\n",
    "    # Setup\n",
    "    stx.utils.setup_logging(args.log_level)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Load parameter grid\n",
    "    param_grid = stx.io.load(args.param_file)\n",
    "    \n",
    "    # Run experiments\n",
    "    results = []\n",
    "    with stx.utils.tqdm(total=len(param_grid) * args.n_runs) as pbar:\n",
    "        for params in param_grid:\n",
    "            for run_id in range(args.n_runs):\n",
    "                logger.info(f\"Running experiment {run_id} with {params}\")\n",
    "                \n",
    "                result = run_single_experiment(params, run_id)\n",
    "                results.append(result)\n",
    "                \n",
    "                # Save intermediate results\n",
    "                if args.save_intermediate:\n",
    "                    stx.io.save(\n",
    "                        results, \n",
    "                        CONFIG['output']['intermediate'] / 'results.pkl'\n",
    "                    )\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Aggregate results\n",
    "    summary = aggregate_results(results)\n",
    "    stx.io.save(summary, CONFIG['output']['final'] / 'summary.pkl')\n",
    "    \n",
    "    return 0\n",
    "'''\n",
    "\n",
    "print(\"Experiment script with multiple runs:\")\n",
    "print(experiment_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Handling and Logging\n",
    "\n",
    "The Framework Server ensures proper error handling and logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive error handling\n",
    "error_handling_pattern = '''\n",
    "import scitex as stx\n",
    "from scitex import logging\n",
    "from typing import Optional\n",
    "\n",
    "class AnalysisError(Exception):\n",
    "    \"\"\"Custom exception for analysis errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "def safe_analysis(\n",
    "    data: pd.DataFrame,\n",
    "    fallback: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Perform analysis with comprehensive error handling.\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if data.empty:\n",
    "            raise AnalysisError(\"Input data is empty\")\n",
    "            \n",
    "        # Main analysis\n",
    "        logger.debug(f\"Analyzing {len(data)} samples\")\n",
    "        results = perform_analysis(data)\n",
    "        \n",
    "        # Validate results\n",
    "        if not validate_results(results):\n",
    "            raise AnalysisError(\"Results validation failed\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except AnalysisError as e:\n",
    "        logger.error(f\"Analysis error: {e}\")\n",
    "        if fallback:\n",
    "            logger.info(f\"Using fallback method: {fallback}\")\n",
    "            return perform_fallback_analysis(data, fallback)\n",
    "        raise\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\", exc_info=True)\n",
    "        \n",
    "        # Save debug information\n",
    "        debug_dir = stx.path.Path(\"./debug\")\n",
    "        debug_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        stx.io.save({\n",
    "            'error': str(e),\n",
    "            'data_shape': data.shape,\n",
    "            'data_sample': data.head(),\n",
    "            'traceback': stx.utils.get_traceback()\n",
    "        }, debug_dir / f\"error_{stx.dt.now().strftime('%Y%m%d_%H%M%S')}.pkl\")\n",
    "        \n",
    "        raise\n",
    "\n",
    "def setup_logging_with_file(args: argparse.Namespace):\n",
    "    \"\"\"Setup logging to both console and file.\"\"\"\n",
    "    log_dir = stx.path.Path(CONFIG['output']['logs'])\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    log_file = log_dir / f\"{args.experiment_id}_{stx.dt.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, args.log_level),\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "'''\n",
    "\n",
    "print(\"Error handling and logging patterns:\")\n",
    "print(error_handling_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing Framework Integration\n",
    "\n",
    "The Framework Server can generate test templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test template generation\n",
    "test_template = '''\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# Test file for analyze_experiment.py\n",
    "\n",
    "import pytest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scitex as stx\n",
    "from pathlib import Path\n",
    "\n",
    "# Import the module to test\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from analyze_experiment import process_signal, compute_statistics\n",
    "\n",
    "class TestSignalProcessing:\n",
    "    \"\"\"Test signal processing functions.\"\"\"\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def sample_data(self):\n",
    "        \"\"\"Create sample data for testing.\"\"\"\n",
    "        time = np.linspace(0, 10, 1000)\n",
    "        signal = np.sin(2 * np.pi * time) + 0.1 * np.random.randn(1000)\n",
    "        return pd.DataFrame({'time': time, 'signal': signal})\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def test_params(self):\n",
    "        \"\"\"Test parameters.\"\"\"\n",
    "        return {\n",
    "            'window_length': 51,\n",
    "            'polyorder': 3\n",
    "        }\n",
    "    \n",
    "    def test_process_signal(self, sample_data, test_params):\n",
    "        \"\"\"Test signal processing.\"\"\"\n",
    "        filtered = process_signal(sample_data, test_params)\n",
    "        \n",
    "        # Check output shape\n",
    "        assert len(filtered) == len(sample_data)\n",
    "        \n",
    "        # Check smoothing effect\n",
    "        assert np.std(filtered) < np.std(sample_data['signal'])\n",
    "    \n",
    "    def test_compute_statistics(self, sample_data):\n",
    "        \"\"\"Test statistics computation.\"\"\"\n",
    "        stats = compute_statistics(sample_data['signal'].values)\n",
    "        \n",
    "        # Check required keys\n",
    "        assert all(key in stats for key in ['mean', 'std', 'min', 'max'])\n",
    "        \n",
    "        # Check value ranges\n",
    "        assert stats['min'] <= stats['mean'] <= stats['max']\n",
    "        assert stats['std'] >= 0\n",
    "    \n",
    "    @pytest.mark.parametrize(\"window_length,polyorder\", [\n",
    "        (11, 3),\n",
    "        (51, 3),\n",
    "        (101, 5),\n",
    "    ])\n",
    "    def test_filter_parameters(self, sample_data, window_length, polyorder):\n",
    "        \"\"\"Test different filter parameters.\"\"\"\n",
    "        params = {\n",
    "            'window_length': window_length,\n",
    "            'polyorder': polyorder\n",
    "        }\n",
    "        \n",
    "        filtered = process_signal(sample_data, params)\n",
    "        assert filtered is not None\n",
    "        assert not np.any(np.isnan(filtered))\n",
    "\n",
    "class TestIntegration:\n",
    "    \"\"\"Integration tests.\"\"\"\n",
    "    \n",
    "    def test_full_pipeline(self, tmp_path):\n",
    "        \"\"\"Test the full analysis pipeline.\"\"\"\n",
    "        # Create temporary config\n",
    "        config_dir = tmp_path / \"config\"\n",
    "        config_dir.mkdir()\n",
    "        \n",
    "        # Run main with test arguments\n",
    "        # ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pytest.main([__file__, \"-v\"])\n",
    "'''\n",
    "\n",
    "print(\"Test template:\")\n",
    "print(test_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Project Scaffolding\n",
    "\n",
    "The Framework Server can generate complete project structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project structure generation\n",
    "project_structure = \"\"\"\n",
    "my_research_project/\n",
    "├── README.md\n",
    "├── pyproject.toml\n",
    "├── .gitignore\n",
    "├── config/\n",
    "│   ├── PATH.yaml\n",
    "│   ├── PARAMS.yaml\n",
    "│   ├── COLORS.yaml\n",
    "│   └── PARAMS.hpc.yaml  # HPC-specific overrides\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── main.py          # Main analysis script\n",
    "│   ├── preprocessing.py\n",
    "│   ├── analysis.py\n",
    "│   ├── visualization.py\n",
    "│   └── utils.py\n",
    "├── tests/\n",
    "│   ├── __init__.py\n",
    "│   ├── test_preprocessing.py\n",
    "│   ├── test_analysis.py\n",
    "│   └── conftest.py\n",
    "├── notebooks/\n",
    "│   ├── 01_exploration.ipynb\n",
    "│   ├── 02_analysis.ipynb\n",
    "│   └── 03_figures.ipynb\n",
    "├── data/\n",
    "│   ├── raw/            # Original data (gitignored)\n",
    "│   ├── processed/      # Processed data (gitignored)\n",
    "│   └── README.md\n",
    "├── results/\n",
    "│   ├── figures/\n",
    "│   ├── tables/\n",
    "│   └── logs/\n",
    "└── scripts/\n",
    "    ├── run_analysis.sh\n",
    "    ├── run_on_hpc.sh\n",
    "    └── setup_env.sh\n",
    "\"\"\"\n",
    "\n",
    "print(\"Generated project structure:\")\n",
    "print(project_structure)\n",
    "\n",
    "# Example generated files\n",
    "generated_files = {\n",
    "    \"README.md\": \"\"\"# My Research Project\n",
    "\n",
    "## Overview\n",
    "Brief description of your research project.\n",
    "\n",
    "## Installation\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "## Usage\n",
    "```bash\n",
    "python src/main.py --experiment-id exp001\n",
    "```\n",
    "\n",
    "## Project Structure\n",
    "- `config/`: Configuration files\n",
    "- `src/`: Source code\n",
    "- `tests/`: Unit tests\n",
    "- `notebooks/`: Jupyter notebooks\n",
    "- `data/`: Data files (not tracked)\n",
    "- `results/`: Output files\n",
    "\"\"\",\n",
    "    \".gitignore\": \"\"\"# Data files\n",
    "data/raw/*\n",
    "data/processed/*\n",
    "!data/raw/README.md\n",
    "!data/processed/README.md\n",
    "\n",
    "# Results\n",
    "results/*\n",
    "!results/.gitkeep\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.pyc\n",
    ".pytest_cache/\n",
    "*.egg-info/\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# Environment\n",
    ".env\n",
    "venv/\n",
    "\"\"\",\n",
    "    \"pyproject.toml\": \"\"\"[project]\n",
    "name = \"my-research-project\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Research project using SciTeX framework\"\n",
    "dependencies = [\n",
    "    \"scitex>=2.0.0\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"scipy\",\n",
    "    \"matplotlib\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "    \"pytest\",\n",
    "    \"pytest-cov\",\n",
    "    \"black\",\n",
    "    \"flake8\",\n",
    "]\n",
    "\n",
    "[tool.black]\n",
    "line-length = 88\n",
    "target-version = [\"py38\"]\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "print(\"\\nExample generated files:\")\n",
    "for filename, content in generated_files.items():\n",
    "    print(f\"\\n=== {filename} ===\")\n",
    "    print(content[:300] + \"...\" if len(content) > 300 else content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. CLI Integration\n",
    "\n",
    "The Framework Server helps create proper command-line interfaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced CLI patterns\n",
    "cli_pattern = '''\n",
    "import argparse\n",
    "import scitex as stx\n",
    "from typing import List\n",
    "\n",
    "def create_parser() -> argparse.ArgumentParser:\n",
    "    \"\"\"Create comprehensive CLI parser.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        prog='analyze',\n",
    "        description='Analyze experimental data with SciTeX',\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\\'\\'\\'Examples:\n",
    "  %(prog)s data.csv                    # Basic analysis\n",
    "  %(prog)s data.csv -o results/        # Specify output directory  \n",
    "  %(prog)s data.csv --method advanced  # Use advanced method\n",
    "  %(prog)s data.csv --config custom.yaml  # Custom config\n",
    "  \\'\\'\\''\n",
    "    )\n",
    "    \n",
    "    # Positional arguments\n",
    "    parser.add_argument(\n",
    "        'input_file',\n",
    "        type=str,\n",
    "        help='Input data file (CSV or HDF5)'\n",
    "    )\n",
    "    \n",
    "    # Optional arguments\n",
    "    parser.add_argument(\n",
    "        '-o', '--output-dir',\n",
    "        type=str,\n",
    "        default='./results',\n",
    "        help='Output directory (default: %(default)s)'\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--method',\n",
    "        choices=['basic', 'standard', 'advanced'],\n",
    "        default='standard',\n",
    "        help='Analysis method (default: %(default)s)'\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--config',\n",
    "        type=str,\n",
    "        help='Custom configuration file'\n",
    "    )\n",
    "    \n",
    "    # Feature flags\n",
    "    parser.add_argument(\n",
    "        '--plot',\n",
    "        action='store_true',\n",
    "        help='Generate plots'\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--report',\n",
    "        action='store_true',\n",
    "        help='Generate PDF report'\n",
    "    )\n",
    "    \n",
    "    # Advanced options\n",
    "    advanced = parser.add_argument_group('advanced options')\n",
    "    \n",
    "    advanced.add_argument(\n",
    "        '--parallel',\n",
    "        action='store_true',\n",
    "        help='Use parallel processing'\n",
    "    )\n",
    "    \n",
    "    advanced.add_argument(\n",
    "        '--n-jobs',\n",
    "        type=int,\n",
    "        default=-1,\n",
    "        help='Number of parallel jobs (-1 for all CPUs)'\n",
    "    )\n",
    "    \n",
    "    advanced.add_argument(\n",
    "        '--seed',\n",
    "        type=int,\n",
    "        default=42,\n",
    "        help='Random seed for reproducibility'\n",
    "    )\n",
    "    \n",
    "    # Debugging\n",
    "    debug = parser.add_argument_group('debugging')\n",
    "    \n",
    "    debug.add_argument(\n",
    "        '--debug',\n",
    "        action='store_true',\n",
    "        help='Enable debug mode'\n",
    "    )\n",
    "    \n",
    "    debug.add_argument(\n",
    "        '--profile',\n",
    "        action='store_true',\n",
    "        help='Profile execution time'\n",
    "    )\n",
    "    \n",
    "    debug.add_argument(\n",
    "        '--log-level',\n",
    "        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],\n",
    "        default='INFO',\n",
    "        help='Logging level'\n",
    "    )\n",
    "    \n",
    "    return parser\n",
    "\n",
    "def validate_args(args: argparse.Namespace) -> None:\n",
    "    \"\"\"Validate command line arguments.\"\"\"\n",
    "    # Check input file exists\n",
    "    if not stx.path.Path(args.input_file).exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {args.input_file}\")\n",
    "    \n",
    "    # Validate custom config\n",
    "    if args.config and not stx.path.Path(args.config).exists():\n",
    "        raise FileNotFoundError(f\"Config file not found: {args.config}\")\n",
    "    \n",
    "    # Check output directory\n",
    "    output_path = stx.path.Path(args.output_dir)\n",
    "    if output_path.exists() and not args.force:\n",
    "        response = input(f\"Output directory {output_path} exists. Overwrite? [y/N] \")\n",
    "        if response.lower() != 'y':\n",
    "            raise ValueError(\"Output directory exists. Use --force to overwrite.\")\n",
    "'''\n",
    "\n",
    "print(\"Advanced CLI patterns:\")\n",
    "print(cli_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The SciTeX Framework Server provides comprehensive support for:\n",
    "\n",
    "1. **Script Conversion**: Transform basic scripts into well-structured SciTeX projects\n",
    "2. **Boilerplate Management**: Standard headers, imports, and configuration loading\n",
    "3. **Function Templates**: Common patterns for data processing, analysis, and visualization\n",
    "4. **Error Handling**: Comprehensive error handling and logging patterns\n",
    "5. **Testing Integration**: Generate test templates and fixtures\n",
    "6. **Project Scaffolding**: Create complete project structures\n",
    "7. **CLI Development**: Advanced command-line interface patterns\n",
    "8. **Reproducibility**: Built-in support for reproducible research\n",
    "\n",
    "This ensures that all SciTeX projects follow consistent patterns and best practices for scientific computing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}