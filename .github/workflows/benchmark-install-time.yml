# Benchmark module installation times using uv
# Results deployed to GitHub Pages for dynamic shields.io badges

name: Install Time Benchmarks

on:
  release:
    types: [published]
  schedule:
    - cron: '0 6 * * 0'  # Weekly on Sunday at 6 AM UTC
  workflow_dispatch:

# Sets permissions for GitHub Pages deployment
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  benchmark:
    name: Benchmark Install Times
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Benchmark core installation
        id: core
        run: |
          START=$(date +%s.%N)
          uv pip install scitex --system
          END=$(date +%s.%N)
          TIME=$(echo "$END - $START" | bc)
          echo "time=$TIME" >> $GITHUB_OUTPUT
          echo "Core: ${TIME}s"

      - name: Benchmark heavy installation
        id: heavy
        run: |
          uv venv .venv-heavy --python 3.11
          START=$(date +%s.%N)
          uv pip install "scitex[heavy]" --python .venv-heavy/bin/python 2>/dev/null || true
          END=$(date +%s.%N)
          TIME=$(echo "$END - $START" | bc)
          echo "time=$TIME" >> $GITHUB_OUTPUT
          echo "Heavy: ${TIME}s"
          rm -rf .venv-heavy

      - name: Benchmark all installation
        id: all
        run: |
          uv venv .venv-all --python 3.11
          START=$(date +%s.%N)
          uv pip install "scitex[all]" --python .venv-all/bin/python 2>/dev/null || true
          END=$(date +%s.%N)
          TIME=$(echo "$END - $START" | bc)
          echo "time=$TIME" >> $GITHUB_OUTPUT
          echo "All: ${TIME}s"
          rm -rf .venv-all

      - name: Benchmark module installations
        run: |
          mkdir -p benchmark-results/badges

          # List of modules to benchmark
          MODULES="ai audio benchmark bridge browser capture cli config db decorators dev diagram dsp dt gen git io linalg logging msword nn parallel path pd plt repro resource scholar session sh stats str tex torch types utils web writer"

          echo "module,install_time_seconds" > benchmark-results/install-times.csv
          echo "core,${{ steps.core.outputs.time }}" >> benchmark-results/install-times.csv
          echo "heavy,${{ steps.heavy.outputs.time }}" >> benchmark-results/install-times.csv
          echo "all,${{ steps.all.outputs.time }}" >> benchmark-results/install-times.csv

          for module in $MODULES; do
            echo "Benchmarking scitex[$module]..."

            # Create fresh venv for each module
            uv venv .venv-$module --python 3.11

            START=$(date +%s.%N)
            uv pip install "scitex[$module]" --python .venv-$module/bin/python 2>/dev/null || true
            END=$(date +%s.%N)
            TIME=$(echo "$END - $START" | bc)

            echo "$module,$TIME" >> benchmark-results/install-times.csv
            echo "  $module: ${TIME}s"

            # Cleanup
            rm -rf .venv-$module
          done

      - name: Generate summary
        run: |
          echo "## Installation Time Benchmarks" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Module | Install Time (s) |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|------------------|" >> $GITHUB_STEP_SUMMARY

          tail -n +2 benchmark-results/install-times.csv | sort -t',' -k2 -n | while IFS=',' read -r module time; do
            printf "| %s | %.2f |\n" "$module" "$time" >> $GITHUB_STEP_SUMMARY
          done

      - name: Create JSON for badges
        run: |
          # Generate JSON files for shields.io endpoint badges
          while IFS=',' read -r module time; do
            [ "$module" = "module" ] && continue  # Skip header
            TIME_FORMATTED=$(printf "%.1fs" "$time")

            # Color based on time: green < 10s, yellow < 30s, red >= 30s
            if (( $(echo "$time < 10" | bc -l) )); then
              COLOR="brightgreen"
            elif (( $(echo "$time < 30" | bc -l) )); then
              COLOR="yellow"
            else
              COLOR="red"
            fi

            cat > "benchmark-results/badges/${module}.json" << EOF
{
  "schemaVersion": 1,
  "label": "install",
  "message": "$TIME_FORMATTED",
  "color": "$COLOR"
}
EOF
          done < benchmark-results/install-times.csv

          # Create index.html for GitHub Pages
          cat > benchmark-results/index.html << 'EOF'
<!DOCTYPE html>
<html>
<head><title>SciTeX Install Time Badges</title></head>
<body>
<h1>SciTeX Installation Time Badges</h1>
<p>Use these endpoints with shields.io:</p>
<pre>https://img.shields.io/endpoint?url=https://ywatanabe1989.github.io/scitex-code/badges/core.json</pre>
<p><a href="badges/">Browse all badges</a></p>
</body>
</html>
EOF

      - name: Upload artifact for Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: benchmark-results/

  deploy:
    name: Deploy to GitHub Pages
    needs: benchmark
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
