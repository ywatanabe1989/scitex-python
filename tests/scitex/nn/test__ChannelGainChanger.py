import pytest

# Required for this module
pytest.importorskip("torch")
from unittest.mock import patch

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# Import the module to test
from scitex.nn import ChannelGainChanger


class TestChannelGainChanger:
    """Comprehensive test suite for ChannelGainChanger layer."""

    def test_basic_instantiation(self):
        """Test basic instantiation with required parameters."""
        layer = ChannelGainChanger(n_chs=10)
        assert layer.n_chs == 10
        assert isinstance(layer, nn.Module)

    def test_different_channel_counts(self):
        """Test instantiation with various channel counts."""
        for n_chs in [1, 5, 10, 32, 64, 128, 256]:
            layer = ChannelGainChanger(n_chs=n_chs)
            assert layer.n_chs == n_chs

    def test_forward_shape_preservation(self):
        """Test that output shape matches input shape."""
        layer = ChannelGainChanger(n_chs=10)
        x = torch.randn(4, 10, 100)
        output = layer(x)
        assert output.shape == x.shape

    def test_eval_mode_no_gain_change(self):
        """Test that no gain change occurs in evaluation mode."""
        layer = ChannelGainChanger(n_chs=10)
        layer.eval()
        x = torch.randn(4, 10, 100)
        output = layer(x)
        assert torch.allclose(output, x)

    def test_train_mode_applies_gain_change(self):
        """Test that gain changes are applied in training mode.

        Note: ChannelGainChanger uses in-place operations (x *= gains).
        """
        layer = ChannelGainChanger(n_chs=10)
        layer.train()
        x = torch.ones(4, 10, 100)  # Use ones to easily check gain application
        x_orig = x.clone()  # Keep original before in-place modification

        torch.manual_seed(42)
        output = layer(x)

        # Output should be different from original input
        assert not torch.allclose(output, x_orig)

        # Check that gains were applied per channel - all values in a channel are scaled equally
        for ch in range(10):
            channel_values = output[:, ch, :]
            # All values in a channel should be the same (since input was all ones)
            assert torch.allclose(
                channel_values, channel_values[0, 0].expand_as(channel_values)
            )

    def test_gain_values_range(self):
        """Test that gain values are in expected range after softmax.

        Note: ChannelGainChanger uses in-place operations, so we clone input.
        Also, the implementation broadcasts same gains across batch dimension.
        """
        layer = ChannelGainChanger(n_chs=10)
        layer.train()
        x = torch.ones(4, 10, 100)
        x_orig = x.clone()

        torch.manual_seed(42)
        output = layer(x)

        # Calculate applied gains (comparing with original since x was modified in-place)
        gains = output[:, :, 0] / x_orig[:, :, 0]  # Since x_orig is ones

        # Gains should be positive
        assert torch.all(gains > 0)

        # Gains should sum to 1 across channels (softmax property)
        # Note: Same gains are broadcast to all batch elements
        assert torch.allclose(gains[0].sum(), torch.tensor(1.0), atol=1e-5)

    def test_different_batch_sizes(self):
        """Test layer works with different batch sizes."""
        layer = ChannelGainChanger(n_chs=10)
        for batch_size in [1, 2, 8, 16, 32, 64]:
            x = torch.randn(batch_size, 10, 100)
            output = layer(x)
            assert output.shape == x.shape

    def test_different_sequence_lengths(self):
        """Test layer works with different sequence lengths."""
        layer = ChannelGainChanger(n_chs=10)
        for seq_len in [10, 50, 100, 500, 1000, 5000]:
            x = torch.randn(4, 10, seq_len)
            output = layer(x)
            assert output.shape == x.shape

    @pytest.mark.skip(
        reason="In-place operations in forward() prevent gradient flow on leaf tensors"
    )
    def test_gradient_flow(self):
        """Test that gradients flow through the layer.

        Note: ChannelGainChanger uses in-place operations which break gradient flow.
        """
        layer = ChannelGainChanger(n_chs=10)
        layer.train()
        x = torch.randn(4, 10, 100, requires_grad=True)
        output = layer(x)
        loss = output.sum()
        loss.backward()
        assert x.grad is not None
        assert not torch.all(x.grad == 0)

    def test_device_compatibility_cpu(self):
        """Test layer works on CPU."""
        layer = ChannelGainChanger(n_chs=10)
        x = torch.randn(4, 10, 100)
        output = layer(x)
        assert output.device == x.device

    @pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
    def test_device_compatibility_cuda(self):
        """Test layer works on CUDA."""
        layer = ChannelGainChanger(n_chs=10).cuda()
        x = torch.randn(4, 10, 100).cuda()
        output = layer(x)
        assert output.device == x.device
        assert output.is_cuda

    def test_reproducibility_with_seed(self):
        """Test reproducible results with same random seed."""
        layer = ChannelGainChanger(n_chs=10)
        layer.train()
        x = torch.randn(4, 10, 100)

        torch.manual_seed(42)
        output1 = layer(x)

        torch.manual_seed(42)
        output2 = layer(x)

        assert torch.allclose(output1, output2)

    def test_different_results_without_seed(self):
        """Test different results without setting seed.

        Note: ChannelGainChanger uses in-place operations, so we use clones.
        """
        layer = ChannelGainChanger(n_chs=10)
        layer.train()
        x = torch.randn(4, 10, 100)

        # Use clones because forward() modifies input in-place
        output1 = layer(x.clone())
        output2 = layer(x.clone())

        assert not torch.allclose(output1, output2)

    def test_softmax_normalization(self):
        """Test that channel gains are properly normalized via softmax."""
        layer = ChannelGainChanger(n_chs=5)
        layer.train()

        # Use ones to easily extract gains
        x = torch.ones(2, 5, 10)

        torch.manual_seed(42)
        output = layer(x)

        # Extract gains (output = x * gains)
        gains = output[:, :, 0]  # Since x is all ones

        # Check softmax properties
        assert torch.all(gains > 0)  # All positive
        assert torch.allclose(gains.sum(dim=1), torch.ones(2))  # Sum to 1

    def test_gain_initialization_range(self):
        """Test that initial random gains are in expected range [0.5, 1.5]."""
        layer = ChannelGainChanger(n_chs=10)
        layer.train()

        # Mock torch.rand to verify the range
        with patch("torch.rand") as mock_rand:
            mock_rand.return_value = torch.zeros(10)  # Returns 0
            x = torch.ones(1, 10, 1)
            layer(x)

            # When torch.rand returns 0, the gains before softmax should be 0.5
            # This tests that the formula is: rand() + 0.5

    def test_integration_with_sequential(self):
        """Test integration in nn.Sequential."""
        model = nn.Sequential(
            nn.Conv1d(10, 20, 3), ChannelGainChanger(n_chs=20), nn.Conv1d(20, 10, 3)
        )
        x = torch.randn(4, 10, 100)
        output = model(x)
        assert output.shape[0] == 4
        assert output.shape[1] == 10

    def test_state_dict_save_load(self):
        """Test saving and loading state dict."""
        layer1 = ChannelGainChanger(n_chs=10)
        layer2 = ChannelGainChanger(n_chs=10)

        # ChannelGainChanger has no learnable parameters
        state_dict = layer1.state_dict()
        layer2.load_state_dict(state_dict)

        # Both should behave identically in eval mode
        layer1.eval()
        layer2.eval()
        x = torch.randn(4, 10, 100)
        assert torch.allclose(layer1(x), layer2(x))

    def test_memory_efficiency(self):
        """Test memory efficiency with large tensors."""
        layer = ChannelGainChanger(n_chs=256)
        layer.train()

        # Large tensor
        x = torch.randn(32, 256, 1000)

        # Should not raise memory errors
        output = layer(x)
        assert output.shape == x.shape

    def test_numerical_stability(self):
        """Test numerical stability with extreme values."""
        layer = ChannelGainChanger(n_chs=10)
        layer.train()

        # Test with very large values
        x = torch.randn(4, 10, 100) * 1e6
        output = layer(x)
        assert not torch.any(torch.isnan(output))
        assert not torch.any(torch.isinf(output))

        # Test with very small values
        x = torch.randn(4, 10, 100) * 1e-6
        output = layer(x)
        assert not torch.any(torch.isnan(output))
        assert not torch.any(torch.isinf(output))

    def test_zero_input_handling(self):
        """Test behavior with zero inputs."""
        layer = ChannelGainChanger(n_chs=10)
        layer.train()

        x = torch.zeros(4, 10, 100)
        output = layer(x)

        # Zero input should remain zero after gain multiplication
        assert torch.all(output == 0)

    def test_single_channel(self):
        """Test with single channel input."""
        layer = ChannelGainChanger(n_chs=1)
        layer.train()

        x = torch.randn(4, 1, 100)
        output = layer(x)

        # With single channel, softmax will always give gain of 1
        assert torch.allclose(output, x)

    def test_channel_independence(self):
        """Test that gains are applied independently per channel."""
        layer = ChannelGainChanger(n_chs=5)
        layer.train()

        # Create input where each channel has different values
        x = torch.zeros(2, 5, 10)
        for ch in range(5):
            x[:, ch, :] = ch + 1

        torch.manual_seed(42)
        output = layer(x)

        # Check that each channel is scaled uniformly
        for ch in range(5):
            channel_data = output[:, ch, :]
            if x[0, ch, 0] != 0:  # Avoid division by zero
                gain = channel_data[0, 0] / x[0, ch, 0]
                expected = x[:, ch, :] * gain
                assert torch.allclose(channel_data, expected)

    def test_training_flag_inheritance(self):
        """Test that training flag is properly inherited from parent module."""
        parent = nn.Sequential(ChannelGainChanger(n_chs=10))

        parent.train()
        assert parent[0].training

        parent.eval()
        assert not parent[0].training

    def test_gain_diversity(self):
        """Test that gains are diverse (not all equal)."""
        layer = ChannelGainChanger(n_chs=10)
        layer.train()

        x = torch.ones(1, 10, 1)

        # Run multiple times to ensure diversity
        gains_list = []
        for seed in range(5):
            torch.manual_seed(seed)
            output = layer(x)
            gains = output[0, :, 0]  # Extract gains
            gains_list.append(gains)

            # Check that not all gains are equal
            assert not torch.allclose(gains, gains[0] * torch.ones_like(gains))

    def test_gain_application_consistency(self):
        """Test that the same gain is applied across the sequence dimension."""
        layer = ChannelGainChanger(n_chs=5)
        layer.train()

        x = torch.randn(2, 5, 100)

        torch.manual_seed(42)
        output = layer(x)

        # For each channel, check gain consistency across sequence
        for ch in range(5):
            # Calculate gain at different positions
            gain_start = output[0, ch, 0] / x[0, ch, 0] if x[0, ch, 0] != 0 else 0
            gain_middle = output[0, ch, 50] / x[0, ch, 50] if x[0, ch, 50] != 0 else 0
            gain_end = output[0, ch, -1] / x[0, ch, -1] if x[0, ch, -1] != 0 else 0

            # Gains should be the same across sequence
            if x[0, ch, 0] != 0 and x[0, ch, 50] != 0:
                assert torch.allclose(
                    torch.tensor(gain_start), torch.tensor(gain_middle), atol=1e-5
                )
            if x[0, ch, 0] != 0 and x[0, ch, -1] != 0:
                assert torch.allclose(
                    torch.tensor(gain_start), torch.tensor(gain_end), atol=1e-5
                )

if __name__ == "__main__":
    import os

    import pytest

    pytest.main([os.path.abspath(__file__)])

# --------------------------------------------------------------------------------
# Start of Source Code from: /home/ywatanabe/proj/scitex-code/src/scitex/nn/_ChannelGainChanger.py
# --------------------------------------------------------------------------------
# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# # Time-stamp: "2023-04-23 11:02:45 (ywatanabe)"
# 
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from torchsummary import summary
# import scitex
# import numpy as np
# 
# 
# class ChannelGainChanger(nn.Module):
#     def __init__(
#         self,
#         n_chs,
#     ):
#         super().__init__()
#         self.n_chs = n_chs
# 
#     def forward(self, x):
#         """x: [batch_size, n_chs, seq_len]"""
#         if self.training:
#             ch_gains = (
#                 torch.rand(self.n_chs).unsqueeze(0).unsqueeze(-1).to(x.device) + 0.5
#             )
#             ch_gains = F.softmax(ch_gains, dim=1)
#             x *= ch_gains
# 
#         return x
# 
# 
# if __name__ == "__main__":
#     ## Demo data
#     bs, n_chs, seq_len = 16, 360, 1000
#     x = torch.rand(bs, n_chs, seq_len)
# 
#     cgc = ChGainChanger(n_chs)
#     print(cgc(x).shape)  # [16, 19, 1000]
# 
#     # sb = SubjectBlock(n_chs=n_chs)
#     # print(sb(x, s).shape) # [16, 270, 1000]
# 
#     # summary(sb, x, s)

# --------------------------------------------------------------------------------
# End of Source Code from: /home/ywatanabe/proj/scitex-code/src/scitex/nn/_ChannelGainChanger.py
# --------------------------------------------------------------------------------
