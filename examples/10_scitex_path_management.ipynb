{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Path Management (path) Module\n",
    "\n",
    "This notebook demonstrates the powerful path management utilities provided by the SciTeX `path` module. These utilities are essential for scientific computing workflows involving:\n",
    "\n",
    "- **File Discovery**: Finding files, directories, and project roots\n",
    "- **Path Management**: Creating, cleaning, and organizing file paths\n",
    "- **Version Control**: Managing file versions and incremental naming\n",
    "- **Project Organization**: Structured path creation for scientific projects\n",
    "- **Size Analysis**: File and directory size calculations\n",
    "\n",
    "The path module ensures consistent and robust file management across scientific computing projects.\n",
    "\n",
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scitex as stx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Configure SciTeX for this notebook\n",
    "stx.repro.fix_seeds(42)\n",
    "print(\"SciTeX Path Management (path) Module Demonstration\")\n",
    "print(f\"SciTeX version: {stx.__version__}\")\n",
    "\n",
    "# Create working directory for demonstrations\n",
    "work_dir = Path('./temp_path_demo')\n",
    "work_dir.mkdir(exist_ok=True)\n",
    "print(f\"Working directory: {work_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. File and Directory Discovery\n",
    "\n",
    "Find files, directories, and project structures efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample directory structure for demonstration\n",
    "print(\"=== Creating Sample Project Structure ===\")\n",
    "\n",
    "# Create a sample scientific project structure\n",
    "project_structure = {\n",
    "    'data': ['raw_data.csv', 'processed_data.csv', 'metadata.json'],\n",
    "    'scripts': ['analysis.py', 'visualization.py', 'preprocessing.py'],\n",
    "    'results': ['figures', 'tables', 'reports'],\n",
    "    'config': ['experiment_config.yaml', 'model_params.json'],\n",
    "    'notebooks': ['exploratory_analysis.ipynb', 'final_report.ipynb']\n",
    "}\n",
    "\n",
    "for folder, items in project_structure.items():\n",
    "    folder_path = work_dir / folder\n",
    "    folder_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    for item in items:\n",
    "        if '.' in item:  # It's a file\n",
    "            (folder_path / item).touch()\n",
    "        else:  # It's a subfolder\n",
    "            (folder_path / item).mkdir(exist_ok=True)\n",
    "            # Create some sample files in subfolders\n",
    "            if item == 'figures':\n",
    "                (folder_path / item / 'plot1.png').touch()\n",
    "                (folder_path / item / 'plot2.pdf').touch()\n",
    "            elif item == 'tables':\n",
    "                (folder_path / item / 'results_table.csv').touch()\n",
    "\n",
    "print(f\"Created sample project structure in: {work_dir}\")\n",
    "\n",
    "# Initialize a git repository for git root demonstration\n",
    "git_dir = work_dir / '.git'\n",
    "git_dir.mkdir(exist_ok=True)\n",
    "(git_dir / 'config').touch()\n",
    "print(\"Initialized sample git repository\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate file and directory finding\n",
    "print(\"=== File and Directory Discovery ===\")\n",
    "\n",
    "# Find specific files\n",
    "print(\"1. Finding specific files:\")\n",
    "config_file = stx.path.find_file('experiment_config.yaml', str(work_dir))\n",
    "print(f\"   Found config file: {config_file}\")\n",
    "\n",
    "analysis_script = stx.path.find_file('analysis.py', str(work_dir))\n",
    "print(f\"   Found analysis script: {analysis_script}\")\n",
    "\n",
    "# Find files with pattern matching\n",
    "python_files = []\n",
    "for root, dirs, files in os.walk(work_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.py'):\n",
    "            python_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"\\n   Found {len(python_files)} Python files:\")\n",
    "for py_file in python_files:\n",
    "    print(f\"     - {py_file}\")\n",
    "\n",
    "# Find directories\n",
    "print(\"\\n2. Finding directories:\")\n",
    "data_dir = stx.path.find_dir('data', str(work_dir))\n",
    "print(f\"   Found data directory: {data_dir}\")\n",
    "\n",
    "results_dir = stx.path.find_dir('results', str(work_dir))\n",
    "print(f\"   Found results directory: {results_dir}\")\n",
    "\n",
    "# Find git root\n",
    "print(\"\\n3. Finding git repository root:\")\n",
    "git_root = stx.path.find_git_root(str(work_dir / 'data'))\n",
    "print(f\"   Git root from data subdirectory: {git_root}\")\n",
    "\n",
    "# Demonstrate finding from nested locations\n",
    "nested_path = work_dir / 'results' / 'figures'\n",
    "git_root_nested = stx.path.find_git_root(str(nested_path))\n",
    "print(f\"   Git root from nested path: {git_root_nested}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scientific Path Management\n",
    "\n",
    "Create and manage structured paths for scientific projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate scientific path creation\n",
    "print(\"=== Scientific Path Management ===\")\n",
    "\n",
    "# Create structured scientific paths\n",
    "print(\"1. Creating structured scientific paths:\")\n",
    "\n",
    "# Generate experiment-specific paths\n",
    "experiment_id = \"EXP_2024_001\"\n",
    "timestamp = stx.repro.gen_timestamp()\n",
    "\n",
    "# Create experiment path structure\n",
    "experiment_spath = stx.path.mk_spath(\n",
    "    experiment_id,\n",
    "    timestamp=timestamp,\n",
    "    makedirs=True\n",
    ")\n",
    "\n",
    "print(f\"   Experiment path: {experiment_spath}\")\n",
    "print(f\"   Path exists: {Path(experiment_spath).exists()}\")\n",
    "\n",
    "# Get structured path for current context\n",
    "current_spath = stx.path.get_spath()\n",
    "print(f\"   Current structured path: {current_spath}\")\n",
    "\n",
    "# Demonstrate path retrieval from current context\n",
    "print(\"\\n2. Current path utilities:\")\n",
    "current_path = stx.path.get_this_path()\n",
    "print(f\"   Current execution path: {current_path}\")\n",
    "\n",
    "this_path_alt = stx.path.this_path()\n",
    "print(f\"   Alternative current path: {this_path_alt}\")\n",
    "\n",
    "# Create data-specific paths\n",
    "print(\"\\n3. Data-specific path creation:\")\n",
    "data_types = ['raw', 'processed', 'analyzed', 'figures']\n",
    "experiment_paths = {}\n",
    "\n",
    "for data_type in data_types:\n",
    "    path = stx.path.mk_spath(\n",
    "        f\"{experiment_id}_{data_type}\",\n",
    "        suffix=data_type,\n",
    "        makedirs=True\n",
    "    )\n",
    "    experiment_paths[data_type] = path\n",
    "    print(f\"   {data_type.capitalize()} data path: {path}\")\n",
    "\n",
    "print(f\"\\nCreated {len(experiment_paths)} data-specific paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Path Cleaning and Normalization\n",
    "\n",
    "Clean and normalize paths for consistent usage across platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate path cleaning and normalization\n",
    "print(\"=== Path Cleaning and Normalization ===\")\n",
    "\n",
    "# Create messy paths for demonstration\n",
    "messy_paths = [\n",
    "    './data//raw_data/../processed_data.csv',\n",
    "    '/tmp/experiment/./results/../figures/plot1.png',\n",
    "    'analysis/../data/./raw_data.csv',\n",
    "    './notebooks/exploratory_analysis.ipynb/../final_report.ipynb',\n",
    "    '/home/user/project/data/../../project/scripts/analysis.py'\n",
    "]\n",
    "\n",
    "print(\"1. Cleaning messy paths:\")\n",
    "cleaned_paths = []\n",
    "for messy_path in messy_paths:\n",
    "    cleaned = stx.path.clean(messy_path)\n",
    "    cleaned_paths.append(cleaned)\n",
    "    print(f\"   Original: {messy_path}\")\n",
    "    print(f\"   Cleaned:  {cleaned}\")\n",
    "    print()\n",
    "\n",
    "# Demonstrate path splitting and analysis\n",
    "print(\"2. Path component analysis:\")\n",
    "sample_path = str(work_dir / 'results' / 'figures' / 'plot1.png')\n",
    "path_components = stx.path.split(sample_path)\n",
    "\n",
    "print(f\"   Sample path: {sample_path}\")\n",
    "print(f\"   Components: {path_components}\")\n",
    "print(f\"   Directory: {path_components['directory']}\")\n",
    "print(f\"   Filename: {path_components['filename']}\")\n",
    "print(f\"   Extension: {path_components['extension']}\")\n",
    "print(f\"   Stem: {path_components['stem']}\")\n",
    "\n",
    "# Demonstrate multiple path analysis\n",
    "print(\"\\n3. Multiple path analysis:\")\n",
    "analysis_paths = [\n",
    "    str(work_dir / 'data' / 'raw_data.csv'),\n",
    "    str(work_dir / 'scripts' / 'analysis.py'),\n",
    "    str(work_dir / 'results' / 'figures' / 'plot2.pdf'),\n",
    "    str(work_dir / 'config' / 'experiment_config.yaml')\n",
    "]\n",
    "\n",
    "path_analysis = []\n",
    "for path in analysis_paths:\n",
    "    components = stx.path.split(path)\n",
    "    path_analysis.append({\n",
    "        'full_path': path,\n",
    "        'directory': components['directory'],\n",
    "        'filename': components['filename'],\n",
    "        'extension': components['extension']\n",
    "    })\n",
    "\n",
    "analysis_df = pd.DataFrame(path_analysis)\n",
    "print(analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. File Size Analysis\n",
    "\n",
    "Analyze file and directory sizes for storage management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files with different sizes for demonstration\n",
    "print(\"=== File Size Analysis ===\")\n",
    "\n",
    "# Create sample files with varying sizes\n",
    "print(\"1. Creating sample files with different sizes:\")\n",
    "sample_files = {\n",
    "    'small_dataset.csv': 1024,      # 1KB\n",
    "    'medium_dataset.csv': 1024**2,  # 1MB \n",
    "    'large_dataset.csv': 5*1024**2, # 5MB\n",
    "    'config.json': 512,             # 512B\n",
    "    'results.txt': 2048             # 2KB\n",
    "}\n",
    "\n",
    "size_test_dir = work_dir / 'size_test'\n",
    "size_test_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for filename, size in sample_files.items():\n",
    "    file_path = size_test_dir / filename\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(os.urandom(size))  # Write random data\n",
    "    print(f\"   Created {filename}: {size} bytes\")\n",
    "\n",
    "# Analyze file sizes using SciTeX utilities\n",
    "print(\"\\n2. Analyzing file sizes:\")\n",
    "file_sizes = []\n",
    "\n",
    "for filename in sample_files.keys():\n",
    "    file_path = size_test_dir / filename\n",
    "    size_info = stx.path.getsize(str(file_path))\n",
    "    file_sizes.append({\n",
    "        'filename': filename,\n",
    "        'size_bytes': size_info['bytes'],\n",
    "        'size_human': size_info['human'],\n",
    "        'size_mb': size_info['bytes'] / (1024**2)\n",
    "    })\n",
    "\n",
    "sizes_df = pd.DataFrame(file_sizes)\n",
    "print(sizes_df.round(3))\n",
    "\n",
    "# Analyze directory sizes\n",
    "print(\"\\n3. Directory size analysis:\")\n",
    "directories_to_analyze = ['data', 'scripts', 'results', 'config', 'size_test']\n",
    "dir_sizes = []\n",
    "\n",
    "for dir_name in directories_to_analyze:\n",
    "    dir_path = work_dir / dir_name\n",
    "    if dir_path.exists():\n",
    "        size_info = stx.path.getsize(str(dir_path))\n",
    "        dir_sizes.append({\n",
    "            'directory': dir_name,\n",
    "            'size_bytes': size_info['bytes'],\n",
    "            'size_human': size_info['human'],\n",
    "            'file_count': len(list(dir_path.rglob('*')))\n",
    "        })\n",
    "\n",
    "dir_sizes_df = pd.DataFrame(dir_sizes)\n",
    "print(dir_sizes_df)\n",
    "\n",
    "# Total project size\n",
    "total_project_size = stx.path.getsize(str(work_dir))\n",
    "print(f\"\\nTotal project size: {total_project_size['human']} ({total_project_size['bytes']} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Version Management and Incremental Naming\n",
    "\n",
    "Manage file versions and create incremental naming schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate version management\n",
    "print(\"=== Version Management and Incremental Naming ===\")\n",
    "\n",
    "# Create versioned files for demonstration\n",
    "versions_dir = work_dir / 'versions'\n",
    "versions_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"1. Creating versioned files:\")\n",
    "base_filename = 'analysis_report'\n",
    "versions = ['v1.0', 'v1.1', 'v1.2', 'v2.0', 'v2.1']\n",
    "\n",
    "versioned_files = []\n",
    "for version in versions:\n",
    "    filename = f\"{base_filename}_{version}.pdf\"\n",
    "    file_path = versions_dir / filename\n",
    "    file_path.touch()\n",
    "    versioned_files.append(str(file_path))\n",
    "    print(f\"   Created: {filename}\")\n",
    "\n",
    "# Find latest version\n",
    "print(\"\\n2. Finding latest version:\")\n",
    "latest_version = stx.path.find_latest(\n",
    "    str(versions_dir),\n",
    "    pattern=f\"{base_filename}_*.pdf\"\n",
    ")\n",
    "print(f\"   Latest version: {latest_version}\")\n",
    "\n",
    "# Demonstrate version increment\n",
    "print(\"\\n3. Version increment demonstration:\")\n",
    "current_version = \"v2.1\"\n",
    "incremented = stx.path.increment_version(current_version)\n",
    "print(f\"   Current version: {current_version}\")\n",
    "print(f\"   Incremented version: {incremented}\")\n",
    "\n",
    "# Create incremented version file\n",
    "new_filename = f\"{base_filename}_{incremented}.pdf\"\n",
    "new_file_path = versions_dir / new_filename\n",
    "new_file_path.touch()\n",
    "print(f\"   Created new file: {new_filename}\")\n",
    "\n",
    "# Demonstrate automatic version increment for existing files\n",
    "print(\"\\n4. Automatic version increment for duplicates:\")\n",
    "test_filename = \"experiment_results.csv\"\n",
    "test_base_path = versions_dir / test_filename\n",
    "\n",
    "# Create original file\n",
    "test_base_path.touch()\n",
    "print(f\"   Created original: {test_filename}\")\n",
    "\n",
    "# Create incremented versions automatically\n",
    "for i in range(3):\n",
    "    incremented_path = stx.path.increment_version(str(test_base_path))\n",
    "    Path(incremented_path).touch()\n",
    "    print(f\"   Created incremented: {Path(incremented_path).name}\")\n",
    "\n",
    "# List all files in versions directory\n",
    "print(\"\\n5. All versioned files:\")\n",
    "all_files = sorted(list(versions_dir.glob('*')))\n",
    "for i, file_path in enumerate(all_files, 1):\n",
    "    print(f\"   {i}. {file_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Package Data Path Management\n",
    "\n",
    "Manage data paths within Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate package data path management\n",
    "print(\"=== Package Data Path Management ===\")\n",
    "\n",
    "# Create a mock package structure\n",
    "package_dir = work_dir / 'mock_package'\n",
    "package_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create package structure\n",
    "package_structure = {\n",
    "    '__init__.py': '',\n",
    "    'data': {\n",
    "        'sample_data.csv': 'sample,data\\n1,2\\n3,4',\n",
    "        'config.json': '{\"param1\": 1, \"param2\": 2}',\n",
    "        'reference.txt': 'Reference data file'\n",
    "    },\n",
    "    'models': {\n",
    "        'trained_model.pkl': 'mock_model_data',\n",
    "        'model_config.yaml': 'model: test\\nversion: 1.0'\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_package_structure(base_path, structure):\n",
    "    for name, content in structure.items():\n",
    "        path = base_path / name\n",
    "        if isinstance(content, dict):\n",
    "            path.mkdir(exist_ok=True)\n",
    "            create_package_structure(path, content)\n",
    "        else:\n",
    "            path.write_text(content)\n",
    "\n",
    "create_package_structure(package_dir, package_structure)\n",
    "print(f\"Created mock package structure at: {package_dir}\")\n",
    "\n",
    "# Demonstrate data path retrieval\n",
    "print(\"\\n1. Package data path retrieval:\")\n",
    "\n",
    "# Simulate getting data paths from package\n",
    "try:\n",
    "    # This would normally work with an installed package\n",
    "    data_path = stx.path.get_data_path_from_a_package('scitex')\n",
    "    print(f\"   SciTeX data path: {data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Note: {e} (expected for demo)\")\n",
    "\n",
    "# Manual demonstration of data path discovery\n",
    "print(\"\\n2. Manual data path discovery:\")\n",
    "data_files = list((package_dir / 'data').glob('*'))\n",
    "model_files = list((package_dir / 'models').glob('*'))\n",
    "\n",
    "print(\"   Data files found:\")\n",
    "for data_file in data_files:\n",
    "    size_info = stx.path.getsize(str(data_file))\n",
    "    print(f\"     - {data_file.name}: {size_info['human']}\")\n",
    "\n",
    "print(\"   Model files found:\")\n",
    "for model_file in model_files:\n",
    "    size_info = stx.path.getsize(str(model_file))\n",
    "    print(f\"     - {model_file.name}: {size_info['human']}\")\n",
    "\n",
    "# Create comprehensive package inventory\n",
    "print(\"\\n3. Package inventory:\")\n",
    "package_inventory = []\n",
    "\n",
    "for root, dirs, files in os.walk(package_dir):\n",
    "    for file in files:\n",
    "        file_path = Path(root) / file\n",
    "        rel_path = file_path.relative_to(package_dir)\n",
    "        size_info = stx.path.getsize(str(file_path))\n",
    "        \n",
    "        package_inventory.append({\n",
    "            'relative_path': str(rel_path),\n",
    "            'size_bytes': size_info['bytes'],\n",
    "            'size_human': size_info['human'],\n",
    "            'type': rel_path.parent.name if rel_path.parent.name != '.' else 'root'\n",
    "        })\n",
    "\n",
    "inventory_df = pd.DataFrame(package_inventory)\n",
    "print(inventory_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scientific Workflow Path Integration\n",
    "\n",
    "Integrate path management into complete scientific workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate comprehensive scientific workflow path management\n",
    "print(\"=== Scientific Workflow Path Integration ===\")\n",
    "\n",
    "class ScientificPathManager:\n",
    "    def __init__(self, project_name, base_dir=None):\n",
    "        self.project_name = project_name\n",
    "        self.base_dir = Path(base_dir) if base_dir else Path.cwd()\n",
    "        self.project_root = None\n",
    "        self.paths = {}\n",
    "        \n",
    "    def initialize_project(self):\n",
    "        \"\"\"Initialize project directory structure.\"\"\"\n",
    "        print(f\"1. Initializing project: {self.project_name}\")\n",
    "        \n",
    "        # Create project root\n",
    "        self.project_root = stx.path.mk_spath(\n",
    "            self.project_name,\n",
    "            parent=str(self.base_dir),\n",
    "            makedirs=True\n",
    "        )\n",
    "        \n",
    "        print(f\"   Project root: {self.project_root}\")\n",
    "        \n",
    "        # Define standard scientific project structure\n",
    "        standard_dirs = [\n",
    "            'data/raw',\n",
    "            'data/processed', \n",
    "            'data/external',\n",
    "            'scripts/preprocessing',\n",
    "            'scripts/analysis',\n",
    "            'scripts/visualization',\n",
    "            'results/figures',\n",
    "            'results/tables',\n",
    "            'results/reports',\n",
    "            'config',\n",
    "            'notebooks',\n",
    "            'docs',\n",
    "            'tests'\n",
    "        ]\n",
    "        \n",
    "        # Create directory structure\n",
    "        for dir_path in standard_dirs:\n",
    "            full_path = Path(self.project_root) / dir_path\n",
    "            full_path.mkdir(parents=True, exist_ok=True)\n",
    "            self.paths[dir_path.replace('/', '_')] = str(full_path)\n",
    "            \n",
    "        print(f\"   Created {len(standard_dirs)} standard directories\")\n",
    "        \n",
    "    def create_experiment_paths(self, experiment_id):\n",
    "        \"\"\"Create experiment-specific paths.\"\"\"\n",
    "        print(f\"\\n2. Creating experiment paths for: {experiment_id}\")\n",
    "        \n",
    "        experiment_base = Path(self.project_root) / 'experiments' / experiment_id\n",
    "        experiment_base.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create experiment-specific structure\n",
    "        exp_dirs = ['data', 'config', 'results', 'logs', 'checkpoints']\n",
    "        experiment_paths = {}\n",
    "        \n",
    "        for exp_dir in exp_dirs:\n",
    "            exp_path = experiment_base / exp_dir\n",
    "            exp_path.mkdir(exist_ok=True)\n",
    "            experiment_paths[exp_dir] = str(exp_path)\n",
    "            \n",
    "        print(f\"   Created experiment structure at: {experiment_base}\")\n",
    "        return experiment_paths\n",
    "        \n",
    "    def manage_output_files(self, experiment_paths):\n",
    "        \"\"\"Manage output files with versioning.\"\"\"\n",
    "        print(\"\\n3. Managing output files with versioning:\")\n",
    "        \n",
    "        # Create sample output files\n",
    "        output_types = {\n",
    "            'model_results.json': 'results',\n",
    "            'training_log.txt': 'logs',\n",
    "            'final_model.pkl': 'checkpoints',\n",
    "            'analysis_plot.png': 'results'\n",
    "        }\n",
    "        \n",
    "        created_files = []\n",
    "        for filename, subdir in output_types.items():\n",
    "            base_path = Path(experiment_paths[subdir]) / filename\n",
    "            \n",
    "            # Create multiple versions\n",
    "            for version in range(3):\n",
    "                if version == 0:\n",
    "                    file_path = base_path\n",
    "                else:\n",
    "                    file_path = Path(stx.path.increment_version(str(base_path)))\n",
    "                \n",
    "                file_path.touch()\n",
    "                created_files.append(str(file_path))\n",
    "                print(f\"   Created: {file_path.name}\")\n",
    "                \n",
    "        return created_files\n",
    "        \n",
    "    def analyze_project_structure(self):\n",
    "        \"\"\"Analyze complete project structure.\"\"\"\n",
    "        print(\"\\n4. Project structure analysis:\")\n",
    "        \n",
    "        # Get project size\n",
    "        project_size = stx.path.getsize(self.project_root)\n",
    "        print(f\"   Total project size: {project_size['human']}\")\n",
    "        \n",
    "        # Analyze directory sizes\n",
    "        dir_analysis = []\n",
    "        project_path = Path(self.project_root)\n",
    "        \n",
    "        for item in project_path.rglob('*'):\n",
    "            if item.is_dir():\n",
    "                try:\n",
    "                    size_info = stx.path.getsize(str(item))\n",
    "                    rel_path = item.relative_to(project_path)\n",
    "                    dir_analysis.append({\n",
    "                        'directory': str(rel_path),\n",
    "                        'size_bytes': size_info['bytes'],\n",
    "                        'size_human': size_info['human'],\n",
    "                        'depth': len(rel_path.parts)\n",
    "                    })\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        if dir_analysis:\n",
    "            analysis_df = pd.DataFrame(dir_analysis)\n",
    "            analysis_df = analysis_df.sort_values('size_bytes', ascending=False)\n",
    "            print(\"\\n   Top directories by size:\")\n",
    "            print(analysis_df.head(10))\n",
    "            \n",
    "        return dir_analysis\n",
    "        \n",
    "    def cleanup_project(self):\n",
    "        \"\"\"Clean up project files.\"\"\"\n",
    "        print(\"\\n5. Project cleanup:\")\n",
    "        \n",
    "        # Find and clean temporary files\n",
    "        temp_patterns = ['*.tmp', '*.temp', '*~', '.DS_Store']\n",
    "        project_path = Path(self.project_root)\n",
    "        \n",
    "        cleaned_files = []\n",
    "        for pattern in temp_patterns:\n",
    "            for temp_file in project_path.rglob(pattern):\n",
    "                cleaned_files.append(str(temp_file))\n",
    "                # temp_file.unlink()  # Uncomment to actually delete\n",
    "                \n",
    "        if cleaned_files:\n",
    "            print(f\"   Found {len(cleaned_files)} temporary files to clean\")\n",
    "        else:\n",
    "            print(\"   No temporary files found\")\n",
    "            \n",
    "        return cleaned_files\n",
    "\n",
    "# Demonstrate complete workflow\n",
    "workflow_manager = ScientificPathManager(\n",
    "    project_name=\"neuroscience_experiment_2024\",\n",
    "    base_dir=work_dir\n",
    ")\n",
    "\n",
    "# Run complete workflow\n",
    "workflow_manager.initialize_project()\n",
    "exp_paths = workflow_manager.create_experiment_paths(\"pilot_study_001\")\n",
    "output_files = workflow_manager.manage_output_files(exp_paths)\n",
    "structure_analysis = workflow_manager.analyze_project_structure()\n",
    "cleanup_results = workflow_manager.cleanup_project()\n",
    "\n",
    "print(f\"\\nâœ“ Workflow completed successfully!\")\n",
    "print(f\"   Created project at: {workflow_manager.project_root}\")\n",
    "print(f\"   Generated {len(output_files)} output files\")\n",
    "print(f\"   Analyzed {len(structure_analysis)} directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Path Operations\n",
    "\n",
    "Demonstrate advanced path operations for complex scientific workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate advanced path operations\n",
    "print(\"=== Advanced Path Operations ===\")\n",
    "\n",
    "# Create complex directory structure for advanced operations\n",
    "advanced_dir = work_dir / 'advanced_demo'\n",
    "advanced_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Simulate a multi-experiment project\n",
    "experiments = ['exp_001', 'exp_002', 'exp_003']\n",
    "conditions = ['control', 'treatment_a', 'treatment_b']\n",
    "sessions = ['session_01', 'session_02', 'session_03']\n",
    "\n",
    "print(\"1. Creating complex experimental structure:\")\n",
    "created_paths = []\n",
    "\n",
    "for exp in experiments:\n",
    "    for condition in conditions:\n",
    "        for session in sessions:\n",
    "            # Create nested path\n",
    "            session_path = advanced_dir / exp / condition / session\n",
    "            session_path.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Create sample data files\n",
    "            data_files = ['behavioral_data.csv', 'neural_data.h5', 'metadata.json']\n",
    "            for data_file in data_files:\n",
    "                file_path = session_path / data_file\n",
    "                file_path.touch()\n",
    "                created_paths.append(str(file_path))\n",
    "\n",
    "print(f\"   Created {len(created_paths)} files in nested structure\")\n",
    "\n",
    "# Advanced file finding with complex patterns\n",
    "print(\"\\n2. Advanced file discovery:\")\n",
    "\n",
    "# Find all behavioral data files\n",
    "behavioral_files = []\n",
    "for file_path in Path(advanced_dir).rglob('behavioral_data.csv'):\n",
    "    behavioral_files.append(str(file_path))\n",
    "\n",
    "print(f\"   Found {len(behavioral_files)} behavioral data files\")\n",
    "\n",
    "# Find files by experiment\n",
    "exp_001_files = []\n",
    "for file_path in Path(advanced_dir / 'exp_001').rglob('*'):\n",
    "    if file_path.is_file():\n",
    "        exp_001_files.append(str(file_path))\n",
    "\n",
    "print(f\"   Found {len(exp_001_files)} files in exp_001\")\n",
    "\n",
    "# Create analysis of file distribution\n",
    "print(\"\\n3. File distribution analysis:\")\n",
    "file_distribution = []\n",
    "\n",
    "for exp in experiments:\n",
    "    exp_path = advanced_dir / exp\n",
    "    if exp_path.exists():\n",
    "        exp_files = list(exp_path.rglob('*'))\n",
    "        exp_size = stx.path.getsize(str(exp_path))\n",
    "        \n",
    "        file_distribution.append({\n",
    "            'experiment': exp,\n",
    "            'total_files': len([f for f in exp_files if f.is_file()]),\n",
    "            'total_dirs': len([f for f in exp_files if f.is_dir()]),\n",
    "            'size_bytes': exp_size['bytes'],\n",
    "            'size_human': exp_size['human']\n",
    "        })\n",
    "\n",
    "distribution_df = pd.DataFrame(file_distribution)\n",
    "print(distribution_df)\n",
    "\n",
    "# Demonstrate path pattern analysis\n",
    "print(\"\\n4. Path pattern analysis:\")\n",
    "path_patterns = {}\n",
    "\n",
    "for file_path in created_paths[:20]:  # Analyze first 20 paths\n",
    "    path_obj = Path(file_path)\n",
    "    components = stx.path.split(file_path)\n",
    "    \n",
    "    # Extract pattern components\n",
    "    parts = path_obj.relative_to(advanced_dir).parts\n",
    "    if len(parts) >= 4:  # exp/condition/session/file\n",
    "        pattern = f\"{parts[0]}/{parts[1]}/{parts[2]}/{components['extension']}\"\n",
    "        if pattern not in path_patterns:\n",
    "            path_patterns[pattern] = 0\n",
    "        path_patterns[pattern] += 1\n",
    "\n",
    "print(\"   Path patterns found:\")\n",
    "for pattern, count in sorted(path_patterns.items()):\n",
    "    print(f\"     {pattern}: {count} files\")\n",
    "\n",
    "# Performance analysis for large file operations\n",
    "print(\"\\n5. Performance analysis:\")\n",
    "import time\n",
    "\n",
    "# Time file discovery operations\n",
    "start_time = time.time()\n",
    "all_files = list(Path(advanced_dir).rglob('*'))\n",
    "discovery_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "all_sizes = [stx.path.getsize(str(f)) for f in all_files[:10] if f.is_file()]\n",
    "sizing_time = time.time() - start_time\n",
    "\n",
    "print(f\"   File discovery time: {discovery_time:.4f} seconds for {len(all_files)} items\")\n",
    "print(f\"   Size calculation time: {sizing_time:.4f} seconds for 10 files\")\n",
    "print(f\"   Average time per file: {sizing_time/10:.6f} seconds\")\n",
    "\n",
    "print(f\"\\nâœ“ Advanced path operations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Best Practices\n",
    "\n",
    "The SciTeX path module provides comprehensive path management for scientific computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key path utilities demonstrated\n",
    "summary = {\n",
    "    'File Discovery': [\n",
    "        'stx.path.find_file() - Locate specific files',\n",
    "        'stx.path.find_dir() - Locate directories',\n",
    "        'stx.path.find_git_root() - Find repository root',\n",
    "        'Pattern-based file discovery'\n",
    "    ],\n",
    "    'Path Management': [\n",
    "        'stx.path.mk_spath() - Create structured paths',\n",
    "        'stx.path.get_spath() - Get current structured path',\n",
    "        'stx.path.clean() - Normalize and clean paths',\n",
    "        'stx.path.split() - Analyze path components'\n",
    "    ],\n",
    "    'Version Control': [\n",
    "        'stx.path.find_latest() - Find latest version',\n",
    "        'stx.path.increment_version() - Create new versions',\n",
    "        'Automatic version management',\n",
    "        'Duplicate file handling'\n",
    "    ],\n",
    "    'Size Analysis': [\n",
    "        'stx.path.getsize() - File and directory sizes',\n",
    "        'Human-readable size formatting',\n",
    "        'Storage analysis and optimization',\n",
    "        'Project size monitoring'\n",
    "    ],\n",
    "    'Project Organization': [\n",
    "        'Scientific project structure creation',\n",
    "        'Experiment-specific path management',\n",
    "        'Package data path handling',\n",
    "        'Complex workflow integration'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"SciTeX Path Management Module - Key Utilities Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category, utilities in summary.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for utility in utilities:\n",
    "        print(f\"  â€¢ {utility}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Best Practices:\")\n",
    "print(\"  â€¢ Use mk_spath() for consistent project structure\")\n",
    "print(\"  â€¢ Implement version management from project start\")\n",
    "print(\"  â€¢ Regularly monitor project size with getsize()\")\n",
    "print(\"  â€¢ Clean paths with clean() for cross-platform compatibility\")\n",
    "print(\"  â€¢ Use find_git_root() to locate project boundaries\")\n",
    "print(\"  â€¢ Implement automated path organization for large projects\")\n",
    "print(\"  â€¢ Plan directory structure before project implementation\")\n",
    "\n",
    "print(f\"\\nDemo completed successfully! ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files and directories\n",
    "import shutil\n",
    "\n",
    "# Remove temporary directories\n",
    "temp_paths = ['./temp_path_demo']\n",
    "\n",
    "for temp_path in temp_paths:\n",
    "    if Path(temp_path).exists():\n",
    "        shutil.rmtree(temp_path)\n",
    "        print(f\"Cleaned up: {temp_path}\")\n",
    "\n",
    "print(\"\\nNotebook cleanup completed.\")"
   ]
  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}