{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Pandas Module - Comprehensive Tutorial\n",
    "\n",
    "This notebook demonstrates the complete functionality of the `scitex.pd` module for advanced pandas DataFrame operations, data manipulation, and statistical analysis utilities.\n",
    "\n",
    "## Features Covered\n",
    "* Universal DataFrame conversion with `force_df`\n",
    "* Statistical column detection and organization\n",
    "* Advanced data reshaping and melting\n",
    "* Column and row movement operations\n",
    "* Matrix to long format conversion\n",
    "* Data type conversion utilities\n",
    "* Warning suppression for clean workflows\n",
    "* Complete data processing pipelines\n",
    "\n",
    "## Table of Contents\n",
    "1. [Universal DataFrame Conversion](#1-universal-dataframe-conversion)\n",
    "2. [Statistical Column Detection](#2-statistical-column-detection)\n",
    "3. [Data Organization Operations](#3-data-organization-operations)\n",
    "4. [Advanced Data Reshaping](#4-advanced-data-reshaping)\n",
    "5. [Matrix Format Conversions](#5-matrix-format-conversions)\n",
    "6. [Data Type Utilities](#6-data-type-utilities)\n",
    "7. [Advanced Operations](#7-advanced-operations)\n",
    "8. [Real-World Applications](#8-real-world-applications)\n",
    "9. [Complete Data Pipeline](#9-complete-data-pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect notebook name for output directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook name (for papermill compatibility)\n",
    "notebook_name = \"15_scitex_pd\"\n",
    "if 'PAPERMILL_NOTEBOOK_NAME' in os.environ:\n",
    "    notebook_name = Path(os.environ['PAPERMILL_NOTEBOOK_NAME']).stem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Universal DataFrame Conversion\n",
    "\n",
    "The `force_df` function converts any data type into a pandas DataFrame with intelligent handling of different input formats."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex as stx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Converting Various Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Scalar value\n",
    "scalar = 42\n",
    "df_scalar = stx.pd.force_df(scalar)\n",
    "\n",
    "# 2. List of values\n",
    "list_data = [1, 2, 3, 4, 5]\n",
    "df_list = stx.pd.force_df(list_data)\n",
    "\n",
    "# 3. 1D NumPy array\n",
    "array_1d = np.array([10, 20, 30, 40, 50])\n",
    "df_array_1d = stx.pd.force_df(array_1d)\n",
    "\n",
    "# 4. 2D NumPy array\n",
    "array_2d = np.random.randn(4, 3)\n",
    "df_array_2d = stx.pd.force_df(array_2d)\n",
    "\n",
    "# 5. Dictionary with equal lengths\n",
    "dict_equal = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'age': [25, 30, 35],\n",
    "    'city': ['New York', 'London', 'Tokyo']\n",
    "}\n",
    "df_dict_equal = stx.pd.force_df(dict_equal)\n",
    "\n",
    "# 6. Dictionary with unequal lengths\n",
    "dict_unequal = {\n",
    "    'experiment_1': [23.5, 24.1, 23.8],\n",
    "    'experiment_2': [22.9, 23.5, 24.0, 23.7, 23.9],\n",
    "    'experiment_3': [24.2, 23.6]\n",
    "}\n",
    "df_dict_unequal = stx.pd.force_df(dict_unequal)\n",
    "\n",
    "# 7. Pandas Series\n",
    "series = pd.Series([100, 200, 300, 400], index=['Q1', 'Q2', 'Q3', 'Q4'], name='sales')\n",
    "df_series = stx.pd.force_df(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Custom Filler Values for Unequal Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Demonstrate different filler strategies\n",
    "\n",
    "# Sample data with unequal lengths\n",
    "measurement_data = {\n",
    "    'control_group': [98.6, 98.8, 99.1, 98.7],\n",
    "    'treatment_a': [99.2, 99.5, 99.8, 100.1, 99.9, 100.3],\n",
    "    'treatment_b': [97.8, 98.1]\n",
    "}\n",
    "\n",
    "for key, values in measurement_data.items():\n",
    "        pass  # Processing key\n",
    "# 1. Default filler (NaN)\n",
    "df_nan = stx.pd.force_df(measurement_data)\n",
    "\n",
    "# 2. Zero filler\n",
    "df_zero = stx.pd.force_df(measurement_data, filler=0)\n",
    "\n",
    "# 3. Mean filler\n",
    "all_values = [v for vals in measurement_data.values() for v in vals]\n",
    "mean_value = np.mean(all_values)\n",
    "df_mean = stx.pd.force_df(measurement_data, filler=mean_value)\n",
    "\n",
    "# 4. Custom string filler\n",
    "df_missing = stx.pd.force_df(measurement_data, filler='MISSING')\n",
    "\n",
    "# Compare statistics\n",
    "fillers = ['NaN', 'Zero', 'Mean', 'String']\n",
    "dataframes = [df_nan, df_zero, df_mean, df_missing]\n",
    "\n",
    "for filler, df in zip(fillers, dataframes):\n",
    "    if filler != 'String':  # Skip string data for numeric stats\n",
    "    try:\n",
    "        numeric_df = df.select_dtypes(include=[np.number])\n",
    "        except:\n",
    "            pass  # Handle exception\n",
    "    else:        pass  # Fixed incomplete block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Column Detection\n",
    "\n",
    "The `find_pval` function automatically detects p-value columns in DataFrames, useful for statistical analysis workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with statistical results\n",
    "statistical_results = {\n",
    "    'feature': ['height', 'weight', 'age', 'blood_pressure', 'cholesterol'],\n",
    "    'mean_control': [170.5, 68.2, 35.4, 120.5, 180.2],\n",
    "    'mean_treatment': [172.1, 66.8, 35.6, 118.2, 175.8],\n",
    "    'std_control': [8.5, 12.1, 8.9, 15.2, 25.4],\n",
    "    'std_treatment': [9.1, 11.8, 9.2, 14.8, 23.9],\n",
    "    'effect_size': [0.19, -0.11, 0.02, -0.15, -0.18],\n",
    "    'p_value': [0.023, 0.156, 0.832, 0.041, 0.067],\n",
    "    'pval_adjusted': [0.092, 0.312, 0.832, 0.164, 0.268],\n",
    "    'confidence_interval_lower': [0.12, -0.18, -0.09, -0.22, -0.25],\n",
    "    'confidence_interval_upper': [0.26, -0.04, 0.13, -0.08, -0.11],\n",
    "    'significance': ['*', 'ns', 'ns', '*', 'ns'],\n",
    "    'p-val-bonferroni': [0.115, 0.780, 1.000, 0.205, 0.335]\n",
    "}\n",
    "\n",
    "df_stats = pd.DataFrame(statistical_results)\n",
    "\n",
    "# Find all p-value columns\n",
    "pval_cols_all = stx.pd.find_pval(df_stats, multiple=True)\n",
    "\n",
    "# Find only the first p-value column\n",
    "pval_col_first = stx.pd.find_pval(df_stats, multiple=False)\n",
    "\n",
    "# Extract and analyze p-values\n",
    "for col in pval_cols_all:\n",
    "    p_values = df_stats[col]\n",
    "    significant = (p_values < 0.05).sum()\n",
    "    total = len(p_values)\n",
    "\n",
    "# Create significance summary\n",
    "significance_summary = df_stats[['feature'] + pval_cols_all].copy()\n",
    "for col in pval_cols_all:\n",
    "    significance_summary[f'{col}_sig'] = significance_summary[col] < 0.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Organization Operations\n",
    "\n",
    "Functions for organizing DataFrames by moving columns and rows to specific positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Column Movement Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'subject_id': ['S001', 'S002', 'S003', 'S004', 'S005'],\n",
    "    'age': [25, 30, 35, 40, 45],\n",
    "    'gender': ['M', 'F', 'M', 'F', 'M'],\n",
    "    'baseline_score': [85, 92, 78, 88, 91],\n",
    "    'treatment_score': [88, 95, 82, 92, 94],\n",
    "    'improvement': [3, 3, 4, 4, 3],\n",
    "    'p_value': [0.023, 0.015, 0.048, 0.012, 0.031]\n",
    "}\n",
    "\n",
    "df_original = pd.DataFrame(data)\n",
    "\n",
    "# 1. Move column to specific position\n",
    "df_moved = stx.pd.mv(df_original, 'p_value', 2)\n",
    "\n",
    "# 2. Move column to first position\n",
    "df_first = stx.pd.mv_to_first(df_original, 'treatment_score')\n",
    "\n",
    "# 3. Move column to last position\n",
    "df_last = stx.pd.mv_to_last(df_original, 'subject_id')\n",
    "\n",
    "# 4. Chain operations for complex reorganization\n",
    "df_reorganized = (df_original\n",
    "    .pipe(stx.pd.mv_to_first, 'p_value')  # P-value first\n",
    "    .pipe(stx.pd.mv_to_first, 'subject_id')  # ID before p-value\n",
    "    .pipe(stx.pd.mv_to_last, 'gender')  # Gender last\n",
    "    )\n",
    "\n",
    "\n",
    "# 5. Organize based on p-value detection\n",
    "pval_col = stx.pd.find_pval(df_original)\n",
    "if pval_col:\n",
    "    df_pval_first = stx.pd.mv_to_first(df_original, pval_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Row Movement Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame with named index\n",
    "experiment_data = {\n",
    "    'measurement': [10.5, 20.3, 15.7, 25.1, 18.9],\n",
    "    'error': [0.2, 0.4, 0.3, 0.5, 0.3],\n",
    "    'category': ['baseline', 'treatment', 'control', 'recovery', 'followup']\n",
    "}\n",
    "\n",
    "df_rows = pd.DataFrame(experiment_data, \n",
    "    index=['exp_baseline', 'exp_treatment', 'exp_control',\n",
    "    'exp_recovery', 'exp_followup'])\n",
    "\n",
    "\n",
    "# 1. Move row to first position\n",
    "df_row_first = stx.pd.mv_to_first(df_rows, 'exp_treatment', axis=0)\n",
    "\n",
    "# 2. Move row to last position\n",
    "df_row_last = stx.pd.mv_to_last(df_rows, 'exp_baseline', axis=0)\n",
    "\n",
    "# 3. Logical ordering (treatment first, then control, then others)\n",
    "desired_order = ['exp_treatment', 'exp_control', 'exp_baseline', 'exp_recovery', 'exp_followup']\n",
    "df_logical = df_rows.reindex(desired_order)\n",
    "\n",
    "# 4. Sort by measurement value and reorganize\n",
    "df_sorted = df_rows.sort_values('measurement')\n",
    "# Move highest value to first\n",
    "highest_idx = df_sorted.index[-1]  # Last after sorting (highest)\n",
    "df_highlight = stx.pd.mv_to_first(df_sorted, highest_idx, axis=0)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Data Reshaping\n",
    "\n",
    "Specialized functions for reshaping data while preserving relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Selective Column Melting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create wide format longitudinal data\n",
    "longitudinal_data = {\n",
    "    'participant_id': ['P001', 'P002', 'P003', 'P004', 'P005'],\n",
    "    'age': [25, 30, 35, 40, 45],\n",
    "    'gender': ['M', 'F', 'M', 'F', 'M'],\n",
    "    'baseline_anxiety': [12, 15, 18, 14, 16],\n",
    "    'week_2_anxiety': [10, 13, 16, 12, 14],\n",
    "    'week_4_anxiety': [8, 11, 14, 10, 12],\n",
    "    'week_8_anxiety': [6, 9, 12, 8, 10],\n",
    "    'baseline_depression': [8, 12, 15, 11, 13],\n",
    "    'week_2_depression': [7, 10, 13, 9, 11],\n",
    "    'week_4_depression': [6, 8, 11, 7, 9],\n",
    "    'week_8_depression': [5, 6, 9, 5, 7]\n",
    "}\n",
    "\n",
    "df_wide = pd.DataFrame(longitudinal_data)\n",
    "\n",
    "# 1. Melt only anxiety measurements\n",
    "anxiety_cols = [col for col in df_wide.columns if 'anxiety' in col]\n",
    "df_anxiety_long = stx.pd.melt_cols(df_wide, anxiety_cols)\n",
    "\n",
    "\n",
    "# 2. Melt depression measurements with specific ID columns\n",
    "depression_cols = [col for col in df_wide.columns if 'depression' in col]\n",
    "df_depression_long = stx.pd.melt_cols(df_wide, depression_cols, \n",
    "    id_columns=['participant_id', 'age', 'gender'])\n",
    "\n",
    "\n",
    "# 3. Melt all measurement columns (both anxiety and depression)\n",
    "measurement_cols = anxiety_cols + depression_cols\n",
    "df_all_long = stx.pd.melt_cols(df_wide, measurement_cols, \n",
    "    id_columns=['participant_id', 'age', 'gender'])\n",
    "\n",
    "\n",
    "# 4. Parse time points and outcome types\n",
    "df_parsed = df_all_long.copy()\n",
    "df_parsed['time_point'] = df_parsed['variable'].str.extract(r'(baseline|week_\\d+)')\n",
    "df_parsed['outcome_type'] = df_parsed['variable'].str.extract(r'(anxiety|depression)')\n",
    "\n",
    "\n",
    "# 5. Create summary statistics\n",
    "summary_stats = (df_parsed.groupby(['time_point', 'outcome_type'])['value']\n",
    "    .agg(['count', 'mean', 'std'])\n",
    "    .round(2))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Complex Reshaping Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Multi-level experimental design\n",
    "\n",
    "# Create complex experimental data\n",
    "complex_data = {\n",
    "    'lab_id': ['Lab_A', 'Lab_B', 'Lab_C'],\n",
    "    'researcher': ['Dr. Smith', 'Dr. Jones', 'Dr. Brown'],\n",
    "    # Condition 1 measurements\n",
    "    'cond1_trial1': [45.2, 43.1, 46.8],\n",
    "    'cond1_trial2': [44.8, 42.9, 47.1],\n",
    "    'cond1_trial3': [45.5, 43.3, 46.9],\n",
    "    # Condition 2 measurements\n",
    "    'cond2_trial1': [52.1, 51.3, 53.2],\n",
    "    'cond2_trial2': [51.8, 50.9, 52.8],\n",
    "    'cond2_trial3': [52.3, 51.1, 53.1],\n",
    "    # Quality metrics\n",
    "    'equipment_calibrated': [True, True, False],\n",
    "    'temperature': [22.1, 22.3, 21.8]\n",
    "}\n",
    "\n",
    "df_complex = pd.DataFrame(complex_data)\n",
    "\n",
    "# Identify measurement columns\n",
    "measurement_cols = [col for col in df_complex.columns if col.startswith('cond')]\n",
    "\n",
    "# Melt measurements while preserving metadata\n",
    "id_cols = ['lab_id', 'researcher', 'equipment_calibrated', 'temperature']\n",
    "df_melted = stx.pd.melt_cols(df_complex, measurement_cols, id_columns=id_cols)\n",
    "\n",
    "\n",
    "# Extract condition and trial information\n",
    "df_extracted = df_melted.copy()\n",
    "df_extracted['condition'] = df_extracted['variable'].str.extract(r'(cond\\d+)')\n",
    "df_extracted['trial'] = df_extracted['variable'].str.extract(r'(trial\\d+)')\n",
    "\n",
    "\n",
    "# Quality-filtered analysis\n",
    "df_calibrated = df_extracted[df_extracted['equipment_calibrated'] == True]\n",
    "quality_summary = (df_calibrated.groupby(['condition', 'trial'])['value']\n",
    "    .agg(['mean', 'std', 'count'])\n",
    "    .round(3))\n",
    "\n",
    "\n",
    "# Compare calibrated vs non-calibrated\n",
    "comparison = (df_extracted.groupby(['condition', 'equipment_calibrated'])['value']\n",
    "    .mean()\n",
    "    .unstack()\n",
    "    .round(3))\n",
    "comparison.columns = ['Not_Calibrated', 'Calibrated']\n",
    "comparison['Difference'] = comparison['Calibrated'] - comparison['Not_Calibrated']\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matrix Format Conversions\n",
    "\n",
    "Converting between wide matrix format and long x,y,z format for analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create correlation matrix\n",
    "np.random.seed(42)\n",
    "n_variables = 6\n",
    "variable_names = [f'var_{i+1}' for i in range(n_variables)]\n",
    "\n",
    "# Generate correlated data\n",
    "base_data = np.random.randn(200, n_variables)\n",
    "# Add some correlations\n",
    "base_data[:, 1] = 0.7 * base_data[:, 0] + 0.3 * base_data[:, 1]  # var_2 correlated with var_1\n",
    "base_data[:, 3] = -0.5 * base_data[:, 2] + 0.5 * base_data[:, 3]  # var_4 anti-correlated with var_3\n",
    "\n",
    "df_vars = pd.DataFrame(base_data, columns=variable_names)\n",
    "corr_matrix = df_vars.corr()\n",
    "\n",
    "\n",
    "# Convert to x,y,z (long) format\n",
    "xyz_data = stx.pd.to_xyz(corr_matrix)\n",
    "\n",
    "# Analyze correlation patterns\n",
    "\n",
    "# Find strongest correlations (excluding diagonal)\n",
    "off_diagonal = xyz_data[xyz_data['x'] != xyz_data['y']]\n",
    "strongest_positive = off_diagonal.loc[off_diagonal['z'].idxmax()]\n",
    "strongest_negative = off_diagonal.loc[off_diagonal['z'].idxmin()]\n",
    "\n",
    "\n",
    "# Create distance matrix and convert\n",
    "distance_matrix = 1 - np.abs(corr_matrix)  # Distance as 1 - |correlation|\n",
    "distance_xyz = stx.pd.to_xyz(distance_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize both formats\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Correlation heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "    square=True, ax=axes[0,0], cbar_kws={'label': 'Correlation'})\n",
    "axes[0,0].set_title('Correlation Matrix (Wide Format)')\n",
    "\n",
    "# 2. Correlation scatter plot from long format\n",
    "scatter1 = axes[0,1].scatter(xyz_data['x'], xyz_data['y'], c=xyz_data['z'], \n",
    "    cmap='RdBu_r', s=100, vmin=-1, vmax=1)\n",
    "axes[0,1].set_xlabel('Variable 1')\n",
    "axes[0,1].set_ylabel('Variable 2')\n",
    "axes[0,1].set_title('Correlation Values (Long Format)')\n",
    "axes[0,1].set_xticks(range(len(variable_names)))\n",
    "axes[0,1].set_xticklabels(variable_names, rotation=45)\n",
    "axes[0,1].set_yticks(range(len(variable_names)))\n",
    "axes[0,1].set_yticklabels(variable_names)\n",
    "axes[0,1].invert_yaxis()\n",
    "plt.colorbar(scatter1, ax=axes[0,1], label='Correlation')\n",
    "\n",
    "# 3. Distance heatmap\n",
    "sns.heatmap(distance_matrix, annot=True, fmt='.2f', cmap='viridis',\n",
    "    square=True, ax=axes[1,0], cbar_kws={'label': 'Distance'})\n",
    "axes[1,0].set_title('Distance Matrix (1 - |correlation|)')\n",
    "\n",
    "# 4. Correlation distribution\n",
    "axes[1,1].hist(off_diagonal['z'], bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1,1].axvline(0, color='red', linestyle='--', alpha=0.7, label='Zero correlation')\n",
    "axes[1,1].set_xlabel('Correlation Value')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].set_title('Distribution of Correlations')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Type Utilities\n",
    "\n",
    "Safe conversion of DataFrame columns to appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create DataFrame with mixed and problematic data types\n",
    "mixed_data = {\n",
    "    'integers_as_strings': ['1', '2', '3', '4', '5'],\n",
    "    'floats_as_strings': ['1.5', '2.7', '3.14', '4.0', '5.5'],\n",
    "    'mixed_with_errors': ['10', '20.5', 'invalid', '40', 'bad_data'],\n",
    "    'percentages': ['10%', '25%', '50%', '75%', '100%'],\n",
    "    'currency': ['$100', '$250.50', '$1,000', '$500', '$750.25'],\n",
    "    'already_numeric': [1, 2, 3, 4, 5],\n",
    "    'scientific_notation': ['1e2', '2.5e3', '1.2e-1', '5e0', '3.14e1'],\n",
    "    'text_data': ['apple', 'banana', 'cherry', 'date', 'elderberry']\n",
    "}\n",
    "\n",
    "df_mixed = pd.DataFrame(mixed_data)\n",
    "\n",
    "# Convert to numeric using SciTeX\n",
    "df_numeric = stx.pd.to_numeric(df_mixed)\n",
    "\n",
    "# Analyze conversion results\n",
    "for col in df_mixed.columns:\n",
    "    original_type = df_mixed[col].dtype\n",
    "    converted_type = df_numeric[col].dtype\n",
    "    \n",
    "    if original_type != converted_type:\n",
    "        # Count successful conversions\n",
    "        if converted_type in ['int64', 'float64']:\n",
    "            non_null = df_numeric[col].notna().sum()\n",
    "            total = len(df_numeric[col])\n",
    "            success_rate = non_null / total * 100\n",
    "        else:\n",
    "            pass  # Fixed incomplete block\n",
    "    else:\n",
    "        pass  # Fixed incomplete block\n",
    "\n",
    "# Show which values couldn't be converted\n",
    "for col in df_mixed.columns:\n",
    "    if df_numeric[col].dtype in ['int64', 'float64']:\n",
    "        failed_mask = df_numeric[col].isna() & df_mixed[col].notna()\n",
    "        if failed_mask.any():\n",
    "            failed_values = df_mixed.loc[failed_mask, col].tolist()\n",
    "\n",
    "# Demonstrate safe numeric operations\n",
    "numeric_cols = df_numeric.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    values = df_numeric[col].dropna()\n",
    "    if len(values) > 0:\n",
    "        # Condition met\n",
    "\n",
    "# Create summary statistics\n",
    "numeric_summary = df_numeric.select_dtypes(include=[np.number]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Operations\n",
    "\n",
    "Additional utilities for DataFrame manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Warning suppression utility\n",
    "\n",
    "# Create DataFrame that might trigger SettingWithCopyWarning\n",
    "df_original = pd.DataFrame({\n",
    "    'A': range(10),\n",
    "    'B': range(10, 20),\n",
    "    'C': range(20, 30)\n",
    "})\n",
    "\n",
    "# This operation might normally trigger a warning\n",
    "df_subset = df_original[df_original['A'] > 5]\n",
    "\n",
    "\n",
    "# Without warning suppression (might show warning)\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    df_subset.loc[:, 'D'] = df_subset['A'] * df_subset['B']\n",
    "    if w:\n",
    "        # Condition met\n",
    "    else:\n",
    "        pass  # Fixed incomplete block\n",
    "\n",
    "# With warning suppression\n",
    "with stx.pd.ignore_SettingWithCopyWarning():\n",
    "    df_subset.loc[:, 'E'] = df_subset['B'] + df_subset['C']\n",
    "\n",
    "\n",
    "# 2. Column merging utility\n",
    "\n",
    "# Create data with columns to merge\n",
    "personal_data = {\n",
    "    'first_name': ['John', 'Jane', 'Bob', 'Alice'],\n",
    "    'middle_initial': ['A', 'B', None, 'C'],\n",
    "    'last_name': ['Doe', 'Smith', 'Johnson', 'Brown'],\n",
    "    'street': ['123 Main St', '456 Oak Ave', '789 Pine Rd', '321 Elm St'],\n",
    "    'city': ['New York', 'London', 'Paris', 'Tokyo'],\n",
    "    'country': ['USA', 'UK', 'France', 'Japan'],\n",
    "    'age': [30, 25, 35, 28]\n",
    "}\n",
    "\n",
    "df_personal = pd.DataFrame(personal_data)\n",
    "\n",
    "# Merge name columns (handling missing middle initial)\n",
    "df_merged = df_personal.copy()\n",
    "\n",
    "# Custom merge function for names with optional middle initial\n",
    "def merge_names(row):\n",
    "    parts = [row['first_name']]\n",
    "    if pd.notna(row['middle_initial']):\n",
    "        parts.append(row['middle_initial'] + '.')\n",
    "    parts.append(row['last_name'])\n",
    "    return ' '.join(parts)\n",
    "\n",
    "df_merged['full_name'] = df_merged.apply(merge_names, axis=1)\n",
    "\n",
    "# Merge address columns\n",
    "df_merged['full_address'] = (df_merged['street'] + ', ' + \n",
    "    df_merged['city'] + ', ' +\n",
    "    df_merged['country'])\n",
    "\n",
    "# Keep only essential columns\n",
    "df_final = df_merged[['full_name', 'full_address', 'age']]\n",
    "\n",
    "\n",
    "# 3. Advanced slicing operations\n",
    "\n",
    "# Create time series data\n",
    "dates = pd.date_range('2024-01-01', periods=50, freq='D')\n",
    "ts_data = {\n",
    "    'date': dates,\n",
    "    'temperature': 20 + 5 * np.sin(np.arange(50) * 2 * np.pi / 30) + np.random.randn(50),\n",
    "    'humidity': 60 + 10 * np.cos(np.arange(50) * 2 * np.pi / 25) + np.random.randn(50) * 2,\n",
    "    'pressure': 1013 + np.random.randn(50) * 5\n",
    "}\n",
    "\n",
    "df_ts = pd.DataFrame(ts_data)\n",
    "df_ts.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "# Slice by date range\n",
    "start_date = '2024-01-15'\n",
    "end_date = '2024-01-25'\n",
    "date_slice = df_ts[start_date:end_date]\n",
    "\n",
    "\n",
    "# Slice by condition\n",
    "high_temp = df_ts[df_ts['temperature'] > df_ts['temperature'].mean()]\n",
    "\n",
    "# Complex filtering\n",
    "complex_filter = df_ts[\n",
    "    (df_ts['temperature'] > 22) & \n",
    "    (df_ts['humidity'] < 65) & \n",
    "    (df_ts['pressure'] > 1010)\n",
    "]\n",
    "\n",
    "if len(complex_filter) > 0:\n",
    "    f\"humidity={complex_filter['humidity'].mean():.1f}, \"\n",
    "    f\"pressure={complex_filter['pressure'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-World Applications\n",
    "\n",
    "Practical examples showing how the pandas utilities work in real data science scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate clinical trial data with realistic challenges\n",
    "np.random.seed(42)\n",
    "n_patients = 100\n",
    "\n",
    "# Generate patient data\n",
    "patients = [f'PT_{i:03d}' for i in range(1, n_patients + 1)]\n",
    "treatment_groups = np.random.choice(['Placebo', 'Drug_A', 'Drug_B'], n_patients)\n",
    "ages = np.random.normal(55, 15, n_patients).astype(int)\n",
    "ages = np.clip(ages, 18, 85)  # Realistic age range\n",
    "\n",
    "# Create unequal measurement schedules (real-world missing data)\n",
    "baseline_scores = np.random.normal(100, 15, n_patients)\n",
    "\n",
    "# Week 4 - some patients drop out\n",
    "week4_mask = np.random.random(n_patients) > 0.1  # 10% dropout\n",
    "week4_scores = np.where(week4_mask, \n",
    "    baseline_scores + np.random.normal(-5, 8, n_patients),\n",
    "    np.nan)\n",
    "\n",
    "# Week 8 - more dropouts\n",
    "week8_mask = week4_mask & (np.random.random(n_patients) > 0.15)  # Additional 15% dropout\n",
    "week8_scores = np.where(week8_mask,\n",
    "    baseline_scores + np.random.normal(-8, 10, n_patients),\n",
    "    np.nan)\n",
    "\n",
    "# Week 12 - final measurements (further dropouts)\n",
    "week12_mask = week8_mask & (np.random.random(n_patients) > 0.2)  # Additional 20% dropout\n",
    "week12_scores = np.where(week12_mask,\n",
    "    baseline_scores + np.random.normal(-12, 12, n_patients),\n",
    "    np.nan)\n",
    "\n",
    "# Create the raw data dictionary (unequal lengths due to dropouts)\n",
    "trial_data = {\n",
    "    'patient_id': patients,\n",
    "    'treatment_group': treatment_groups,\n",
    "    'age': ages,\n",
    "    'baseline_score': baseline_scores,\n",
    "    'week_4_score': week4_scores,\n",
    "    'week_8_score': week8_scores,\n",
    "    'week_12_score': week12_scores\n",
    "}\n",
    "\n",
    "# Convert to DataFrame using force_df\n",
    "df_trial = stx.pd.force_df(trial_data)\n",
    "\n",
    "\n",
    "# Analyze dropout patterns\n",
    "score_cols = ['baseline_score', 'week_4_score', 'week_8_score', 'week_12_score']\n",
    "dropout_analysis = {}\n",
    "\n",
    "for col in score_cols:\n",
    "    available = df_trial[col].notna().sum()\n",
    "    dropout_analysis[col] = {\n",
    "    'available': available,\n",
    "    'missing': len(df_trial) - available,\n",
    "    'retention_rate': available / len(df_trial) * 100\n",
    "    }\n",
    "\n",
    "for timepoint, stats in dropout_analysis.items():\n",
    "    f\"{stats['missing']:2d} missing ({stats['retention_rate']:5.1f}% retention)\")\n",
    "\n",
    "# Reshape data for longitudinal analysis\n",
    "df_long = stx.pd.melt_cols(df_trial, score_cols, \n",
    "    id_columns=['patient_id', 'treatment_group', 'age'])\n",
    "\n",
    "# Extract timepoint information\n",
    "df_long['timepoint'] = df_long['variable'].str.replace('_score', '')\n",
    "df_long['weeks'] = df_long['timepoint'].map({\n",
    "    'baseline': 0,\n",
    "    'week_4': 4,\n",
    "    'week_8': 8, \n",
    "    'week_12': 12\n",
    "})\n",
    "\n",
    "\n",
    "# Statistical analysis by treatment group\n",
    "treatment_stats = (df_long.groupby(['treatment_group', 'timepoint'])['value']\n",
    "    .agg(['count', 'mean', 'std'])\n",
    "    .round(2))\n",
    "\n",
    "\n",
    "# Calculate change from baseline\n",
    "baseline_values = df_trial.set_index('patient_id')['baseline_score']\n",
    "\n",
    "change_data = []\n",
    "for _, row in df_trial.iterrows():\n",
    "    patient_baseline = row['baseline_score']\n",
    "    for col in ['week_4_score', 'week_8_score', 'week_12_score']:\n",
    "        if pd.notna(row[col]):\n",
    "            change_data.append({\n",
    "            'patient_id': row['patient_id'],\n",
    "            'treatment_group': row['treatment_group'],\n",
    "            'timepoint': col.replace('_score', ''),\n",
    "            'change_from_baseline': row[col] - patient_baseline\n",
    "            })\n",
    "\n",
    "df_changes = pd.DataFrame(change_data)\n",
    "\n",
    "change_summary = (df_changes.groupby(['treatment_group', 'timepoint'])['change_from_baseline']\n",
    "    .agg(['count', 'mean', 'std'])\n",
    "    .round(2))\n",
    "\n",
    "# Statistical testing\n",
    "week12_changes = df_changes[df_changes['timepoint'] == 'week_12']\n",
    "\n",
    "if len(week12_changes) > 0:\n",
    "    groups = week12_changes['treatment_group'].unique()\n",
    "    group_data = {}\n",
    "    \n",
    "    for group in groups:\n",
    "        group_data[group] = week12_changes[week12_changes['treatment_group'] == group]['change_from_baseline']\n",
    "    \n",
    "    # Pairwise comparisons\n",
    "    results = []\n",
    "    for i, group1 in enumerate(groups):\n",
    "        for group2 in groups[i+1:]:\n",
    "            data1 = group_data[group1].dropna()\n",
    "            data2 = group_data[group2].dropna()\n",
    "            \n",
    "            if len(data1) > 1 and len(data2) > 1:\n",
    "                t_stat, p_val = stats.ttest_ind(data1, data2)\n",
    "                results.append({\n",
    "                'comparison': f'{group1}_vs_{group2}',\n",
    "                'mean_diff': data1.mean() - data2.mean(),\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_val\n",
    "                })\n",
    "    \n",
    "    if results:\n",
    "        df_stats_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Move p-value column to front for emphasis\n",
    "        df_stats_organized = stx.pd.mv_to_first(df_stats_results, 'p_value')\n",
    "        \n",
    "        \n",
    "        # Find significant results\n",
    "        significant = df_stats_organized[df_stats_organized['p_value'] < 0.05]\n",
    "        \n",
    "        if len(significant) > 0:\n",
    "            # Condition met\n",
    "else:\n",
    "    pass  # Fixed incomplete block\n",
    "\n",
    "# Visualize the results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Retention rates\n",
    "timepoints = list(dropout_analysis.keys())\n",
    "retention_rates = [dropout_analysis[tp]['retention_rate'] for tp in timepoints]\n",
    "axes[0,0].plot(range(len(timepoints)), retention_rates, 'o-', linewidth=2, markersize=8)\n",
    "axes[0,0].set_xticks(range(len(timepoints)))\n",
    "axes[0,0].set_xticklabels([tp.replace('_', ' ').title() for tp in timepoints], rotation=45)\n",
    "axes[0,0].set_ylabel('Retention Rate (%)')\n",
    "axes[0,0].set_title('Patient Retention Over Time')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].set_ylim(0, 105)\n",
    "\n",
    "# 2. Treatment group scores over time\n",
    "for group in df_trial['treatment_group'].unique():\n",
    "    group_data = df_long[df_long['treatment_group'] == group]\n",
    "    group_means = group_data.groupby('weeks')['value'].mean()\n",
    "    axes[0,1].plot(group_means.index, group_means.values, 'o-', label=group, linewidth=2)\n",
    "\n",
    "axes[0,1].set_xlabel('Weeks')\n",
    "axes[0,1].set_ylabel('Score')\n",
    "axes[0,1].set_title('Treatment Group Trajectories')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Change from baseline distribution\n",
    "if len(df_changes) > 0:\n",
    "    for group in df_changes['treatment_group'].unique():\n",
    "        group_changes = df_changes[df_changes['treatment_group'] == group]['change_from_baseline']\n",
    "        axes[1,0].hist(group_changes, alpha=0.6, label=group, bins=15, edgecolor='black')\n",
    "    \n",
    "    axes[1,0].set_xlabel('Change from Baseline')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    axes[1,0].set_title('Distribution of Changes from Baseline')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Age distribution by treatment group\n",
    "df_trial.boxplot(column='age', by='treatment_group', ax=axes[1,1])\n",
    "axes[1,1].set_title('Age Distribution by Treatment Group')\n",
    "axes[1,1].set_xlabel('Treatment Group')\n",
    "axes[1,1].set_ylabel('Age (years)')\n",
    "axes[1,1].get_figure().suptitle('')  # Remove automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete Data Pipeline\n",
    "\n",
    "A comprehensive example showing all pandas utilities working together in a complete data processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class AdvancedDataProcessor:\n",
    "    \"\"\"Complete data processing pipeline using SciTeX pandas utilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"DataProcessor\"):\n",
    "        self.name = name\n",
    "        self.data = None\n",
    "        self.processed = None\n",
    "        self.steps_performed = []\n",
    "        \n",
    "    def load_data(self, data_source, filler=np.nan):\n",
    "        \"\"\"Load data from any source using force_df.\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.data = stx.pd.force_df(data_source, filler=filler)\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        self.steps_performed.append({\n",
    "        'step': 'load_data',\n",
    "        'time': load_time,\n",
    "        'shape': self.data.shape,\n",
    "        'details': f'Loaded with filler={filler}'\n",
    "        })\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def convert_numeric(self):\n",
    "        \"\"\"Convert columns to numeric where possible.\"\"\"\n",
    "        start_time = time.time()\n",
    "        original_types = self.data.dtypes.to_dict()\n",
    "        \n",
    "        self.data = stx.pd.to_numeric(self.data)\n",
    "        conversion_time = time.time() - start_time\n",
    "        \n",
    "        new_types = self.data.dtypes.to_dict()\n",
    "        changed_cols = [col for col in original_types.keys() \n",
    "        if original_types[col] != new_types[col]]\n",
    "        \n",
    "        self.steps_performed.append({\n",
    "        'step': 'convert_numeric',\n",
    "        'time': conversion_time,\n",
    "        'details': f'Converted {len(changed_cols)} columns to numeric'\n",
    "        })\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def organize_statistics(self):\n",
    "        \"\"\"Find and organize statistical columns.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Find p-value columns\n",
    "        pval_cols = stx.pd.find_pval(self.data, multiple=True)\n",
    "        \n",
    "        if pval_cols:\n",
    "            # Move first p-value column to front\n",
    "            self.data = stx.pd.mv_to_first(self.data, pval_cols[0])\n",
    "            \n",
    "            # Move other statistical columns forward\n",
    "            stat_keywords = ['effect', 'statistic', 'confidence', 'significant']\n",
    "            stat_cols = [col for col in self.data.columns \n",
    "            if any(keyword in col.lower() for keyword in stat_keywords)]\n",
    "            \n",
    "            for col in stat_cols:\n",
    "                if col != pval_cols[0]:  # Don't move p-value again\n",
    "                try:\n",
    "                    self.data = stx.pd.mv(self.data, col, len(pval_cols))\n",
    "                    except:\n",
    "                        pass  # Column might not exist or already moved\n",
    "        \n",
    "        organize_time = time.time() - start_time\n",
    "        \n",
    "        self.steps_performed.append({\n",
    "            'step': 'organize_statistics',\n",
    "            'time': organize_time,\n",
    "            'details': f'Found {len(pval_cols)} p-value columns, organized statistical columns'\n",
    "        })\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def reshape_longitudinal(self, value_vars, id_vars=None, time_var='time'):\n",
    "        \"\"\"Reshape data for longitudinal analysis.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if id_vars is None:\n",
    "            # Auto-detect ID variables (non-numeric or explicitly named)\n",
    "            id_candidates = []\n",
    "            for col in self.data.columns:\n",
    "                if (col.lower() in ['id', 'subject', 'patient', 'participant'] or \n",
    "                'id' in col.lower() or\n",
    "                self.data[col].dtype == 'object'):\n",
    "                id_candidates.append(col)\n",
    "            \n",
    "            # Remove value_vars from id_candidates\n",
    "            id_vars = [col for col in id_candidates if col not in value_vars]\n",
    "        \n",
    "        self.processed = stx.pd.melt_cols(self.data, value_vars, id_columns=id_vars)\n",
    "        \n",
    "        # Extract time information if variable names contain time indicators\n",
    "        if any(any(indicator in var.lower() for indicator in ['time', 'week', 'day', 'month', 'visit']) \n",
    "            for var in value_vars):\n",
    "                # Process var\n",
    "            self.processed[time_var] = (self.processed['variable']\n",
    "                .str.extract(r'(\\d+)')\n",
    "                .astype(float))\n",
    "        \n",
    "        reshape_time = time.time() - start_time\n",
    "        \n",
    "        self.steps_performed.append({\n",
    "            'step': 'reshape_longitudinal',\n",
    "            'time': reshape_time,\n",
    "            'details': f'Melted {len(value_vars)} columns with {len(id_vars)} ID variables'\n",
    "        })\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_matrix(self, index_col, columns_col, values_col, aggfunc='mean'):\n",
    "        \"\"\"Create summary matrix and convert to long format.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.processed is None:\n",
    "            data_to_use = self.data\n",
    "        else:\n",
    "            data_to_use = self.processed\n",
    "        \n",
    "        # Create pivot table\n",
    "        matrix = data_to_use.pivot_table(\n",
    "            index=index_col,\n",
    "            columns=columns_col,\n",
    "            values=values_col,\n",
    "            aggfunc=aggfunc\n",
    "        )\n",
    "        \n",
    "        # Convert to xyz format\n",
    "        xyz_data = stx.pd.to_xyz(matrix)\n",
    "        \n",
    "        matrix_time = time.time() - start_time\n",
    "        \n",
    "        self.steps_performed.append({\n",
    "            'step': 'create_summary_matrix',\n",
    "            'time': matrix_time,\n",
    "            'details': f'Created {matrix.shape} matrix, converted to {xyz_data.shape} long format'\n",
    "        })\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            'matrix': matrix,\n",
    "            'long_format': xyz_data\n",
    "        }\n",
    "    \n",
    "    def get_results(self):\n",
    "        \"\"\"Get processed data and pipeline summary.\"\"\"\n",
    "        total_time = sum(step['time'] for step in self.steps_performed)\n",
    "        \n",
    "        return {\n",
    "        'data': self.processed if self.processed is not None else self.data,\n",
    "        'steps': self.steps_performed,\n",
    "        'total_time': total_time,\n",
    "        'pipeline_summary': self._create_summary()\n",
    "        }\n",
    "    \n",
    "    def _create_summary(self):\n",
    "        \"\"\"Create pipeline execution summary.\"\"\"\n",
    "        return {\n",
    "        'processor_name': self.name,\n",
    "        'steps_completed': len(self.steps_performed),\n",
    "        'total_time': sum(step['time'] for step in self.steps_performed),\n",
    "        'final_shape': (self.processed if self.processed is not None else self.data).shape,\n",
    "        'step_details': [\n",
    "        f\"{step['step']}: {step['time']:.3f}s - {step.get('details', '')}\"\n",
    "        for step in self.steps_performed\n",
    "        ]\n",
    "        }\n",
    "\n",
    "# Example usage: Complex multi-source data processing\n",
    "\n",
    "# Create complex, realistic dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate multi-site study data with various challenges\n",
    "sites = ['Site_A', 'Site_B', 'Site_C']\n",
    "n_subjects_per_site = [50, 45, 55]  # Unequal site sizes\n",
    "\n",
    "all_data = []\n",
    "subject_counter = 1\n",
    "\n",
    "for site, n_subjects in zip(sites, n_subjects_per_site):\n",
    "    for i in range(n_subjects):\n",
    "        subject_id = f'{site}_S{subject_counter:03d}'\n",
    "        \n",
    "        # Baseline characteristics\n",
    "        age = np.random.normal(50, 12)\n",
    "        baseline_score = np.random.normal(100, 15)\n",
    "        \n",
    "        # Follow-up measurements with realistic dropout\n",
    "        measurements = {\n",
    "        'subject_id': subject_id,\n",
    "        'site': site,\n",
    "        'age': f'{age:.1f}',  # String to test conversion\n",
    "        'baseline_measurement': baseline_score,\n",
    "        }\n",
    "        \n",
    "        # Add follow-up measurements with increasing dropout\n",
    "        for week in [4, 8, 12, 24]:\n",
    "            dropout_prob = 0.05 + 0.02 * (week / 4)  # Increasing dropout over time\n",
    "            if np.random.random() > dropout_prob:\n",
    "                # Measurement with some improvement trend\n",
    "                improvement = -week * 0.5 + np.random.normal(0, 5)\n",
    "                measurements[f'week_{week}_measurement'] = baseline_score + improvement\n",
    "        \n",
    "        all_data.append(measurements)\n",
    "        subject_counter += 1\n",
    "\n",
    "# Add statistical results (simulated analysis outcomes)\n",
    "comparison_data = {\n",
    "    'comparison': ['Site_A_vs_Site_B', 'Site_A_vs_Site_C', 'Site_B_vs_Site_C'],\n",
    "    'effect_size': [0.23, 0.15, -0.08],\n",
    "    'p_value_unadjusted': [0.032, 0.156, 0.423],\n",
    "    'p_value_bonferroni': [0.096, 0.468, 1.000],\n",
    "    'confidence_interval_lower': [0.02, -0.05, -0.28],\n",
    "    'confidence_interval_upper': [0.44, 0.35, 0.12],\n",
    "    'significant_unadjusted': ['Yes', 'No', 'No']\n",
    "}\n",
    "\n",
    "# Process subject data\n",
    "processor_subjects = AdvancedDataProcessor(\"Subject_Data_Processor\")\n",
    "\n",
    "subject_results = (processor_subjects\n",
    "    .load_data(all_data, filler=np.nan)\n",
    "    .convert_numeric()\n",
    "    .get_results())\n",
    "\n",
    "for detail in subject_results['pipeline_summary']['step_details']:\n",
    "    # Process detail\n",
    "\n",
    "# Process comparison statistics\n",
    "processor_stats = AdvancedDataProcessor(\"Statistical_Results_Processor\")\n",
    "\n",
    "stats_results = (processor_stats\n",
    "    .load_data(comparison_data)\n",
    "    .convert_numeric()\n",
    "    .organize_statistics()\n",
    "    .get_results())\n",
    "\n",
    "for detail in stats_results['pipeline_summary']['step_details']:\n",
    "    # Process detail\n",
    "\n",
    "# Longitudinal analysis\n",
    "measurement_cols = [col for col in subject_results['data'].columns if 'measurement' in col]\n",
    "id_cols = ['subject_id', 'site', 'age']\n",
    "\n",
    "processor_longitudinal = AdvancedDataProcessor(\"Longitudinal_Processor\")\n",
    "\n",
    "longitudinal_results = (processor_longitudinal\n",
    "    .load_data(subject_results['data'])\n",
    "    .reshape_longitudinal(measurement_cols, id_cols)\n",
    "    .get_results())\n",
    "\n",
    "# Create site comparison matrix\n",
    "matrix_results = processor_longitudinal.create_summary_matrix(\n",
    "    index_col='site',\n",
    "    columns_col='variable', \n",
    "    values_col='value'\n",
    ")\n",
    "\n",
    "for detail in longitudinal_results['pipeline_summary']['step_details']:\n",
    "    # Process detail\n",
    "\n",
    "# Final summary\n",
    "\n",
    "# Show key results\n",
    "\n",
    "# Show p-value organization\n",
    "pval_col = stx.pd.find_pval(stats_results['data'])\n",
    "if pval_col:\n",
    "    significant_count = (stats_results['data'][pval_col] < 0.05).sum()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive tutorial has demonstrated the full capabilities of the SciTeX pandas module:\n",
    "\n",
    "### Key Features Covered:\n",
    "1. **Universal DataFrame Conversion** - `force_df` handles any data type with intelligent filling strategies\n",
    "2. **Statistical Column Detection** - Automatic identification and organization of p-value columns\n",
    "3. **Data Organization** - Flexible column and row movement for optimal data presentation\n",
    "4. **Advanced Reshaping** - Selective melting and longitudinal data transformation\n",
    "5. **Matrix Conversions** - Seamless conversion between wide and long formats for analysis\n",
    "6. **Type Safety** - Robust numeric conversion with error handling\n",
    "7. **Workflow Integration** - Complete data processing pipelines with performance tracking\n",
    "\n",
    "### Best Practices Demonstrated:\n",
    "- **Always use `force_df`** for consistent DataFrame creation from any data source\n",
    "- **Organize statistical results** by moving p-value columns to prominent positions\n",
    "- **Leverage selective melting** to preserve data relationships during reshaping\n",
    "- **Convert formats strategically** between wide and long layouts for specific analyses\n",
    "- **Handle missing data gracefully** with appropriate filler strategies\n",
    "- **Chain operations efficiently** using method chaining for readable pipelines\n",
    "\n",
    "### Performance Benefits:\n",
    "- **Robust data loading** handles inconsistent input formats automatically\n",
    "- **Intelligent type conversion** preserves data integrity while enabling numeric operations\n",
    "- **Efficient reshaping** maintains data relationships during complex transformations\n",
    "- **Streamlined workflows** reduce boilerplate code for common data science tasks\n",
    "\n",
    "The SciTeX pandas module transforms complex data manipulation tasks into simple, reliable operations that scale from quick analyses to production data pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}