{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX I/O Operations\n",
    "\n",
    "This notebook demonstrates advanced I/O features in SciTeX, including:\n",
    "- Multiple format support\n",
    "- Compression options\n",
    "- Caching mechanisms\n",
    "- Batch operations\n",
    "- HDF5 hierarchical storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scitex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"io_demo\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"Working directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Format Auto-Detection\n",
    "\n",
    "SciTeX automatically detects the appropriate format based on file extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "sample_data = {\n",
    "    'array': np.random.randn(100, 50),\n",
    "    'dataframe': pd.DataFrame({\n",
    "        'x': np.random.randn(1000),\n",
    "        'y': np.random.randn(1000),\n",
    "        'category': np.random.choice(['A', 'B', 'C'], 1000)\n",
    "    }),\n",
    "    'metadata': {\n",
    "        'experiment': 'io_demo',\n",
    "        'timestamp': time.time(),\n",
    "        'parameters': {'alpha': 0.5, 'beta': 1.0}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test different formats\n",
    "formats = {\n",
    "    'pkl': sample_data,  # Pickle - best for complex Python objects\n",
    "    'json': sample_data['metadata'],  # JSON - human readable, simple data\n",
    "    'npy': sample_data['array'],  # NumPy - efficient for arrays\n",
    "    'csv': sample_data['dataframe'],  # CSV - tabular data\n",
    "    'h5': sample_data  # HDF5 - hierarchical data\n",
    "}\n",
    "\n",
    "print(\"Saving in different formats:\")\n",
    "for ext, data in formats.items():\n",
    "    filepath = output_dir / f\"sample.{ext}\"\n",
    "    scitex.io.save(data, filepath)\n",
    "    size_kb = filepath.stat().st_size / 1024\n",
    "    print(f\"  {ext:4} - {size_kb:6.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compression Support\n",
    "\n",
    "SciTeX supports automatic compression for space-efficient storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large dataset for compression demo\n",
    "large_data = np.random.randn(10000, 100)\n",
    "print(f\"Data size in memory: {large_data.nbytes / (1024**2):.1f} MB\")\n",
    "\n",
    "# Save with different compression levels\n",
    "compression_results = {}\n",
    "\n",
    "# Uncompressed\n",
    "uncompressed_path = output_dir / \"large_data.npy\"\n",
    "scitex.io.save(large_data, uncompressed_path)\n",
    "uncompressed_size = uncompressed_path.stat().st_size / (1024**2)\n",
    "compression_results['none'] = uncompressed_size\n",
    "\n",
    "# Compressed pickle\n",
    "compressed_path = output_dir / \"large_data_compressed.pkl.gz\"\n",
    "scitex.io.save(large_data, compressed_path)\n",
    "compressed_size = compressed_path.stat().st_size / (1024**2)\n",
    "compression_results['gzip'] = compressed_size\n",
    "\n",
    "# Display results\n",
    "print(\"\\nCompression results:\")\n",
    "for method, size in compression_results.items():\n",
    "    ratio = uncompressed_size / size if size > 0 else 0\n",
    "    print(f\"  {method:12} - {size:6.2f} MB (ratio: {ratio:.1f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Caching for Performance\n",
    "\n",
    "SciTeX provides caching to speed up repeated operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator-based caching for expensive computations\n",
    "@scitex.decorators.cache_disk\n",
    "def expensive_computation(n_samples=1000, n_features=50):\n",
    "    \"\"\"Simulate an expensive computation.\"\"\"\n",
    "    print(f\"Computing with n_samples={n_samples}, n_features={n_features}...\")\n",
    "    time.sleep(1)  # Simulate computation time\n",
    "    \n",
    "    data = np.random.randn(n_samples, n_features)\n",
    "    result = {\n",
    "        'mean': data.mean(axis=0),\n",
    "        'std': data.std(axis=0),\n",
    "        'correlation': np.corrcoef(data.T)\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# First call - will compute\n",
    "print(\"First call (computing):\")\n",
    "start = time.time()\n",
    "result1 = expensive_computation(500, 20)\n",
    "time1 = time.time() - start\n",
    "print(f\"  Time: {time1:.3f} seconds\")\n",
    "\n",
    "# Second call - will load from cache\n",
    "print(\"\\nSecond call (from cache):\")\n",
    "start = time.time()\n",
    "result2 = expensive_computation(500, 20)\n",
    "time2 = time.time() - start\n",
    "print(f\"  Time: {time2:.3f} seconds\")\n",
    "print(f\"  Speedup: {time1/time2:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Operations\n",
    "\n",
    "Process multiple files efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch of files\n",
    "batch_dir = output_dir / \"batch\"\n",
    "batch_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Creating batch files...\")\n",
    "for i in range(5):\n",
    "    data = {\n",
    "        'id': i,\n",
    "        'values': np.random.randn(100),\n",
    "        'timestamp': time.time() + i\n",
    "    }\n",
    "    scitex.io.save(data, batch_dir / f\"data_{i:03d}.pkl\")\n",
    "\n",
    "# Load all files in batch\n",
    "print(\"\\nLoading batch files...\")\n",
    "batch_data = []\n",
    "for file in sorted(batch_dir.glob(\"data_*.pkl\")):\n",
    "    data = scitex.io.load(file)\n",
    "    batch_data.append(data)\n",
    "    print(f\"  Loaded {file.name} - ID: {data['id']}\")\n",
    "\n",
    "# Combine results\n",
    "all_values = np.vstack([d['values'] for d in batch_data])\n",
    "print(f\"\\nCombined data shape: {all_values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HDF5 for Hierarchical Data\n",
    "\n",
    "HDF5 is perfect for storing complex, hierarchical scientific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hierarchical experiment data\n",
    "experiment_data = {\n",
    "    'metadata': {\n",
    "        'name': 'Multi-condition experiment',\n",
    "        'date': '2025-01-25',\n",
    "        'researcher': 'SciTeX User'\n",
    "    },\n",
    "    'conditions': {\n",
    "        'control': {\n",
    "            'raw_data': np.random.randn(1000, 50),\n",
    "            'processed': np.random.randn(1000, 10),\n",
    "            'parameters': {'temperature': 20, 'pressure': 1.0}\n",
    "        },\n",
    "        'treatment_A': {\n",
    "            'raw_data': np.random.randn(1000, 50),\n",
    "            'processed': np.random.randn(1000, 10),\n",
    "            'parameters': {'temperature': 25, 'pressure': 1.2}\n",
    "        },\n",
    "        'treatment_B': {\n",
    "            'raw_data': np.random.randn(1000, 50),\n",
    "            'processed': np.random.randn(1000, 10),\n",
    "            'parameters': {'temperature': 30, 'pressure': 1.5}\n",
    "        }\n",
    "    },\n",
    "    'analysis': {\n",
    "        'summary_stats': pd.DataFrame({\n",
    "            'condition': ['control', 'treatment_A', 'treatment_B'],\n",
    "            'mean_response': [0.1, 0.5, 0.8],\n",
    "            'std_response': [0.05, 0.08, 0.12]\n",
    "        })\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as HDF5\n",
    "h5_path = output_dir / \"experiment.h5\"\n",
    "scitex.io.save(experiment_data, h5_path)\n",
    "print(f\"Saved hierarchical data to {h5_path}\")\n",
    "print(f\"File size: {h5_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "# Load and explore\n",
    "loaded_exp = scitex.io.load(h5_path)\n",
    "print(\"\\nLoaded structure:\")\n",
    "for key in loaded_exp.keys():\n",
    "    print(f\"  {key}/\")\n",
    "    if isinstance(loaded_exp[key], dict):\n",
    "        for subkey in loaded_exp[key].keys():\n",
    "            print(f\"    {subkey}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format selection guide\n",
    "format_guide = pd.DataFrame({\n",
    "    'Format': ['pickle', 'JSON', 'CSV', 'NumPy', 'HDF5'],\n",
    "    'Extension': ['.pkl', '.json', '.csv', '.npy', '.h5'],\n",
    "    'Best For': [\n",
    "        'Complex Python objects',\n",
    "        'Configuration, metadata',\n",
    "        'Tabular data, sharing',\n",
    "        'Numeric arrays',\n",
    "        'Large hierarchical data'\n",
    "    ],\n",
    "    'Human Readable': ['No', 'Yes', 'Yes', 'No', 'No'],\n",
    "    'Compression': ['Yes', 'Text', 'Text', 'No', 'Yes']\n",
    "})\n",
    "\n",
    "print(\"SciTeX I/O Format Guide:\")\n",
    "print(format_guide.to_string(index=False))\n",
    "\n",
    "# Performance tips\n",
    "print(\"\\n\\nPerformance Tips:\")\n",
    "print(\"1. Use caching (@cache_disk) for expensive computations\")\n",
    "print(\"2. Use HDF5 for large hierarchical datasets\")\n",
    "print(\"3. Enable compression for large files (add .gz extension)\")\n",
    "print(\"4. Use batch operations for processing multiple files\")\n",
    "print(\"5. Choose the right format for your data type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up demo files\n",
    "import shutil\n",
    "\n",
    "if output_dir.exists():\n",
    "    shutil.rmtree(output_dir)\n",
    "    print(f\"âœ“ Cleaned up {output_dir}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ I/O operations demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}