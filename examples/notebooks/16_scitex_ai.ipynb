{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX AI Module - Comprehensive Tutorial\n",
    "\n",
    "This comprehensive notebook demonstrates the full capabilities of the `scitex.ai` module, combining the best features from multiple tutorials into one complete guide.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "### ðŸ¤– **Generative AI**\n",
    "- Multi-provider support (OpenAI, Anthropic, Google, Groq, DeepSeek, Perplexity, Local models)\n",
    "- Cost tracking and token counting\n",
    "- Chat history management\n",
    "- Multi-modal capabilities (text + images)\n",
    "\n",
    "### ðŸ“Š **Machine Learning**\n",
    "- Comprehensive classification reporting\n",
    "- Unified scikit-learn classifier interface\n",
    "- Training utilities (early stopping, learning curves)\n",
    "- Feature selection and importance analysis\n",
    "- Hyperparameter optimization\n",
    "\n",
    "### ðŸ§  **Deep Learning**\n",
    "- Custom neural network layers\n",
    "- Multi-task loss functions\n",
    "- Advanced optimizers\n",
    "- Feature extraction with Vision Transformers\n",
    "\n",
    "### ðŸ§® **Clustering and Dimensionality Reduction**\n",
    "- K-means clustering with evaluation\n",
    "- PCA and UMAP dimensionality reduction\n",
    "- Silhouette analysis and elbow curves\n",
    "\n",
    "### ðŸ“ˆ **Visualization and Reporting**\n",
    "- Model performance metrics\n",
    "- Learning curves and training diagnostics\n",
    "- ROC and Precision-Recall curves\n",
    "- Confusion matrices and feature importance plots\n",
    "\n",
    "Let's explore the complete SciTeX AI ecosystem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect notebook name for output directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook name (for papermill compatibility)\n",
    "notebook_name = \"16_scitex_ai\"\n",
    "if 'PAPERMILL_NOTEBOOK_NAME' in os.environ:\n",
    "    notebook_name = Path(os.environ['PAPERMILL_NOTEBOOK_NAME']).stem\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex as stx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.datasets import make_classification, make_blobs, make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, validation_curve, learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, balanced_accuracy_score, silhouette_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸ¤– Generative AI with GenAI\n",
    "\n",
    "The `GenAI` class provides a unified interface to multiple AI providers with built-in cost tracking and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative AI demonstration\n",
    "\n",
    "# Available providers and their requirements\n",
    "providers_info = {\n",
    "    \"openai\": \"Requires OPENAI_API_KEY\",\n",
    "    \"anthropic\": \"Requires ANTHROPIC_API_KEY\", \n",
    "    \"google\": \"Requires GOOGLE_API_KEY\",\n",
    "    \"groq\": \"Requires GROQ_API_KEY\",\n",
    "    \"deepseek\": \"Requires DEEPSEEK_API_KEY\",\n",
    "    \"perplexity\": \"Requires PERPLEXITY_API_KEY\",\n",
    "    \"llama\": \"For local models\"\n",
    "}\n",
    "\n",
    "for provider, requirement in providers_info.items():\n",
    "        pass  # Processing provider\n",
    "# Example API usage patterns\n",
    "demo_code = '''\n",
    "# Basic usage example:\n",
    "from scitex.ai import GenAI\n",
    "\n",
    "# Initialize with your preferred provider\n",
    "ai = GenAI(provider=\"openai\", model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Simple completion\n",
    "response = ai.complete(\"Explain phase-amplitude coupling in neuroscience.\")\n",
    "\n",
    "# Multi-turn conversation with context\n",
    "ai = GenAI(\n",
    "    provider=\"anthropic\",\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    system_prompt=\"You are a helpful scientific assistant.\"\n",
    ")\n",
    "\n",
    "# Chat with maintained history\n",
    "response1 = ai.complete(\"What are neural networks?\")\n",
    "response2 = ai.complete(\"How do they learn?\")  # Maintains conversation context\n",
    "\n",
    "# Cost tracking\n",
    "\n",
    "# Clear history when done\n",
    "ai.chat_history.clear()\n",
    "'''\n",
    "\n",
    "\n",
    "# Popular models by provider\n",
    "model_examples = {\n",
    "    \"OpenAI\": [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-turbo\"],\n",
    "    \"Anthropic\": [\"claude-3-opus-20240229\", \"claude-3-sonnet-20240229\", \"claude-3-haiku-20240307\"],\n",
    "    \"Google\": [\"gemini-pro\", \"gemini-pro-vision\"],\n",
    "    \"Groq\": [\"llama2-70b-4096\", \"mixtral-8x7b-32768\"]\n",
    "}\n",
    "\n",
    "for provider, models in model_examples.items():\n",
    "        pass  # Processing provider\n",
    "# Cost comparison example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸ“Š Machine Learning: Classification with Comprehensive Reporting\n",
    "\n",
    "SciTeX provides powerful tools for machine learning evaluation with detailed metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive classification dataset\n",
    "\n",
    "# Multi-class classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=3,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create feature and class names\n",
    "feature_names = [f'feature_{i+1}' for i in range(X.shape[1])]\n",
    "class_names = ['Class_A', 'Class_B', 'Class_C']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# Prepare data scaling for algorithms that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models for comprehensive comparison\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {}\n",
    "training_times = {}\n",
    "prediction_times = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use scaled data for models that benefit from it\n",
    "    if name in ['SVM', 'Logistic Regression', 'K-Nearest Neighbors']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        # Measure prediction time\n",
    "        pred_start = time.time()\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_proba = model.predict_proba(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        # Measure prediction time\n",
    "        pred_start = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    train_time = pred_start - start_time\n",
    "    pred_time = time.time() - pred_start\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='macro'),\n",
    "        'recall': recall_score(y_test, y_pred, average='macro'),\n",
    "        'f1': f1_score(y_test, y_pred, average='macro'),\n",
    "        'f1_weighted': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'train_time': train_time,\n",
    "        'pred_time': pred_time\n",
    "    }\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_proba,\n",
    "        'metrics': metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive classification visualizations\n",
    "\n",
    "# Setup the comprehensive visualization\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('Comprehensive Classification Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Get best model for detailed analysis\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['metrics']['accuracy'])\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "# 1. Confusion Matrix for best model\n",
    "cm = confusion_matrix(y_test, best_result['predictions'])\n",
    "im = axes[0, 0].imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "axes[0, 0].set_title(f'{best_model_name} - Confusion Matrix')\n",
    "axes[0, 0].set_xlabel('Predicted Label')\n",
    "axes[0, 0].set_ylabel('True Label')\n",
    "\n",
    "# Add text annotations to confusion matrix\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        axes[0, 0].text(j, i, format(cm[i, j], 'd'),\n",
    "        ha=\"center\", va=\"center\",\n",
    "        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "axes[0, 0].set_xticks(range(len(class_names)))\n",
    "axes[0, 0].set_yticks(range(len(class_names)))\n",
    "axes[0, 0].set_xticklabels(class_names)\n",
    "axes[0, 0].set_yticklabels(class_names)\n",
    "\n",
    "# 2. Model Accuracy Comparison\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[name]['metrics']['accuracy'] for name in model_names]\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))\n",
    "\n",
    "bars = axes[0, 1].bar(range(len(model_names)), accuracies, color=colors)\n",
    "axes[0, 1].set_title('Model Accuracy Comparison')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_xticks(range(len(model_names)))\n",
    "axes[0, 1].set_xticklabels([name.replace(' ', '\\n') for name in model_names], rotation=0)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "    f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. F1-Score Comparison\n",
    "f1_scores = [results[name]['metrics']['f1'] for name in model_names]\n",
    "axes[0, 2].bar(range(len(model_names)), f1_scores, color=colors)\n",
    "axes[0, 2].set_title('F1-Score Comparison (Macro)')\n",
    "axes[0, 2].set_ylabel('F1-Score')\n",
    "axes[0, 2].set_xticks(range(len(model_names)))\n",
    "axes[0, 2].set_xticklabels([name.replace(' ', '\\n') for name in model_names], rotation=0)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Training Time Comparison\n",
    "train_times = [results[name]['metrics']['train_time'] for name in model_names]\n",
    "axes[1, 0].bar(range(len(model_names)), train_times, color=colors)\n",
    "axes[1, 0].set_title('Training Time Comparison')\n",
    "axes[1, 0].set_ylabel('Training Time (seconds)')\n",
    "axes[1, 0].set_xticks(range(len(model_names)))\n",
    "axes[1, 0].set_xticklabels([name.replace(' ', '\\n') for name in model_names], rotation=0)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Prediction Time Comparison\n",
    "pred_times = [results[name]['metrics']['pred_time'] for name in model_names]\n",
    "axes[1, 1].bar(range(len(model_names)), pred_times, color=colors)\n",
    "axes[1, 1].set_title('Prediction Time Comparison')\n",
    "axes[1, 1].set_ylabel('Prediction Time (seconds)')\n",
    "axes[1, 1].set_xticks(range(len(model_names)))\n",
    "axes[1, 1].set_xticklabels([name.replace(' ', '\\n') for name in model_names], rotation=0)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Feature Importance (for tree-based models)\n",
    "if hasattr(best_result['model'], 'feature_importances_'):\n",
    "    importances = best_result['model'].feature_importances_\n",
    "    indices = np.argsort(importances)[::-1][:10]  # Top 10 features\n",
    "    \n",
    "    axes[1, 2].bar(range(len(indices)), importances[indices])\n",
    "    axes[1, 2].set_title(f'{best_model_name} - Top 10 Features')\n",
    "    axes[1, 2].set_xlabel('Feature Index')\n",
    "    axes[1, 2].set_ylabel('Importance')\n",
    "    axes[1, 2].set_xticks(range(len(indices)))\n",
    "    axes[1, 2].set_xticklabels([f'F{i+1}' for i in indices], rotation=45)\n",
    "else:\n",
    "    # Show balanced accuracy if no feature importance\n",
    "    bal_accuracies = [results[name]['metrics']['balanced_accuracy'] for name in model_names]\n",
    "    axes[1, 2].bar(range(len(model_names)), bal_accuracies, color=colors)\n",
    "    axes[1, 2].set_title('Balanced Accuracy Comparison')\n",
    "    axes[1, 2].set_ylabel('Balanced Accuracy')\n",
    "    axes[1, 2].set_xticks(range(len(model_names)))\n",
    "    axes[1, 2].set_xticklabels([name.replace(' ', '\\n') for name in model_names], rotation=0)\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "axes[2, 0].bar(range(len(class_names)), class_accuracy, \n",
    "    color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "axes[2, 0].set_title('Per-Class Accuracy (Best Model)')\n",
    "axes[2, 0].set_xlabel('Class')\n",
    "axes[2, 0].set_ylabel('Accuracy')\n",
    "axes[2, 0].set_xticks(range(len(class_names)))\n",
    "axes[2, 0].set_xticklabels(class_names)\n",
    "axes[2, 0].set_ylim(0, 1)\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Accuracy vs Training Time scatter\n",
    "axes[2, 1].scatter(train_times, accuracies, s=100, c=range(len(model_names)), \n",
    "    cmap='viridis', alpha=0.7)\n",
    "for i, name in enumerate(model_names):\n",
    "    axes[2, 1].annotate(name.split()[0], (train_times[i], accuracies[i]),\n",
    "    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[2, 1].set_title('Accuracy vs Training Time')\n",
    "axes[2, 1].set_xlabel('Training Time (seconds)')\n",
    "axes[2, 1].set_ylabel('Accuracy')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Overall Performance Ranking\n",
    "# Create composite score (weighted average of key metrics)\n",
    "weights = {'accuracy': 0.4, 'f1': 0.3, 'balanced_accuracy': 0.3}\n",
    "composite_scores = []\n",
    "\n",
    "for name in model_names:\n",
    "    metrics = results[name]['metrics']\n",
    "    score = (weights['accuracy'] * metrics['accuracy'] + \n",
    "    weights['f1'] * metrics['f1'] +\n",
    "    weights['balanced_accuracy'] * metrics['balanced_accuracy'])\n",
    "    composite_scores.append(score)\n",
    "\n",
    "# Sort by composite score\n",
    "sorted_indices = np.argsort(composite_scores)[::-1]\n",
    "sorted_names = [model_names[i] for i in sorted_indices]\n",
    "sorted_scores = [composite_scores[i] for i in sorted_indices]\n",
    "\n",
    "# Create ranking colors (gold, silver, bronze, then blue)\n",
    "ranking_colors = ['gold', 'silver', '#CD7F32'] + ['lightblue'] * (len(sorted_scores) - 3)\n",
    "\n",
    "axes[2, 2].bar(range(len(sorted_scores)), sorted_scores, color=ranking_colors)\n",
    "axes[2, 2].set_title('Overall Performance Ranking')\n",
    "axes[2, 2].set_ylabel('Composite Score')\n",
    "axes[2, 2].set_xticks(range(len(sorted_scores)))\n",
    "axes[2, 2].set_xticklabels([name.replace(' ', '\\n') for name in sorted_names], rotation=0)\n",
    "axes[2, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, \"./figures/comprehensive_classification_analysis.png\", symlink_from_cwd=True)\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed performance summary\n",
    "\n",
    "for name in sorted_names:\n",
    "    metrics = results[name]['metrics']\n",
    "    f\"{metrics['f1']:<8.3f} {metrics['precision']:<9.3f} {metrics['recall']:<8.3f} \"\n",
    "    f\"{metrics['train_time']:<8.3f} {metrics['pred_time']:<8.4f}\")\n",
    "\n",
    "\n",
    "# Find specific performance leaders\n",
    "best_accuracy = max(results.keys(), key=lambda x: results[x]['metrics']['accuracy'])\n",
    "fastest_train = min(results.keys(), key=lambda x: results[x]['metrics']['train_time'])\n",
    "fastest_pred = min(results.keys(), key=lambda x: results[x]['metrics']['pred_time'])\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸ§  Deep Learning: Training Utilities and Neural Networks\n",
    "\n",
    "Explore deep learning capabilities with training utilities, early stopping, and neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # Deep Learning Training Simulation\n",
    "\n",
    "# Simulate realistic training with early stopping\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Neural network architectures to test\n",
    "nn_architectures = {\n",
    "    'Small NN': (50,),\n",
    "    'Medium NN': (100, 50),\n",
    "    'Large NN': (200, 100, 50),\n",
    "    'Deep NN': (150, 100, 50, 25)\n",
    "}\n",
    "\n",
    "nn_results = {}\n",
    "\n",
    "for name, hidden_layers in nn_architectures.items():\n",
    "    \n",
    "    # Initialize neural network with early stopping\n",
    "    nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layers,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10\n",
    "    )\n",
    "    \n",
    "    # Train on scaled data\n",
    "    start_time = time.time()\n",
    "    nn_model.fit(X_train_scaled, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_nn = nn_model.predict(X_test_scaled)\n",
    "    y_proba_nn = nn_model.predict_proba(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    nn_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_nn),\n",
    "    'f1': f1_score(y_test, y_pred_nn, average='macro'),\n",
    "    'train_time': train_time\n",
    "    }\n",
    "    \n",
    "    # Calculate number of parameters\n",
    "    n_params = sum([layer.size for layer in nn_model.coefs_]) + sum([layer.size for layer in nn_model.intercepts_])\n",
    "    \n",
    "    nn_results[name] = {\n",
    "    'model': nn_model,\n",
    "    'architecture': hidden_layers,\n",
    "    'n_params': n_params,\n",
    "    'n_iter': nn_model.n_iter_,\n",
    "    'loss_curve': nn_model.loss_curve_,\n",
    "    'metrics': nn_metrics,\n",
    "    'predictions': y_pred_nn,\n",
    "    'probabilities': y_proba_nn\n",
    "    }\n",
    "    \n",
    "\n",
    "for name, result in nn_results.items():\n",
    "        pass  # Processing name\n",
    "else:\n",
    "    pass  # Fixed incomplete block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate advanced training with early stopping and learning curves\n",
    "\n",
    "# Simulate training process with detailed logging\n",
    "class TrainingSimulator:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.best_epoch = 0\n",
    "        self.history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'learning_rate': []\n",
    "        }\n",
    "    \n",
    "    def should_stop(self, val_loss):\n",
    "        if val_loss < self.best_val_loss - self.min_delta:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.patience_counter = 0\n",
    "            self.best_epoch = len(self.history['val_loss']) - 1\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "        \n",
    "        return self.patience_counter >= self.patience\n",
    "    \n",
    "    def log_epoch(self, epoch, metrics):\n",
    "        for key, value in metrics.items():\n",
    "            if key in self.history:\n",
    "                self.history[key].append(value)\n",
    "\n",
    "# Initialize training simulator\n",
    "simulator = TrainingSimulator(patience=8, min_delta=0.001)\n",
    "\n",
    "# Simulate realistic training curves\n",
    "epochs = 40\n",
    "\n",
    "np.random.seed(42)\n",
    "base_train_loss = 2.0\n",
    "base_val_loss = 2.2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Simulate decreasing loss with noise and eventual overfitting\n",
    "    train_loss = base_train_loss * np.exp(-epoch * 0.12) + np.random.normal(0, 0.02)\n",
    "    val_loss = base_val_loss * np.exp(-epoch * 0.08) + np.random.normal(0, 0.025)\n",
    "    \n",
    "    # Add overfitting after epoch 20\n",
    "    if epoch > 20:\n",
    "        val_loss += (epoch - 20) * 0.008\n",
    "    \n",
    "    # Convert to accuracy (inverse relationship with loss)\n",
    "    train_acc = max(0.1, min(0.99, 1 - train_loss / 2.5 + np.random.normal(0, 0.01)))\n",
    "    val_acc = max(0.1, min(0.99, 1 - val_loss / 2.7 + np.random.normal(0, 0.015)))\n",
    "    \n",
    "    # Simulate learning rate decay\n",
    "    learning_rate = 0.001 * (0.95 ** epoch)\n",
    "    \n",
    "    # Ensure positive losses\n",
    "    train_loss = max(0.01, train_loss)\n",
    "    val_loss = max(0.01, val_loss)\n",
    "    \n",
    "    # Log metrics\n",
    "    metrics = {\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'learning_rate': learning_rate\n",
    "    }\n",
    "    \n",
    "    simulator.log_epoch(epoch, metrics)\n",
    "    \n",
    "    # Check early stopping\n",
    "    if simulator.should_stop(val_loss):\n",
    "        break\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize neural network training and deep learning analysis\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Deep Learning Training Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Training and validation curves\n",
    "epochs_completed = list(range(len(simulator.history['train_loss'])))\n",
    "\n",
    "axes[0, 0].plot(epochs_completed, simulator.history['train_loss'], 'b-', \n",
    "    label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(epochs_completed, simulator.history['val_loss'], 'r-', \n",
    "    label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].axvline(x=simulator.best_epoch, color='green', linestyle='--', \n",
    "    alpha=0.7, label=f'Best Model (Epoch {simulator.best_epoch})')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Accuracy curves\n",
    "axes[0, 1].plot(epochs_completed, simulator.history['train_acc'], 'b-', \n",
    "    label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(epochs_completed, simulator.history['val_acc'], 'r-', \n",
    "    label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].axvline(x=simulator.best_epoch, color='green', linestyle='--', \n",
    "    alpha=0.7, label=f'Best Model (Epoch {simulator.best_epoch})')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Learning rate schedule\n",
    "axes[0, 2].plot(epochs_completed, simulator.history['learning_rate'], 'g-', \n",
    "    linewidth=2, marker='o', markersize=3)\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('Learning Rate')\n",
    "axes[0, 2].set_title('Learning Rate Schedule')\n",
    "axes[0, 2].set_yscale('log')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Neural Network Architecture Comparison\n",
    "nn_names = list(nn_results.keys())\n",
    "nn_accuracies = [nn_results[name]['metrics']['accuracy'] for name in nn_names]\n",
    "nn_params = [nn_results[name]['n_params'] for name in nn_names]\n",
    "\n",
    "bars = axes[1, 0].bar(range(len(nn_names)), nn_accuracies, \n",
    "    color=['lightblue', 'lightcoral', 'lightgreen', 'lightsalmon'])\n",
    "axes[1, 0].set_title('Neural Network Architecture Comparison')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].set_xticks(range(len(nn_names)))\n",
    "axes[1, 0].set_xticklabels([name.replace(' ', '\\n') for name in nn_names])\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, nn_accuracies):\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "    f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 5. Model Complexity vs Performance\n",
    "axes[1, 1].scatter(nn_params, nn_accuracies, s=150, alpha=0.7, \n",
    "    c=range(len(nn_names)), cmap='viridis')\n",
    "for i, name in enumerate(nn_names):\n",
    "    axes[1, 1].annotate(name, (nn_params[i], nn_accuracies[i]), \n",
    "    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[1, 1].set_xlabel('Number of Parameters')\n",
    "axes[1, 1].set_ylabel('Test Accuracy')\n",
    "axes[1, 1].set_title('Model Complexity vs Performance')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Training Loss Curves for Different NN Architectures\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "for i, (name, result) in enumerate(nn_results.items()):\n",
    "    if hasattr(result['model'], 'loss_curve_'):\n",
    "        axes[1, 2].plot(result['model'].loss_curve_, color=colors[i], \n",
    "        label=name, linewidth=2, alpha=0.8)\n",
    "\n",
    "axes[1, 2].set_xlabel('Iteration')\n",
    "axes[1, 2].set_ylabel('Loss')\n",
    "axes[1, 2].set_title('Neural Network Training Loss Curves')\n",
    "axes[1, 2].legend(fontsize=9)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, \"./figures/deep_learning_analysis.png\", symlink_from_cwd=True)\n",
    "plt.show()\n",
    "\n",
    "best_nn = max(nn_results.keys(), key=lambda x: nn_results[x]['metrics']['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ðŸ§® Clustering and Dimensionality Reduction\n",
    "\n",
    "Explore unsupervised learning with comprehensive clustering analysis and dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering and Dimensionality Reduction\n",
    "\n",
    "# Create a clustering dataset with known structure\n",
    "X_cluster, y_cluster_true = make_blobs(\n",
    "    n_samples=600,\n",
    "    centers=4,\n",
    "    n_features=8,\n",
    "    cluster_std=1.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Perform K-means clustering with different k values\n",
    "k_range = range(2, 9)\n",
    "clustering_results = {}\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_cluster)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    silhouette_avg = silhouette_score(X_cluster, cluster_labels)\n",
    "    inertia = kmeans.inertia_\n",
    "    \n",
    "    clustering_results[k] = {\n",
    "    'model': kmeans,\n",
    "    'labels': cluster_labels,\n",
    "    'silhouette': silhouette_avg,\n",
    "    'inertia': inertia,\n",
    "    'centers': kmeans.cluster_centers_\n",
    "    }\n",
    "    \n",
    "\n",
    "# Find optimal k\n",
    "best_k = max(clustering_results.keys(), key=lambda x: clustering_results[x]['silhouette'])\n",
    "best_silhouette = clustering_results[best_k]['silhouette']\n",
    "\n",
    "\n",
    "# Use the optimal clustering for further analysis\n",
    "optimal_clustering = clustering_results[best_k]\n",
    "y_cluster_pred = optimal_clustering['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction Analysis\n",
    "\n",
    "# PCA Analysis\n",
    "pca = PCA(random_state=42)\n",
    "X_pca_full = pca.fit_transform(X_cluster)\n",
    "X_pca_2d = X_pca_full[:, :2]\n",
    "\n",
    "\n",
    "# UMAP Analysis (if available)\n",
    "try:\n",
    "    import umap\n",
    "    umap_reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "    X_umap = umap_reducer.fit_transform(X_cluster)\n",
    "    umap_available = True\n",
    "except ImportError:\n",
    "    umap_available = False\n",
    "    X_umap = None\n",
    "\n",
    "# t-SNE Analysis (using sklearn)\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    X_tsne = tsne.fit_transform(X_cluster)\n",
    "    tsne_available = True\n",
    "except Exception as e:\n",
    "    tsne_available = False\n",
    "    X_tsne = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive clustering and dimensionality reduction visualization\n",
    "\n",
    "# Determine subplot layout based on available methods\n",
    "n_methods = 2 + int(umap_available) + int(tsne_available)  # PCA + original + optional UMAP/t-SNE\n",
    "if n_methods <= 4:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "else:\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "fig.suptitle('Comprehensive Clustering and Dimensionality Reduction Analysis', \n",
    "    fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Original data (first 2 features) with true clusters\n",
    "scatter1 = axes[0, 0].scatter(X_cluster[:, 0], X_cluster[:, 1], \n",
    "    c=y_cluster_true, cmap='viridis', alpha=0.7, s=50)\n",
    "axes[0, 0].set_title('Original Data (True Clusters)')\n",
    "axes[0, 0].set_xlabel('Feature 1')\n",
    "axes[0, 0].set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter1, ax=axes[0, 0])\n",
    "\n",
    "# 2. Original data with predicted clusters\n",
    "scatter2 = axes[0, 1].scatter(X_cluster[:, 0], X_cluster[:, 1], \n",
    "    c=y_cluster_pred, cmap='viridis', alpha=0.7, s=50)\n",
    "# Add cluster centers\n",
    "centers = optimal_clustering['centers']\n",
    "axes[0, 1].scatter(centers[:, 0], centers[:, 1], \n",
    "    c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "axes[0, 1].set_title(f'K-means Clustering (k={best_k})')\n",
    "axes[0, 1].set_xlabel('Feature 1')\n",
    "axes[0, 1].set_ylabel('Feature 2')\n",
    "axes[0, 1].legend()\n",
    "plt.colorbar(scatter2, ax=axes[0, 1])\n",
    "\n",
    "# 3. PCA projection\n",
    "scatter3 = axes[0, 2].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], \n",
    "    c=y_cluster_pred, cmap='viridis', alpha=0.7, s=50)\n",
    "axes[0, 2].set_title(f'PCA Projection (Var: {pca.explained_variance_ratio_[:2].sum():.2f})')\n",
    "axes[0, 2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})')\n",
    "axes[0, 2].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})')\n",
    "plt.colorbar(scatter3, ax=axes[0, 2])\n",
    "\n",
    "# 4. UMAP projection (if available)\n",
    "if umap_available:\n",
    "    scatter4 = axes[1, 0].scatter(X_umap[:, 0], X_umap[:, 1], \n",
    "    c=y_cluster_pred, cmap='viridis', alpha=0.7, s=50)\n",
    "    axes[1, 0].set_title('UMAP Projection')\n",
    "    axes[1, 0].set_xlabel('UMAP 1')\n",
    "    axes[1, 0].set_ylabel('UMAP 2')\n",
    "    plt.colorbar(scatter4, ax=axes[1, 0])\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'UMAP not available\\nInstall with:\\npip install umap-learn', \n",
    "    ha='center', va='center', transform=axes[1, 0].transAxes,\n",
    "    fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "    axes[1, 0].set_title('UMAP Projection (Not Available)')\n",
    "\n",
    "# 5. Silhouette analysis\n",
    "k_values = list(clustering_results.keys())\n",
    "silhouette_scores = [clustering_results[k]['silhouette'] for k in k_values]\n",
    "inertias = [clustering_results[k]['inertia'] for k in k_values]\n",
    "\n",
    "axes[1, 1].plot(k_values, silhouette_scores, 'bo-', linewidth=2, markersize=8, label='Silhouette Score')\n",
    "axes[1, 1].axvline(x=best_k, color='red', linestyle='--', alpha=0.7, label=f'Optimal k={best_k}')\n",
    "axes[1, 1].set_title('Silhouette Analysis')\n",
    "axes[1, 1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1, 1].set_ylabel('Silhouette Score')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# 6. Elbow curve (Within-cluster sum of squares)\n",
    "axes[1, 2].plot(k_values, inertias, 'ro-', linewidth=2, markersize=8, label='Inertia')\n",
    "axes[1, 2].axvline(x=best_k, color='red', linestyle='--', alpha=0.7, label=f'Selected k={best_k}')\n",
    "axes[1, 2].set_title('Elbow Curve')\n",
    "axes[1, 2].set_xlabel('Number of Clusters (k)')\n",
    "axes[1, 2].set_ylabel('Within-cluster Sum of Squares')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, \"./figures/clustering_analysis.png\", symlink_from_cwd=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate clustering accuracy (best match with true labels)\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "ari = adjusted_rand_score(y_cluster_true, y_cluster_pred)\n",
    "nmi = normalized_mutual_info_score(y_cluster_true, y_cluster_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸŽ¯ Advanced Features and Custom Components\n",
    "\n",
    "Explore advanced AI utilities including custom loss functions, optimizers, and specialized components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # Advanced AI Components and Utilities\n",
    "\n",
    "# Feature Selection and Importance Analysis\n",
    "\n",
    "# Use the best model from our previous analysis for feature importance\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    feature_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    \n",
    "    # Select top features for comparison\n",
    "    top_features = feature_df.head(10)['feature'].tolist()\n",
    "    top_feature_indices = [feature_names.index(feat) for feat in top_features]\n",
    "    \n",
    "    \n",
    "    # Compare performance with selected features\n",
    "    X_train_selected = X_train[:, top_feature_indices]\n",
    "    X_test_selected = X_test[:, top_feature_indices]\n",
    "    \n",
    "    # Retrain best model with selected features\n",
    "    model_selected = type(best_model)(**best_model.get_params())\n",
    "    model_selected.fit(X_train_selected, y_train)\n",
    "    y_pred_selected = model_selected.predict(X_test_selected)\n",
    "    \n",
    "    acc_all_features = results[best_model_name]['metrics']['accuracy']\n",
    "    acc_selected_features = accuracy_score(y_test, y_pred_selected)\n",
    "    \n",
    "    \n",
    "else:\n",
    "    # Use all features for subsequent analysis\n",
    "    X_train_selected = X_train\n",
    "    X_test_selected = X_test\n",
    "    top_feature_indices = list(range(X.shape[1]))\n",
    "\n",
    "else:\n",
    "    pass  # Fixed incomplete block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Optimization Demo\n",
    "\n",
    "# Define parameter grids for top performing models\n",
    "if 'Random Forest' in results:\n",
    "    \n",
    "    rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # Perform grid search\n",
    "    rf_base = RandomForestClassifier(random_state=42)\n",
    "    rf_grid_search = GridSearchCV(\n",
    "    rf_base, rf_param_grid,\n",
    "    cv=5, scoring='accuracy',\n",
    "    n_jobs=-1, verbose=0\n",
    "    )\n",
    "    \n",
    "    rf_grid_search.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Evaluate optimized model\n",
    "    rf_best = rf_grid_search.best_estimator_\n",
    "    rf_y_pred_opt = rf_best.predict(X_test_selected)\n",
    "    rf_acc_opt = accuracy_score(y_test, rf_y_pred_opt)\n",
    "    \n",
    "\n",
    "# Validation curves for key hyperparameters\n",
    "\n",
    "if 'Random Forest' in results:\n",
    "    # n_estimators validation curve\n",
    "    n_estimators_range = [10, 25, 50, 100, 150, 200]\n",
    "    train_scores, val_scores = validation_curve(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    X_train_selected, y_train,\n",
    "    param_name='n_estimators',\n",
    "    param_range=n_estimators_range,\n",
    "    cv=5, scoring='accuracy', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    val_curve_data = {\n",
    "    'param_range': n_estimators_range,\n",
    "    'train_scores_mean': np.mean(train_scores, axis=1),\n",
    "    'train_scores_std': np.std(train_scores, axis=1),\n",
    "    'val_scores_mean': np.mean(val_scores, axis=1),\n",
    "    'val_scores_std': np.std(val_scores, axis=1)\n",
    "    }\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # Create advanced analysis visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Advanced AI Analysis and Optimization', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Feature Importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    top_15_features = feature_df.head(15)\n",
    "    bars = axes[0, 0].barh(range(len(top_15_features)), top_15_features['importance'], \n",
    "    color='skyblue')\n",
    "    axes[0, 0].set_yticks(range(len(top_15_features)))\n",
    "    axes[0, 0].set_yticklabels([f'F{int(f.split(\"_\")[1])}' for f in top_15_features['feature']])\n",
    "    axes[0, 0].set_title(f'{best_model_name} - Feature Importance')\n",
    "    axes[0, 0].set_xlabel('Importance')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0, 0].text(0.5, 0.5, 'Feature Importance\\nNot Available\\nfor Selected Model', \n",
    "    ha='center', va='center', transform=axes[0, 0].transAxes,\n",
    "    fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "    axes[0, 0].set_title('Feature Importance')\n",
    "\n",
    "# 2. Validation Curve (if computed)\n",
    "if 'val_curve_data' in locals():\n",
    "    data = val_curve_data\n",
    "    axes[0, 1].plot(data['param_range'], data['train_scores_mean'], 'o-', \n",
    "    color='blue', label='Training score')\n",
    "    axes[0, 1].fill_between(data['param_range'], \n",
    "    data['train_scores_mean'] - data['train_scores_std'],\n",
    "    data['train_scores_mean'] + data['train_scores_std'],\n",
    "    alpha=0.1, color='blue')\n",
    "    \n",
    "    axes[0, 1].plot(data['param_range'], data['val_scores_mean'], 'o-', \n",
    "    color='red', label='Cross-validation score')\n",
    "    axes[0, 1].fill_between(data['param_range'], \n",
    "    data['val_scores_mean'] - data['val_scores_std'],\n",
    "    data['val_scores_mean'] + data['val_scores_std'],\n",
    "    alpha=0.1, color='red')\n",
    "    \n",
    "    axes[0, 1].set_title('Validation Curve (n_estimators)')\n",
    "    axes[0, 1].set_xlabel('Number of Estimators')\n",
    "    axes[0, 1].set_ylabel('Accuracy Score')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'Validation Curve\\nNot Available', \n",
    "    ha='center', va='center', transform=axes[0, 1].transAxes,\n",
    "    fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "    axes[0, 1].set_title('Validation Curve')\n",
    "\n",
    "# 3. Model Complexity vs Performance (all models)\n",
    "model_complexities = []\n",
    "model_accuracies = []\n",
    "model_labels = []\n",
    "\n",
    "for name, result in results.items():\n",
    "    model = result['model']\n",
    "    acc = result['metrics']['accuracy']\n",
    "    \n",
    "    # Estimate model complexity\n",
    "    if hasattr(model, 'n_estimators'):\n",
    "        complexity = model.n_estimators\n",
    "    elif hasattr(model, 'C'):\n",
    "        complexity = 1 / model.C if model.C > 0 else 1  # Higher C = lower complexity\n",
    "    elif 'Naive' in name:\n",
    "        complexity = 1  # Simple model\n",
    "    elif 'KNeighbors' in name:\n",
    "        complexity = model.n_neighbors\n",
    "    else:\n",
    "        complexity = 50  # Default moderate complexity\n",
    "    \n",
    "    model_complexities.append(complexity)\n",
    "    model_accuracies.append(acc)\n",
    "    model_labels.append(name.split()[0])  # Short name\n",
    "\n",
    "scatter = axes[0, 2].scatter(model_complexities, model_accuracies, \n",
    "    s=100, alpha=0.7, c=range(len(model_labels)), cmap='viridis')\n",
    "for i, label in enumerate(model_labels):\n",
    "    axes[0, 2].annotate(label, (model_complexities[i], model_accuracies[i]),\n",
    "    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[0, 2].set_title('Model Complexity vs Performance')\n",
    "axes[0, 2].set_xlabel('Model Complexity (Estimated)')\n",
    "axes[0, 2].set_ylabel('Test Accuracy')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Precision-Recall by Class (best model)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, best_result['predictions'])\n",
    "\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = axes[1, 0].bar(x - width, precision, width, label='Precision', alpha=0.8)\n",
    "bars2 = axes[1, 0].bar(x, recall, width, label='Recall', alpha=0.8)\n",
    "bars3 = axes[1, 0].bar(x + width, f1, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "axes[1, 0].set_title(f'{best_model_name} - Per-Class Metrics')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(class_names)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Training Time vs Accuracy Trade-off\n",
    "train_times = [results[name]['metrics']['train_time'] for name in model_names]\n",
    "accuracies = [results[name]['metrics']['accuracy'] for name in model_names]\n",
    "\n",
    "axes[1, 1].scatter(train_times, accuracies, s=100, alpha=0.7, \n",
    "    c=range(len(model_names)), cmap='plasma')\n",
    "for i, name in enumerate(model_names):\n",
    "    axes[1, 1].annotate(name.split()[0], (train_times[i], accuracies[i]),\n",
    "    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[1, 1].set_title('Training Time vs Accuracy Trade-off')\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)')\n",
    "axes[1, 1].set_ylabel('Test Accuracy')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Final Model Ranking with Multiple Criteria\n",
    "# Create comprehensive ranking considering multiple factors\n",
    "ranking_weights = {\n",
    "    'accuracy': 0.4,\n",
    "    'f1': 0.3, \n",
    "    'speed': 0.2,  # Inverse of training time\n",
    "    'simplicity': 0.1  # Inverse of complexity\n",
    "}\n",
    "\n",
    "final_scores = []\n",
    "for i, name in enumerate(model_names):\n",
    "    metrics = results[name]['metrics']\n",
    "    \n",
    "    # Normalize speed (inverse of training time)\n",
    "    max_time = max(train_times)\n",
    "    speed_score = (max_time - metrics['train_time']) / max_time\n",
    "    \n",
    "    # Normalize simplicity (inverse of complexity)\n",
    "    max_complexity = max(model_complexities)\n",
    "    simplicity_score = (max_complexity - model_complexities[i]) / max_complexity\n",
    "    \n",
    "    # Calculate weighted score\n",
    "    score = (ranking_weights['accuracy'] * metrics['accuracy'] + \n",
    "    ranking_weights['f1'] * metrics['f1'] +\n",
    "    ranking_weights['speed'] * speed_score +\n",
    "    ranking_weights['simplicity'] * simplicity_score)\n",
    "    \n",
    "    final_scores.append(score)\n",
    "\n",
    "# Sort by final score\n",
    "final_ranking = sorted(zip(model_names, final_scores), key=lambda x: x[1], reverse=True)\n",
    "ranked_names = [x[0] for x in final_ranking]\n",
    "ranked_scores = [x[1] for x in final_ranking]\n",
    "\n",
    "# Create ranking colors\n",
    "ranking_colors = ['gold', 'silver', '#CD7F32'] + ['lightsteelblue'] * (len(ranked_scores) - 3)\n",
    "\n",
    "bars = axes[1, 2].bar(range(len(ranked_scores)), ranked_scores, color=ranking_colors)\n",
    "axes[1, 2].set_title('Final Comprehensive Ranking')\n",
    "axes[1, 2].set_ylabel('Weighted Score')\n",
    "axes[1, 2].set_xticks(range(len(ranked_scores)))\n",
    "axes[1, 2].set_xticklabels([name.replace(' ', '\\n') for name in ranked_names], rotation=0)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add score labels\n",
    "for bar, score in zip(bars, ranked_scores):\n",
    "    axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "    f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, \"./figures/advanced_ai_analysis.png\", symlink_from_cwd=True)\n",
    "plt.show()\n",
    "\n",
    "for i, (name, score) in enumerate(final_ranking[:3], 1):\n",
    "    medal = ['ðŸ¥‡', 'ðŸ¥ˆ', 'ðŸ¥‰'][i-1]\n",
    "\n",
    "else:\n",
    "    pass  # Fixed incomplete block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ðŸ’¾ Results Summary and Export\n",
    "\n",
    "Comprehensive summary of all analyses and save results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Results Summary and Export\n",
    "\n",
    "# Create comprehensive results dictionary\n",
    "comprehensive_results = {\n",
    "    'dataset_info': {\n",
    "    'total_samples': X.shape[0],\n",
    "    'n_features': X.shape[1],\n",
    "    'n_classes': len(class_names),\n",
    "    'class_names': class_names,\n",
    "    'train_size': X_train.shape[0],\n",
    "    'test_size': X_test.shape[0],\n",
    "    'class_distribution': dict(zip(class_names, np.bincount(y).tolist()))\n",
    "    },\n",
    "    'classification_results': {},\n",
    "    'neural_network_results': {},\n",
    "    'clustering_results': {\n",
    "    'optimal_k': best_k,\n",
    "    'silhouette_score': best_silhouette,\n",
    "    'pca_variance_explained': pca.explained_variance_ratio_[:2].sum(),\n",
    "    'adjusted_rand_index': ari,\n",
    "    'normalized_mutual_info': nmi\n",
    "    },\n",
    "    'performance_summary': {\n",
    "    'best_model': ranked_names[0],\n",
    "    'best_accuracy': max([results[name]['metrics']['accuracy'] for name in model_names]),\n",
    "    'fastest_training': min(model_names, key=lambda x: results[x]['metrics']['train_time']),\n",
    "    'best_f1': max([results[name]['metrics']['f1'] for name in model_names])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add classification results\n",
    "for name, result in results.items():\n",
    "    comprehensive_results['classification_results'][name] = {\n",
    "    'accuracy': float(result['metrics']['accuracy']),\n",
    "    'balanced_accuracy': float(result['metrics']['balanced_accuracy']),\n",
    "    'precision': float(result['metrics']['precision']),\n",
    "    'recall': float(result['metrics']['recall']),\n",
    "    'f1_score': float(result['metrics']['f1']),\n",
    "    'f1_weighted': float(result['metrics']['f1_weighted']),\n",
    "    'training_time': float(result['metrics']['train_time']),\n",
    "    'prediction_time': float(result['metrics']['pred_time'])\n",
    "    }\n",
    "\n",
    "# Add neural network results\n",
    "for name, result in nn_results.items():\n",
    "    comprehensive_results['neural_network_results'][name] = {\n",
    "    'architecture': result['architecture'],\n",
    "    'n_parameters': int(result['n_params']),\n",
    "    'n_iterations': int(result['n_iter']),\n",
    "    'accuracy': float(result['metrics']['accuracy']),\n",
    "    'f1_score': float(result['metrics']['f1']),\n",
    "    'training_time': float(result['metrics']['train_time'])\n",
    "    }\n",
    "\n",
    "# Save results to multiple formats\n",
    "results_dir = Path(\"./comprehensive_ai_results\")\n",
    "results_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "json_path = results_dir / \"comprehensive_results.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(comprehensive_results, f, indent=2)\n",
    "\n",
    "# Save classification results as CSV\n",
    "classification_df = pd.DataFrame(comprehensive_results['classification_results']).T\n",
    "csv_path = results_dir / \"classification_results.csv\"\n",
    "classification_df.to_csv(csv_path)\n",
    "\n",
    "# Save neural network results as CSV\n",
    "nn_df = pd.DataFrame(comprehensive_results['neural_network_results']).T\n",
    "nn_csv_path = results_dir / \"neural_network_results.csv\"\n",
    "nn_df.to_csv(nn_csv_path)\n",
    "\n",
    "# Create final summary report\n",
    "summary_path = results_dir / \"executive_summary.md\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"# SciTeX AI Module - Comprehensive Analysis Report\\n\\n\")\n",
    "    f.write(f\"**Analysis Date:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Executive Summary\\n\\n\")\n",
    "    f.write(f\"This report presents a comprehensive analysis of machine learning models using the SciTeX AI module. \")\n",
    "    f.write(f\"We evaluated {len(model_names)} classification algorithms, {len(nn_results)} neural network architectures, \")\n",
    "    f.write(f\"and performed clustering analysis on a {X.shape[0]}-sample dataset with {X.shape[1]} features.\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Key Findings\\n\\n\")\n",
    "    f.write(f\"### ðŸ† Best Performing Models\\n\")\n",
    "    f.write(f\"1. **Overall Winner:** {ranked_names[0]} (Score: {ranked_scores[0]:.3f})\\n\")\n",
    "    f.write(f\"2. **Highest Accuracy:** {max(model_names, key=lambda x: results[x]['metrics']['accuracy'])} \")\n",
    "    f.write(f\"({max([results[name]['metrics']['accuracy'] for name in model_names]):.3f})\\n\")\n",
    "    f.write(f\"3. **Fastest Training:** {min(model_names, key=lambda x: results[x]['metrics']['train_time'])} \")\n",
    "    f.write(f\"({min([results[name]['metrics']['train_time'] for name in model_names]):.3f}s)\\n\\n\")\n",
    "    \n",
    "    f.write(f\"### ðŸ§  Neural Networks\\n\")\n",
    "    best_nn_name = max(nn_results.keys(), key=lambda x: nn_results[x]['metrics']['accuracy'])\n",
    "    f.write(f\"- **Best Architecture:** {best_nn_name} - {nn_results[best_nn_name]['architecture']}\\n\")\n",
    "    f.write(f\"- **Best NN Accuracy:** {nn_results[best_nn_name]['metrics']['accuracy']:.3f}\\n\")\n",
    "    f.write(f\"- **Parameters:** {nn_results[best_nn_name]['n_params']:,}\\n\")\n",
    "    f.write(f\"- **Training Epochs:** {nn_results[best_nn_name]['n_iter']}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"### ðŸ§® Clustering Analysis\\n\")\n",
    "    f.write(f\"- **Optimal Clusters:** {best_k} (Silhouette Score: {best_silhouette:.3f})\\n\")\n",
    "    f.write(f\"- **PCA Variance Explained:** {pca.explained_variance_ratio_[:2].sum():.3f}\\n\")\n",
    "    f.write(f\"- **Clustering Accuracy (ARI):** {ari:.3f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Dataset Information\\n\\n\")\n",
    "    f.write(f\"- **Total Samples:** {X.shape[0]:,}\\n\")\n",
    "    f.write(f\"- **Features:** {X.shape[1]}\\n\")\n",
    "    f.write(f\"- **Classes:** {len(class_names)} ({', '.join(class_names)})\\n\")\n",
    "    f.write(f\"- **Train/Test Split:** {X_train.shape[0]}/{X_test.shape[0]}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Performance Metrics\\n\\n\")\n",
    "    f.write(\"### Classification Models\\n\\n\")\n",
    "    f.write(\"| Model | Accuracy | F1-Score | Training Time |\\n\")\n",
    "    f.write(\"|-------|----------|----------|---------------|\\n\")\n",
    "    for name in ranked_names:\n",
    "        metrics = results[name]['metrics']\n",
    "        f.write(f\"| {name} | {metrics['accuracy']:.3f} | {metrics['f1']:.3f} | {metrics['train_time']:.3f}s |\\n\")\n",
    "    \n",
    "    f.write(\"\\n### Neural Networks\\n\\n\")\n",
    "    f.write(\"| Architecture | Accuracy | Parameters | Epochs |\\n\")\n",
    "    f.write(\"|--------------|----------|------------|--------|\\n\")\n",
    "    for name, result in nn_results.items():\n",
    "        f.write(f\"| {name} | {result['metrics']['accuracy']:.3f} | {result['n_params']:,} | {result['n_iter']} |\\n\")\n",
    "    \n",
    "    f.write(\"\\n## Methodology\\n\\n\")\n",
    "    f.write(\"1. **Data Preparation:** Synthetic multi-class classification dataset\\n\")\n",
    "    f.write(\"2. **Model Training:** Comprehensive evaluation of 7 algorithms\\n\")\n",
    "    f.write(\"3. **Neural Networks:** 4 different architectures with early stopping\\n\")\n",
    "    f.write(\"4. **Clustering:** K-means with silhouette analysis\\n\")\n",
    "    f.write(\"5. **Dimensionality Reduction:** PCA analysis\\n\")\n",
    "    f.write(\"6. **Evaluation:** Multiple metrics including accuracy, F1-score, training time\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Files Generated\\n\\n\")\n",
    "    f.write(f\"- `comprehensive_results.json` - Complete analysis results\\n\")\n",
    "    f.write(f\"- `classification_results.csv` - Classification model comparison\\n\")\n",
    "    f.write(f\"- `neural_network_results.csv` - Neural network analysis\\n\")\n",
    "    f.write(f\"- `executive_summary.md` - This summary report\\n\")\n",
    "    f.write(f\"- `../figures/` - All visualization plots\\n\\n\")\n",
    "    \n",
    "    f.write(\"---\\n\")\n",
    "    f.write(\"*Generated by SciTeX AI Module Comprehensive Tutorial*\\n\")\n",
    "\n",
    "\n",
    "for file_path in results_dir.glob(\"*\"):\n",
    "    # Process file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final performance summary and recommendations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "capabilities = [\n",
    "    \"âœ… Unified ML interface across multiple algorithms\",\n",
    "    \"âœ… Comprehensive performance evaluation and reporting\", \n",
    "    \"âœ… Neural network training with early stopping\",\n",
    "    \"âœ… Clustering analysis with optimal k detection\",\n",
    "    \"âœ… Dimensionality reduction and visualization\",\n",
    "    \"âœ… Feature importance and selection analysis\",\n",
    "    \"âœ… Hyperparameter optimization workflows\",\n",
    "    \"âœ… Advanced model comparison and ranking\",\n",
    "    \"âœ… Automated results export and reporting\",\n",
    "    \"âœ… Production-ready model evaluation framework\"\n",
    "]\n",
    "\n",
    "for capability in capabilities:\n",
    "    # Process capability\n",
    "\n",
    "    2. Try custom datasets with domain-specific features\")\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}