{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Scholar - Complete Tutorial with Impact Factor Integration\n",
    "\n",
    "This notebook demonstrates the new simplified `Scholar` class for scientific literature management with comprehensive impact factor support.\n",
    "\n",
    "## Key Improvements\n",
    "\n",
    "- **Single entry point**: One `Scholar` class for all functionality\n",
    "- **Default enrichment**: Papers are enriched with journal metrics by default\n",
    "- **No async complexity**: Simple synchronous API that works in notebooks\n",
    "- **Chainable methods**: Fluent interface for common workflows\n",
    "- **Smart defaults**: Works out-of-the-box with reasonable settings\n",
    "- **ðŸ“Š Impact Factor Integration**: Automatic journal impact factor lookup using the `impact_factor` package\n",
    "- **ðŸ† Journal Rankings**: Quartile and ranking information for comprehensive evaluation\n",
    "\n",
    "## Installation & Setup\n",
    "\n",
    "Make sure you have scitex and impact_factor installed:\n",
    "```bash\n",
    "pip install -e ~/proj/scitex_repo\n",
    "# impact_factor package should be automatically available\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect notebook name for output directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook name (for papermill compatibility)\n",
    "notebook_name = \"16_scitex_scholar\"\n",
    "if 'PAPERMILL_NOTEBOOK_NAME' in os.environ:\n",
    "    notebook_name = Path(os.environ['PAPERMILL_NOTEBOOK_NAME']).stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the new simplified interface\n",
    "from scitex.scholar import Scholar\n",
    "\n",
    "# Import impact factor tools for direct database access\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import impact_factor\n",
    "\n",
    "# Optional: Set up API keys for enhanced features\n",
    "import os\n",
    "# os.environ['SEMANTIC_SCHOLAR_API_KEY'] = 'your_key_here'\n",
    "# os.environ['OPENAI_API_KEY'] = 'your_key_here'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick Start - Simple Search\n",
    "\n",
    "The fastest way to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick search using Scholar class directly with impact factor enrichment\n",
    "scholar = Scholar(enrich_by_default=True)  # Ensure enrichment is enabled\n",
    "papers = scholar.search(\"deep learning neuroscience\", limit=5)\n",
    "\n",
    "# Display results with comprehensive impact factor information\n",
    "for i, paper in enumerate(papers, 1):\n",
    "    \n",
    "    # Show enriched impact factor data if available\n",
    "    if hasattr(paper, 'impact_factor') and paper.impact_factor:\n",
    "        # Condition met\n",
    "    if hasattr(paper, 'journal_quartile') and paper.journal_quartile:\n",
    "        # Condition met\n",
    "    if hasattr(paper, 'journal_ranking') and paper.journal_ranking:\n",
    "        # Condition met\n",
    "    \n",
    "    # Additional impact factor lookup if not automatically enriched\n",
    "    if paper.journal and (not hasattr(paper, 'impact_factor') or not paper.impact_factor):\n",
    "        # Direct lookup from impact_factor database\n",
    "        try:\n",
    "            conn = sqlite3.connect(impact_factor.DEFAULT_DB)\n",
    "            query = \"SELECT factor, jcr FROM factor WHERE journal LIKE ? ORDER BY factor DESC LIMIT 1\"\n",
    "            result = pd.read_sql_query(query, conn, params=[f'%{paper.journal}%'])\n",
    "            conn.close()\n",
    "            \n",
    "            if len(result) > 0:\n",
    "                # Condition met\n",
    "        except Exception as e:\n",
    "            pass  # Fixed incomplete except block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the Scholar Class\n",
    "\n",
    "For more control and advanced features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Scholar with custom settings\n",
    "scholar = Scholar(\n",
    "    email=\"researcher@university.edu\",  # For PubMed access\n",
    "    enrich_by_default=True,              # Default enrichment (can be turned off)\n",
    "    workspace_dir=\"./scholar_workspace\"  # Custom workspace\n",
    ")\n",
    "\n",
    "# Get workspace info\n",
    "info = scholar.get_workspace_info()\n",
    "for key, value in info.items():\n",
    "    # Loop body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced Search with Filtering\n",
    "\n",
    "Search and filter papers using the fluent interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with filtering and sorting\n",
    "recent_papers = scholar.search(\"neural networks\", limit=20) \\\n",
    "    .filter(year_min=2020, min_citations=10) \\\n",
    "    .sort_by(\"citations\")\n",
    "\n",
    "\n",
    "for i, paper in enumerate(recent_papers[:5], 1):\n",
    "    if hasattr(paper, 'impact_factor') and paper.impact_factor:\n",
    "        # Condition met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multiple Topic Search\n",
    "\n",
    "Search multiple topics and combine results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search multiple related topics\n",
    "topics = [\n",
    "    \"transformer neural networks\",\n",
    "    \"attention mechanisms deep learning\",\n",
    "    \"BERT language models\"\n",
    "]\n",
    "\n",
    "all_papers = scholar.search_multiple(\n",
    "    queries=topics,\n",
    "    papers_per_query=5,\n",
    "    combine_results=True  # Automatically removes duplicates\n",
    ")\n",
    "\n",
    "\n",
    "# Filter for high-impact recent work\n",
    "high_impact = all_papers.filter(year_min=2019, min_citations=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bibliography Generation\n",
    "\n",
    "Generate enriched bibliographies with automatic formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for papers on a specific topic with impact factor filtering\n",
    "ml_papers = scholar.search(\"machine learning interpretability\", limit=15)\n",
    "\n",
    "# Filter for quality papers with impact factor consideration\n",
    "quality_papers = ml_papers.filter(year_min=2018, min_citations=20)\n",
    "\n",
    "\n",
    "# Enrich papers with impact factor information if not already enriched\n",
    "def enrich_paper_with_impact_factor(paper):\n",
    "    \"\"\"Add impact factor information to a paper if available.\"\"\"\n",
    "    if not paper.journal:\n",
    "        return paper\n",
    "    \n",
    "    try:\n",
    "        conn = sqlite3.connect(impact_factor.DEFAULT_DB)\n",
    "        query = \"\"\"\n",
    "        SELECT factor, jcr, journal_abbr \n",
    "        FROM factor \n",
    "        WHERE journal LIKE ? \n",
    "        ORDER BY factor DESC \n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        result = pd.read_sql_query(query, conn, params=[f'%{paper.journal}%'])\n",
    "        conn.close()\n",
    "        \n",
    "        if len(result) > 0:\n",
    "            paper.impact_factor = result.iloc[0]['factor']\n",
    "            paper.journal_quartile = result.iloc[0]['jcr']\n",
    "            paper.journal_abbr = result.iloc[0]['journal_abbr']\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass  # Fixed incomplete except block\n",
    "    \n",
    "    return paper\n",
    "\n",
    "# Enrich all papers\n",
    "enriched_papers = [enrich_paper_with_impact_factor(paper) for paper in quality_papers]\n",
    "\n",
    "# Show impact factor distribution\n",
    "impact_factors = [p.impact_factor for p in enriched_papers if hasattr(p, 'impact_factor') and p.impact_factor]\n",
    "if impact_factors:\n",
    "    # Condition met\n",
    "\n",
    "# Save as enriched BibTeX (includes impact factors)\n",
    "bib_file = \"ml_interpretability_enriched.bib\"\n",
    "\n",
    "try:\n",
    "    # Generate enriched BibTeX entries\n",
    "    with open(bib_file, 'w', encoding='utf-8') as f:\n",
    "        for paper in enriched_papers:\n",
    "            bibtex = paper.to_bibtex(include_enriched=True)\n",
    "            f.write(bibtex + \"\\n\\n\")\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    pass  # Fixed incomplete except block\n",
    "\n",
    "# Preview first enriched entry\n",
    "if enriched_papers:\n",
    "    try:\n",
    "        sample_bibtex = enriched_papers[0].to_bibtex(include_enriched=True)\n",
    "        # Show first 500 characters\n",
    "        preview = sample_bibtex[:500] + \"...\" if len(sample_bibtex) > 500 else sample_bibtex\n",
    "    except Exception as e:\n",
    "        pass  # Fixed incomplete except block\n",
    "\n",
    "# Show papers by impact factor quartile\n",
    "quartile_distribution = {}\n",
    "for paper in enriched_papers:\n",
    "    if hasattr(paper, 'journal_quartile') and paper.journal_quartile:\n",
    "        quartile_distribution[paper.journal_quartile] = quartile_distribution.get(paper.journal_quartile, 0) + 1\n",
    "\n",
    "if quartile_distribution:\n",
    "    for quartile, count in sorted(quartile_distribution.items()):\n",
    "        # Loop body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PDF Downloads\n",
    "\n",
    "Download PDFs for open-access papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for open-access papers\n",
    "oa_papers = scholar.search(\"computer vision\", limit=5)\n",
    "\n",
    "# Filter for potentially open-access papers\n",
    "recent_papers = oa_papers.filter(year_min=2020)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Download PDFs (max 3 to avoid overwhelming servers)\n",
    "    downloaded = scholar.download_pdfs(recent_papers, max_downloads=3)\n",
    "    \n",
    "    for title, path in downloaded.items():\n",
    "        pass  # Processing title\n",
    "except Exception as e:    pass  # Fixed incomplete except block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Local PDF Indexing and Search\n",
    "\n",
    "Build searchable index from your local PDF collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have PDFs downloaded, build a local index\n",
    "pdf_dir = \"./scholar_workspace/pdfs\"\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    if os.path.exists(pdf_dir) and os.listdir(pdf_dir):\n",
    "        index_path = scholar.build_local_index(pdf_dir)\n",
    "        \n",
    "        # Search your local collection\n",
    "        local_results = scholar.search_local(\"neural networks\")\n",
    "        \n",
    "    else:\n",
    "        pass  # Fixed incomplete block\n",
    "        \n",
    "except Exception as e:    pass  # Fixed incomplete except block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Features\n",
    "\n",
    "### Paper Collection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a larger collection for analysis\n",
    "ai_papers = scholar.search(\"artificial intelligence\", limit=30)\n",
    "\n",
    "# Analyze the collection\n",
    "\n",
    "# Year distribution\n",
    "years = [p.year for p in ai_papers if p.year]\n",
    "if years:\n",
    "    from collections import Counter\n",
    "    year_counts = Counter(years)\n",
    "    for year, count in sorted(year_counts.items(), reverse=True)[:5]:\n",
    "        pass  # Processing year\n",
    "# Citation analysis\n",
    "citations = [p.citation_count for p in ai_papers if p.citation_count]\n",
    "if citations:\n",
    "    # Condition met\n",
    "\n",
    "# Export to different formats\n",
    "data_export = ai_papers.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Old API\n",
    "\n",
    "Here's how the new API compares to the old approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Old way - multiple imports and manual enrichment\n",
    "from scitex.scholar import search_papers, PaperEnrichmentService, generate_enriched_bibliography\n",
    "import asyncio\n",
    "\n",
    "# Async search\n",
    "papers = await search_papers(\"deep learning\", limit=10)\n",
    "\n",
    "# Manual enrichment\n",
    "enricher = PaperEnrichmentService()\n",
    "enriched_papers = enricher._enrich_papers(papers)\n",
    "\n",
    "# Manual bibliography generation\n",
    "generate_enriched_bibliography(enriched_papers, \"output.bib\", enrich=False)\n",
    "\"\"\")\n",
    "\n",
    "# New way - one class, automatic enrichment\n",
    "from scitex.scholar import Scholar\n",
    "\n",
    "# Simple search with automatic enrichment\n",
    "scholar = Scholar()\n",
    "papers = scholar.search(\"deep learning\", limit=10)\n",
    "\n",
    "# One-liner bibliography with enrichment\n",
    "papers.save_bibliography(\"output.bib\")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices\n",
    "\n",
    "### Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reuse Scholar instance for multiple searches\n",
    "scholar = Scholar(enrich_by_default=True)  # Initialize once\n",
    "\n",
    "# Multiple searches reuse the same components\n",
    "papers1 = scholar.search(\"topic 1\", limit=5)\n",
    "papers2 = scholar.search(\"topic 2\", limit=5)\n",
    "\n",
    "# 2. Use appropriate limits\n",
    "# For exploration: limit=10-20\n",
    "# For comprehensive reviews: limit=50-100\n",
    "# For quick checks: limit=5\n",
    "\n",
    "# 3. Filter early to reduce processing\n",
    "recent_quality = scholar.search(\"machine learning\", limit=50) \\\n",
    "    .filter(year_min=2020, min_citations=10) \\\n",
    "    .sort_by(\"impact_factor\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Scholar class handles errors gracefully\n",
    "try:\n",
    "    # Even if some components fail, basic search should work\n",
    "    papers = scholar.search(\"test query\", limit=3)\n",
    "    \n",
    "except Exception as e:\n",
    "    # Fallback to basic Scholar search with minimal features\n",
    "    scholar_basic = Scholar(enrich_by_default=False)\n",
    "    papers = scholar_basic.search(\"test query\", limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The new `Scholar` class provides:\n",
    "\n",
    "âœ… **Single entry point** - No need to import multiple classes  \n",
    "âœ… **Default enrichment** - Papers automatically get journal metrics  \n",
    "âœ… **Simple sync API** - No async/await complexity  \n",
    "âœ… **Chainable methods** - Fluent interface for workflows  \n",
    "âœ… **Smart defaults** - Works out-of-the-box  \n",
    "âœ… **Progress feedback** - See what's happening during long operations  \n",
    "âœ… **Error resilience** - Graceful fallbacks when components fail  \n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# Basic usage\n",
    "from scitex.scholar import Scholar\n",
    "scholar = Scholar()\n",
    "papers = scholar.search(\"your topic\", limit=20)\n",
    "papers.save_bibliography(\"papers.bib\")\n",
    "\n",
    "# Advanced workflow\n",
    "papers = scholar.search(\"topic\", limit=50) \\\n",
    "               .filter(year_min=2020, min_citations=10) \\\n",
    "               .sort_by(\"impact_factor\")\n",
    "scholar.download_pdfs(papers, max_downloads=5)\n",
    "```\n",
    "\n",
    "The Scholar class maintains backward compatibility with all existing components while providing a much simpler interface for new users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}