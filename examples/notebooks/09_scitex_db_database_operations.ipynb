{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Database Operations (db) Module\n",
    "\n",
    "This notebook demonstrates the powerful database utilities provided by the SciTeX `db` module. These utilities are designed for scientific data management with support for:\n",
    "\n",
    "- **SQLite3 Operations**: Lightweight, file-based database operations\n",
    "- **PostgreSQL Operations**: Advanced database functionality for larger datasets\n",
    "- **Data Integrity**: Duplicate detection and removal\n",
    "- **Database Inspection**: Schema analysis and metadata extraction\n",
    "- **Scientific Workflows**: Integration with pandas and numpy\n",
    "\n",
    "The db module provides a unified interface for database operations commonly needed in scientific computing.\n",
    "\n",
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scitex as stx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Configure SciTeX for this notebook\n",
    "stx.repro.fix_seeds(42)\n",
    "print(\"SciTeX Database Operations (db) Module Demonstration\")\n",
    "print(f\"SciTeX version: {stx.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Create temporary directory for database files\n",
    "temp_dir = Path('./temp_db_demo')\n",
    "temp_dir.mkdir(exist_ok=True)\n",
    "print(f\"Working directory: {temp_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Sample Scientific Data\n",
    "\n",
    "Let's create realistic scientific datasets to demonstrate database operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample scientific datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# Experimental subjects data\n",
    "subjects_data = pd.DataFrame({\n",
    "    'subject_id': range(1, 101),\n",
    "    'age': np.random.normal(30, 8, 100).astype(int),\n",
    "    'gender': np.random.choice(['M', 'F'], 100),\n",
    "    'group': np.random.choice(['Control', 'Treatment_A', 'Treatment_B'], 100),\n",
    "    'enrollment_date': pd.date_range('2024-01-01', periods=100, freq='D'),\n",
    "    'baseline_score': np.random.normal(50, 10, 100)\n",
    "})\n",
    "\n",
    "# Experimental measurements data\n",
    "measurements_data = []\n",
    "for subject_id in range(1, 101):\n",
    "    for session in range(1, 6):  # 5 sessions per subject\n",
    "        measurements_data.append({\n",
    "            'measurement_id': len(measurements_data) + 1,\n",
    "            'subject_id': subject_id,\n",
    "            'session': session,\n",
    "            'measurement_date': pd.Timestamp('2024-01-01') + pd.Timedelta(days=subject_id-1+session*7),\n",
    "            'cognitive_score': np.random.normal(50 + session*2, 8),\n",
    "            'reaction_time': np.random.normal(500, 50),\n",
    "            'accuracy': np.random.beta(8, 2)  # Beta distribution for accuracy (0-1)\n",
    "        })\n",
    "\n",
    "measurements_df = pd.DataFrame(measurements_data)\n",
    "\n",
    "# Equipment/instrument data\n",
    "equipment_data = pd.DataFrame({\n",
    "    'equipment_id': ['EEG_001', 'EEG_002', 'EyeTracker_001', 'Stimuli_001', 'Stimuli_002'],\n",
    "    'equipment_type': ['EEG', 'EEG', 'EyeTracker', 'StimuliSystem', 'StimuliSystem'],\n",
    "    'manufacturer': ['NeuroSys', 'NeuroSys', 'EyeTech', 'StimuliCorp', 'StimuliCorp'],\n",
    "    'calibration_date': pd.date_range('2023-12-01', periods=5, freq='30D'),\n",
    "    'status': ['Active', 'Active', 'Maintenance', 'Active', 'Active']\n",
    "})\n",
    "\n",
    "print(\"Generated Sample Datasets:\")\n",
    "print(f\"Subjects: {len(subjects_data)} records\")\n",
    "print(f\"Measurements: {len(measurements_df)} records\")\n",
    "print(f\"Equipment: {len(equipment_data)} records\")\n",
    "\n",
    "print(\"\\nSubjects data sample:\")\n",
    "print(subjects_data.head())\n",
    "print(\"\\nMeasurements data sample:\")\n",
    "print(measurements_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SQLite3 Database Operations\n",
    "\n",
    "Demonstrate SQLite3 operations for lightweight scientific data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SQLite3 database\n",
    "print(\"=== SQLite3 Database Operations ===\")\n",
    "\n",
    "# Create SQLite3 database instance\n",
    "db_path = temp_dir / 'scientific_experiment.db'\n",
    "sqlite_db = stx.db.SQLite3(str(db_path))\n",
    "\n",
    "print(f\"Created SQLite3 database: {db_path}\")\n",
    "print(f\"Database exists: {db_path.exists()}\")\n",
    "\n",
    "# Connect to database\n",
    "sqlite_db.connect()\n",
    "print(\"Connected to SQLite3 database\")\n",
    "\n",
    "# Check database info\n",
    "db_info = sqlite_db.get_info()\n",
    "print(f\"Database info: {db_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables and insert data\n",
    "print(\"=== Creating Tables and Inserting Data ===\")\n",
    "\n",
    "# Create subjects table\n",
    "subjects_schema = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS subjects (\n",
    "    subject_id INTEGER PRIMARY KEY,\n",
    "    age INTEGER,\n",
    "    gender TEXT,\n",
    "    group_name TEXT,\n",
    "    enrollment_date DATE,\n",
    "    baseline_score REAL\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sqlite_db.execute_query(subjects_schema)\n",
    "print(\"Created subjects table\")\n",
    "\n",
    "# Create measurements table\n",
    "measurements_schema = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS measurements (\n",
    "    measurement_id INTEGER PRIMARY KEY,\n",
    "    subject_id INTEGER,\n",
    "    session INTEGER,\n",
    "    measurement_date DATE,\n",
    "    cognitive_score REAL,\n",
    "    reaction_time REAL,\n",
    "    accuracy REAL,\n",
    "    FOREIGN KEY (subject_id) REFERENCES subjects (subject_id)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sqlite_db.execute_query(measurements_schema)\n",
    "print(\"Created measurements table\")\n",
    "\n",
    "# Create equipment table\n",
    "equipment_schema = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS equipment (\n",
    "    equipment_id TEXT PRIMARY KEY,\n",
    "    equipment_type TEXT,\n",
    "    manufacturer TEXT,\n",
    "    calibration_date DATE,\n",
    "    status TEXT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sqlite_db.execute_query(equipment_schema)\n",
    "print(\"Created equipment table\")\n",
    "\n",
    "# Insert data using pandas DataFrame\n",
    "subjects_insert = subjects_data.copy()\n",
    "subjects_insert.columns = ['subject_id', 'age', 'gender', 'group_name', 'enrollment_date', 'baseline_score']\n",
    "sqlite_db.insert_df(subjects_insert, 'subjects')\n",
    "print(f\"Inserted {len(subjects_insert)} subjects\")\n",
    "\n",
    "sqlite_db.insert_df(measurements_df, 'measurements')\n",
    "print(f\"Inserted {len(measurements_df)} measurements\")\n",
    "\n",
    "sqlite_db.insert_df(equipment_data, 'equipment')\n",
    "print(f\"Inserted {len(equipment_data)} equipment records\")\n",
    "\n",
    "# Verify data insertion\n",
    "table_info = sqlite_db.get_table_info()\n",
    "print(f\"\\nDatabase contains {len(table_info)} tables:\")\n",
    "for table, info in table_info.items():\n",
    "    print(f\"  {table}: {info['row_count']} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Database Querying and Analysis\n",
    "\n",
    "Perform scientific analysis using database queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate database querying for scientific analysis\n",
    "print(\"=== Scientific Database Queries ===\")\n",
    "\n",
    "# Query 1: Basic subject statistics by group\n",
    "group_stats_query = \"\"\"\n",
    "SELECT \n",
    "    group_name,\n",
    "    COUNT(*) as subject_count,\n",
    "    AVG(age) as avg_age,\n",
    "    AVG(baseline_score) as avg_baseline,\n",
    "    MIN(baseline_score) as min_baseline,\n",
    "    MAX(baseline_score) as max_baseline\n",
    "FROM subjects \n",
    "GROUP BY group_name\n",
    "ORDER BY avg_baseline DESC\n",
    "\"\"\"\n",
    "\n",
    "group_stats = sqlite_db.query_df(group_stats_query)\n",
    "print(\"Group Statistics:\")\n",
    "print(group_stats.round(2))\n",
    "\n",
    "# Query 2: Longitudinal analysis - improvement over sessions\n",
    "longitudinal_query = \"\"\"\n",
    "SELECT \n",
    "    s.group_name,\n",
    "    m.session,\n",
    "    COUNT(*) as n_subjects,\n",
    "    AVG(m.cognitive_score) as avg_cognitive,\n",
    "    AVG(m.reaction_time) as avg_reaction_time,\n",
    "    AVG(m.accuracy) as avg_accuracy\n",
    "FROM measurements m\n",
    "JOIN subjects s ON m.subject_id = s.subject_id\n",
    "GROUP BY s.group_name, m.session\n",
    "ORDER BY s.group_name, m.session\n",
    "\"\"\"\n",
    "\n",
    "longitudinal_results = sqlite_db.query_df(longitudinal_query)\n",
    "print(\"\\nLongitudinal Analysis (first 10 rows):\")\n",
    "print(longitudinal_results.head(10).round(2))\n",
    "\n",
    "# Query 3: Complex analysis - subjects with improving performance\n",
    "improvement_query = \"\"\"\n",
    "WITH session_scores AS (\n",
    "    SELECT \n",
    "        subject_id,\n",
    "        session,\n",
    "        cognitive_score,\n",
    "        LAG(cognitive_score) OVER (PARTITION BY subject_id ORDER BY session) as prev_score\n",
    "    FROM measurements\n",
    "),\n",
    "improvements AS (\n",
    "    SELECT \n",
    "        subject_id,\n",
    "        COUNT(CASE WHEN cognitive_score > prev_score THEN 1 END) as improvement_count,\n",
    "        COUNT(*) - 1 as total_comparisons\n",
    "    FROM session_scores\n",
    "    WHERE prev_score IS NOT NULL\n",
    "    GROUP BY subject_id\n",
    ")\n",
    "SELECT \n",
    "    s.group_name,\n",
    "    COUNT(*) as total_subjects,\n",
    "    COUNT(CASE WHEN i.improvement_count >= 3 THEN 1 END) as improving_subjects,\n",
    "    ROUND(100.0 * COUNT(CASE WHEN i.improvement_count >= 3 THEN 1 END) / COUNT(*), 2) as improvement_rate\n",
    "FROM subjects s\n",
    "JOIN improvements i ON s.subject_id = i.subject_id\n",
    "GROUP BY s.group_name\n",
    "\"\"\"\n",
    "\n",
    "improvement_analysis = sqlite_db.query_df(improvement_query)\n",
    "print(\"\\nImprovement Analysis (subjects improving â‰¥3 sessions):\")\n",
    "print(improvement_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Inspection and Metadata\n",
    "\n",
    "Inspect database structure and extract metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate database inspection capabilities\n",
    "print(\"=== Database Inspection and Metadata ===\")\n",
    "\n",
    "# Use the dedicated inspect function\n",
    "db_inspection = stx.db.inspect(sqlite_db)\n",
    "\n",
    "print(\"Database Inspection Results:\")\n",
    "print(f\"Database type: {db_inspection.get('database_type', 'Unknown')}\")\n",
    "print(f\"Total tables: {len(db_inspection.get('tables', {}))}\")\n",
    "\n",
    "# Detailed table inspection\n",
    "for table_name, table_info in db_inspection.get('tables', {}).items():\n",
    "    print(f\"\\nTable: {table_name}\")\n",
    "    print(f\"  Rows: {table_info.get('row_count', 'Unknown')}\")\n",
    "    print(f\"  Columns: {len(table_info.get('columns', []))}\")\n",
    "    \n",
    "    if 'columns' in table_info:\n",
    "        print(\"  Column details:\")\n",
    "        for col in table_info['columns']:\n",
    "            print(f\"    - {col.get('name', 'Unknown')}: {col.get('type', 'Unknown')}\")\n",
    "\n",
    "# Get schema information directly\n",
    "schema_info = sqlite_db.get_schema()\n",
    "print(f\"\\nSchema Information:\")\n",
    "for table, schema in schema_info.items():\n",
    "    print(f\"\\n{table} schema:\")\n",
    "    print(schema)\n",
    "\n",
    "# Check database size and statistics\n",
    "db_stats = sqlite_db.get_database_size()\n",
    "print(f\"\\nDatabase Statistics:\")\n",
    "print(f\"Database file size: {db_stats.get('size_bytes', 'Unknown')} bytes\")\n",
    "print(f\"Page size: {db_stats.get('page_size', 'Unknown')}\")\n",
    "print(f\"Page count: {db_stats.get('page_count', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Duplicate Detection and Data Cleaning\n",
    "\n",
    "Identify and handle duplicate records in scientific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate duplicate detection and removal\n",
    "print(\"=== Duplicate Detection and Data Cleaning ===\")\n",
    "\n",
    "# First, let's intentionally create some duplicates for demonstration\n",
    "duplicate_subjects = subjects_data.iloc[:5].copy()  # Duplicate first 5 subjects\n",
    "duplicate_subjects['subject_id'] = range(101, 106)  # Change IDs to avoid primary key conflicts\n",
    "\n",
    "# Insert duplicate-like data (same demographics, different IDs)\n",
    "duplicate_subjects.columns = ['subject_id', 'age', 'gender', 'group_name', 'enrollment_date', 'baseline_score']\n",
    "sqlite_db.insert_df(duplicate_subjects, 'subjects')\n",
    "print(f\"Inserted {len(duplicate_subjects)} duplicate-like subjects for testing\")\n",
    "\n",
    "# Check for potential duplicates based on multiple columns\n",
    "duplicate_check_query = \"\"\"\n",
    "SELECT \n",
    "    age, gender, group_name, baseline_score,\n",
    "    COUNT(*) as count,\n",
    "    GROUP_CONCAT(subject_id) as subject_ids\n",
    "FROM subjects\n",
    "GROUP BY age, gender, group_name, baseline_score\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "potential_duplicates = sqlite_db.query_df(duplicate_check_query)\n",
    "print(f\"\\nFound {len(potential_duplicates)} potential duplicate groups:\")\n",
    "print(potential_duplicates.head())\n",
    "\n",
    "# Use SciTeX duplicate detection function\n",
    "# Note: This would work on a DataFrame version of the data\n",
    "all_subjects = sqlite_db.query_df(\"SELECT * FROM subjects\")\n",
    "print(f\"\\nTotal subjects before duplicate removal: {len(all_subjects)}\")\n",
    "\n",
    "# Identify duplicates based on multiple columns (excluding ID)\n",
    "duplicate_columns = ['age', 'gender', 'group_name', 'baseline_score']\n",
    "cleaned_subjects = stx.db.delete_duplicates(\n",
    "    all_subjects, \n",
    "    subset=duplicate_columns,\n",
    "    keep='first'\n",
    ")\n",
    "\n",
    "print(f\"Subjects after duplicate removal: {len(cleaned_subjects)}\")\n",
    "print(f\"Removed {len(all_subjects) - len(cleaned_subjects)} duplicate records\")\n",
    "\n",
    "# Show the removed duplicates\n",
    "if len(all_subjects) > len(cleaned_subjects):\n",
    "    print(\"\\nRemoved duplicate records:\")\n",
    "    duplicate_mask = all_subjects.duplicated(subset=duplicate_columns, keep='first')\n",
    "    removed_duplicates = all_subjects[duplicate_mask]\n",
    "    print(removed_duplicates[['subject_id'] + duplicate_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Database Operations\n",
    "\n",
    "Demonstrate advanced database features for scientific workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate advanced database operations\n",
    "print(\"=== Advanced Database Operations ===\")\n",
    "\n",
    "# Batch operations for efficiency\n",
    "print(\"1. Batch Operations:\")\n",
    "\n",
    "# Create a new table for computed results\n",
    "computed_results_schema = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS computed_results (\n",
    "    subject_id INTEGER,\n",
    "    avg_cognitive_score REAL,\n",
    "    avg_reaction_time REAL,\n",
    "    avg_accuracy REAL,\n",
    "    improvement_slope REAL,\n",
    "    PRIMARY KEY (subject_id),\n",
    "    FOREIGN KEY (subject_id) REFERENCES subjects (subject_id)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sqlite_db.execute_query(computed_results_schema)\n",
    "\n",
    "# Batch computation and insertion\n",
    "batch_compute_query = \"\"\"\n",
    "INSERT OR REPLACE INTO computed_results \n",
    "SELECT \n",
    "    subject_id,\n",
    "    AVG(cognitive_score) as avg_cognitive_score,\n",
    "    AVG(reaction_time) as avg_reaction_time,\n",
    "    AVG(accuracy) as avg_accuracy,\n",
    "    -- Simple linear regression slope for improvement\n",
    "    CASE \n",
    "        WHEN COUNT(*) > 1 THEN\n",
    "            (COUNT(*) * SUM(session * cognitive_score) - SUM(session) * SUM(cognitive_score)) /\n",
    "            (COUNT(*) * SUM(session * session) - SUM(session) * SUM(session))\n",
    "        ELSE 0\n",
    "    END as improvement_slope\n",
    "FROM measurements\n",
    "GROUP BY subject_id\n",
    "\"\"\"\n",
    "\n",
    "sqlite_db.execute_query(batch_compute_query)\n",
    "computed_count = sqlite_db.query_df(\"SELECT COUNT(*) as count FROM computed_results\").iloc[0]['count']\n",
    "print(f\"Computed results for {computed_count} subjects\")\n",
    "\n",
    "# Transaction management for data integrity\n",
    "print(\"\\n2. Transaction Management:\")\n",
    "\n",
    "try:\n",
    "    sqlite_db.begin_transaction()\n",
    "    \n",
    "    # Perform multiple related operations\n",
    "    sqlite_db.execute_query(\n",
    "        \"UPDATE subjects SET baseline_score = baseline_score * 1.1 WHERE group_name = 'Treatment_A'\"\n",
    "    )\n",
    "    \n",
    "    # Verify the update\n",
    "    updated_count = sqlite_db.query_df(\n",
    "        \"SELECT COUNT(*) as count FROM subjects WHERE group_name = 'Treatment_A'\"\n",
    "    ).iloc[0]['count']\n",
    "    \n",
    "    sqlite_db.commit_transaction()\n",
    "    print(f\"Successfully updated {updated_count} Treatment_A subjects in transaction\")\n",
    "    \n",
    "except Exception as e:\n",
    "    sqlite_db.rollback_transaction()\n",
    "    print(f\"Transaction failed and rolled back: {e}\")\n",
    "\n",
    "# Index creation for performance\n",
    "print(\"\\n3. Performance Optimization:\")\n",
    "\n",
    "# Create indexes for frequently queried columns\n",
    "indexes = [\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_subjects_group ON subjects(group_name)\",\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_measurements_subject ON measurements(subject_id)\",\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_measurements_session ON measurements(session)\"\n",
    "]\n",
    "\n",
    "for idx_query in indexes:\n",
    "    sqlite_db.execute_query(idx_query)\n",
    "    \n",
    "print(f\"Created {len(indexes)} performance indexes\")\n",
    "\n",
    "# Query performance comparison\n",
    "import time\n",
    "\n",
    "complex_query = \"\"\"\n",
    "SELECT s.group_name, AVG(m.cognitive_score) as avg_score\n",
    "FROM subjects s \n",
    "JOIN measurements m ON s.subject_id = m.subject_id \n",
    "WHERE m.session = 5\n",
    "GROUP BY s.group_name\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "result = sqlite_db.query_df(complex_query)\n",
    "query_time = time.time() - start_time\n",
    "\n",
    "print(f\"Complex query executed in {query_time:.4f} seconds\")\n",
    "print(\"Query result:\")\n",
    "print(result.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Export and Integration\n",
    "\n",
    "Export database results for further analysis and integration with scientific workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate data export and integration\n",
    "print(\"=== Data Export and Integration ===\")\n",
    "\n",
    "# Export complete dataset for external analysis\n",
    "export_query = \"\"\"\n",
    "SELECT \n",
    "    s.subject_id,\n",
    "    s.age,\n",
    "    s.gender,\n",
    "    s.group_name,\n",
    "    s.baseline_score,\n",
    "    cr.avg_cognitive_score,\n",
    "    cr.avg_reaction_time,\n",
    "    cr.avg_accuracy,\n",
    "    cr.improvement_slope\n",
    "FROM subjects s\n",
    "LEFT JOIN computed_results cr ON s.subject_id = cr.subject_id\n",
    "ORDER BY s.subject_id\n",
    "\"\"\"\n",
    "\n",
    "complete_dataset = sqlite_db.query_df(export_query)\n",
    "print(f\"Exported complete dataset: {complete_dataset.shape}\")\n",
    "\n",
    "# Save to multiple formats using SciTeX I/O\n",
    "export_dir = temp_dir / 'exports'\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export to CSV\n",
    "stx.io.save(complete_dataset, export_dir / 'complete_dataset.csv', symlink_from_cwd=True)\n",
    "print(f\"Exported to CSV: {export_dir / 'complete_dataset.csv'}\")\n",
    "\n",
    "# Export to Excel with multiple sheets\n",
    "with pd.ExcelWriter(export_dir / 'scientific_data.xlsx') as writer:\n",
    "    complete_dataset.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    # Export raw measurements for detailed analysis\n",
    "    measurements_export = sqlite_db.query_df(\n",
    "        \"SELECT * FROM measurements ORDER BY subject_id, session\"\n",
    "    )\n",
    "    measurements_export.to_excel(writer, sheet_name='Measurements', index=False)\n",
    "    \n",
    "    # Export group statistics\n",
    "    group_stats.to_excel(writer, sheet_name='GroupStats', index=False)\n",
    "\n",
    "print(f\"Exported to Excel: {export_dir / 'scientific_data.xlsx'}\")\n",
    "\n",
    "# Create analysis-ready datasets\n",
    "print(\"\\n=== Analysis-Ready Datasets ===\")\n",
    "\n",
    "# Dataset for statistical analysis\n",
    "stats_dataset = complete_dataset.dropna()\n",
    "print(f\"Statistical analysis dataset: {stats_dataset.shape} (removed {len(complete_dataset) - len(stats_dataset)} rows with NaN)\")\n",
    "\n",
    "# Dataset for machine learning\n",
    "ml_features = ['age', 'baseline_score', 'avg_cognitive_score', 'avg_reaction_time', 'avg_accuracy']\n",
    "ml_dataset = stats_dataset[ml_features + ['improvement_slope']].copy()\n",
    "ml_dataset = ml_dataset.dropna()\n",
    "\n",
    "print(f\"ML-ready dataset: {ml_dataset.shape}\")\n",
    "print(\"Feature correlation matrix:\")\n",
    "correlation_matrix = ml_dataset.corr().round(3)\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Save ML dataset\n",
    "stx.io.save(ml_dataset, export_dir / 'ml_dataset.csv', symlink_from_cwd=True)\n",
    "print(f\"\\nML dataset saved: {export_dir / 'ml_dataset.csv'}\")\n",
    "\n",
    "# Export database schema for documentation\n",
    "schema_doc = {\n",
    "    'database_info': db_inspection,\n",
    "    'table_schemas': schema_info,\n",
    "    'export_timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'record_counts': {\n",
    "        'subjects': len(sqlite_db.query_df(\"SELECT * FROM subjects\")),\n",
    "        'measurements': len(sqlite_db.query_df(\"SELECT * FROM measurements\")),\n",
    "        'equipment': len(sqlite_db.query_df(\"SELECT * FROM equipment\"))\n",
    "    }\n",
    "}\n",
    "\n",
    "stx.io.save(schema_doc, export_dir / 'database_schema.json', symlink_from_cwd=True)\n",
    "print(f\"Database schema documentation: {export_dir / 'database_schema.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Database Maintenance and Optimization\n",
    "\n",
    "Perform database maintenance tasks for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate database maintenance operations\n",
    "print(\"=== Database Maintenance and Optimization ===\")\n",
    "\n",
    "# Analyze database before optimization\n",
    "print(\"1. Pre-optimization Analysis:\")\n",
    "pre_stats = sqlite_db.get_database_size()\n",
    "print(f\"Database size: {pre_stats.get('size_bytes', 0)} bytes\")\n",
    "print(f\"Page count: {pre_stats.get('page_count', 0)}\")\n",
    "\n",
    "# Vacuum database to reclaim space\n",
    "print(\"\\n2. Database Vacuum:\")\n",
    "sqlite_db.vacuum()\n",
    "print(\"Database vacuum completed\")\n",
    "\n",
    "# Analyze database after optimization\n",
    "post_stats = sqlite_db.get_database_size()\n",
    "print(f\"Database size after vacuum: {post_stats.get('size_bytes', 0)} bytes\")\n",
    "size_reduction = pre_stats.get('size_bytes', 0) - post_stats.get('size_bytes', 0)\n",
    "if size_reduction > 0:\n",
    "    print(f\"Space saved: {size_reduction} bytes\")\n",
    "\n",
    "# Analyze table statistics\n",
    "print(\"\\n3. Table Statistics:\")\n",
    "table_stats = sqlite_db.analyze_tables()\n",
    "for table, stats in table_stats.items():\n",
    "    print(f\"{table}:\")\n",
    "    for stat, value in stats.items():\n",
    "        print(f\"  {stat}: {value}\")\n",
    "\n",
    "# Check for query optimization opportunities\n",
    "print(\"\\n4. Query Performance Analysis:\")\n",
    "\n",
    "# Explain query plan for complex query\n",
    "explain_query = \"EXPLAIN QUERY PLAN \" + complex_query\n",
    "query_plan = sqlite_db.query_df(explain_query)\n",
    "print(\"Query execution plan:\")\n",
    "print(query_plan)\n",
    "\n",
    "# Check index usage\n",
    "index_info = sqlite_db.query_df(\n",
    "    \"SELECT name, sql FROM sqlite_master WHERE type = 'index' AND sql IS NOT NULL\"\n",
    ")\n",
    "print(f\"\\nActive indexes: {len(index_info)}\")\n",
    "for idx, row in index_info.iterrows():\n",
    "    print(f\"  {row['name']}: {row['sql']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete Scientific Database Workflow\n",
    "\n",
    "Demonstrate a complete scientific database workflow from data ingestion to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete scientific database workflow\n",
    "print(\"=== Complete Scientific Database Workflow ===\")\n",
    "\n",
    "class ScientificDatabaseWorkflow:\n",
    "    def __init__(self, db_path):\n",
    "        self.db = stx.db.SQLite3(str(db_path))\n",
    "        self.db.connect()\n",
    "        self.results = {}\n",
    "        \n",
    "    def setup_experiment_database(self):\n",
    "        \"\"\"Initialize database schema for scientific experiment.\"\"\"\n",
    "        print(\"1. Setting up experiment database...\")\n",
    "        \n",
    "        # Create comprehensive schema\n",
    "        schemas = {\n",
    "            'experiments': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS experiments (\n",
    "                    experiment_id TEXT PRIMARY KEY,\n",
    "                    title TEXT,\n",
    "                    description TEXT,\n",
    "                    start_date DATE,\n",
    "                    end_date DATE,\n",
    "                    principal_investigator TEXT,\n",
    "                    status TEXT\n",
    "                )\n",
    "            \"\"\",\n",
    "            'conditions': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS conditions (\n",
    "                    condition_id TEXT PRIMARY KEY,\n",
    "                    experiment_id TEXT,\n",
    "                    condition_name TEXT,\n",
    "                    description TEXT,\n",
    "                    parameters TEXT,\n",
    "                    FOREIGN KEY (experiment_id) REFERENCES experiments (experiment_id)\n",
    "                )\n",
    "            \"\"\",\n",
    "            'data_quality': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS data_quality (\n",
    "                    quality_id INTEGER PRIMARY KEY,\n",
    "                    table_name TEXT,\n",
    "                    check_date DATE,\n",
    "                    total_records INTEGER,\n",
    "                    valid_records INTEGER,\n",
    "                    quality_score REAL,\n",
    "                    issues TEXT\n",
    "                )\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        for table, schema in schemas.items():\n",
    "            self.db.execute_query(schema)\n",
    "            \n",
    "        print(f\"   Created {len(schemas)} additional tables\")\n",
    "        \n",
    "    def perform_quality_checks(self):\n",
    "        \"\"\"Perform comprehensive data quality checks.\"\"\"\n",
    "        print(\"2. Performing data quality checks...\")\n",
    "        \n",
    "        quality_checks = []\n",
    "        \n",
    "        # Check subjects table\n",
    "        subjects_total = self.db.query_df(\"SELECT COUNT(*) as count FROM subjects\").iloc[0]['count']\n",
    "        subjects_valid = self.db.query_df(\n",
    "            \"SELECT COUNT(*) as count FROM subjects WHERE age > 0 AND baseline_score IS NOT NULL\"\n",
    "        ).iloc[0]['count']\n",
    "        \n",
    "        quality_checks.append({\n",
    "            'table_name': 'subjects',\n",
    "            'check_date': pd.Timestamp.now().date(),\n",
    "            'total_records': subjects_total,\n",
    "            'valid_records': subjects_valid,\n",
    "            'quality_score': subjects_valid / subjects_total if subjects_total > 0 else 0,\n",
    "            'issues': 'None' if subjects_valid == subjects_total else 'Invalid age or missing baseline'\n",
    "        })\n",
    "        \n",
    "        # Check measurements table\n",
    "        measurements_total = self.db.query_df(\"SELECT COUNT(*) as count FROM measurements\").iloc[0]['count']\n",
    "        measurements_valid = self.db.query_df(\n",
    "            \"\"\"SELECT COUNT(*) as count FROM measurements \n",
    "               WHERE cognitive_score IS NOT NULL \n",
    "               AND reaction_time > 0 \n",
    "               AND accuracy BETWEEN 0 AND 1\"\"\"\n",
    "        ).iloc[0]['count']\n",
    "        \n",
    "        quality_checks.append({\n",
    "            'table_name': 'measurements',\n",
    "            'check_date': pd.Timestamp.now().date(),\n",
    "            'total_records': measurements_total,\n",
    "            'valid_records': measurements_valid,\n",
    "            'quality_score': measurements_valid / measurements_total if measurements_total > 0 else 0,\n",
    "            'issues': 'None' if measurements_valid == measurements_total else 'Invalid measurements detected'\n",
    "        })\n",
    "        \n",
    "        # Insert quality check results\n",
    "        quality_df = pd.DataFrame(quality_checks)\n",
    "        self.db.insert_df(quality_df, 'data_quality')\n",
    "        \n",
    "        self.results['quality_checks'] = quality_checks\n",
    "        print(f\"   Completed quality checks for {len(quality_checks)} tables\")\n",
    "        \n",
    "        for check in quality_checks:\n",
    "            print(f\"   {check['table_name']}: {check['quality_score']:.2%} quality score\")\n",
    "            \n",
    "    def generate_scientific_reports(self):\n",
    "        \"\"\"Generate comprehensive scientific analysis reports.\"\"\"\n",
    "        print(\"3. Generating scientific reports...\")\n",
    "        \n",
    "        # Efficacy analysis by group\n",
    "        efficacy_query = \"\"\"\n",
    "        SELECT \n",
    "            s.group_name,\n",
    "            COUNT(DISTINCT s.subject_id) as n_subjects,\n",
    "            AVG(s.baseline_score) as baseline_mean,\n",
    "            AVG(cr.avg_cognitive_score) as endpoint_mean,\n",
    "            AVG(cr.avg_cognitive_score - s.baseline_score) as mean_change,\n",
    "            AVG(cr.improvement_slope) as improvement_rate,\n",
    "            COUNT(CASE WHEN cr.improvement_slope > 0 THEN 1 END) as improvers,\n",
    "            ROUND(100.0 * COUNT(CASE WHEN cr.improvement_slope > 0 THEN 1 END) / COUNT(*), 2) as responder_rate\n",
    "        FROM subjects s\n",
    "        JOIN computed_results cr ON s.subject_id = cr.subject_id\n",
    "        GROUP BY s.group_name\n",
    "        ORDER BY mean_change DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        efficacy_results = self.db.query_df(efficacy_query)\n",
    "        self.results['efficacy_analysis'] = efficacy_results\n",
    "        \n",
    "        print(\"   Efficacy Analysis Results:\")\n",
    "        print(efficacy_results.round(2))\n",
    "        \n",
    "        # Safety/tolerability analysis\n",
    "        safety_query = \"\"\"\n",
    "        SELECT \n",
    "            s.group_name,\n",
    "            AVG(cr.avg_reaction_time) as avg_reaction_time,\n",
    "            AVG(cr.avg_accuracy) as avg_accuracy,\n",
    "            COUNT(CASE WHEN cr.avg_accuracy < 0.5 THEN 1 END) as low_accuracy_subjects,\n",
    "            COUNT(CASE WHEN cr.avg_reaction_time > 600 THEN 1 END) as slow_reaction_subjects\n",
    "        FROM subjects s\n",
    "        JOIN computed_results cr ON s.subject_id = cr.subject_id\n",
    "        GROUP BY s.group_name\n",
    "        \"\"\"\n",
    "        \n",
    "        safety_results = self.db.query_df(safety_query)\n",
    "        self.results['safety_analysis'] = safety_results\n",
    "        \n",
    "        print(\"\\n   Safety Analysis Results:\")\n",
    "        print(safety_results.round(3))\n",
    "        \n",
    "    def export_final_report(self):\n",
    "        \"\"\"Export comprehensive final report.\"\"\"\n",
    "        print(\"4. Exporting final report...\")\n",
    "        \n",
    "        report_dir = temp_dir / 'final_report'\n",
    "        report_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Export all results\n",
    "        final_report = {\n",
    "            'experiment_summary': {\n",
    "                'total_subjects': len(self.db.query_df(\"SELECT * FROM subjects\")),\n",
    "                'total_measurements': len(self.db.query_df(\"SELECT * FROM measurements\")),\n",
    "                'analysis_date': pd.Timestamp.now().isoformat()\n",
    "            },\n",
    "            'quality_assessment': self.results['quality_checks'],\n",
    "            'efficacy_results': self.results['efficacy_analysis'].to_dict('records'),\n",
    "            'safety_results': self.results['safety_analysis'].to_dict('records')\n",
    "        }\n",
    "        \n",
    "        # Save comprehensive report\n",
    "        stx.io.save(final_report, report_dir / 'scientific_analysis_report.json', symlink_from_cwd=True)\n",
    "        \n",
    "        # Export datasets for regulatory submission\n",
    "        self.results['efficacy_analysis'].to_csv(report_dir / 'efficacy_results.csv', index=False)\n",
    "        self.results['safety_analysis'].to_csv(report_dir / 'safety_results.csv', index=False)\n",
    "        \n",
    "        print(f\"   Final report exported to: {report_dir}\")\n",
    "        \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up database resources.\"\"\"\n",
    "        self.db.close()\n",
    "        print(\"5. Database resources cleaned up\")\n",
    "        \n",
    "    def run_complete_workflow(self):\n",
    "        \"\"\"Execute complete scientific database workflow.\"\"\"\n",
    "        print(\"Starting complete scientific database workflow...\\n\")\n",
    "        \n",
    "        try:\n",
    "            self.setup_experiment_database()\n",
    "            self.perform_quality_checks()\n",
    "            self.generate_scientific_reports()\n",
    "            self.export_final_report()\n",
    "            \n",
    "            print(\"\\nâœ“ Scientific database workflow completed successfully!\")\n",
    "            return self.results\n",
    "            \n",
    "        finally:\n",
    "            self.cleanup()\n",
    "\n",
    "# Run the complete workflow\n",
    "workflow_db_path = temp_dir / 'workflow_database.db'\n",
    "workflow = ScientificDatabaseWorkflow(workflow_db_path)\n",
    "workflow_results = workflow.run_complete_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Best Practices\n",
    "\n",
    "The SciTeX db module provides comprehensive database operations for scientific computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key db utilities demonstrated\n",
    "summary = {\n",
    "    'Database Management': [\n",
    "        'stx.db.SQLite3() - Lightweight scientific databases',\n",
    "        'stx.db.PostgreSQL() - Advanced database operations',\n",
    "        'Connection management and transactions',\n",
    "        'Schema creation and table management'\n",
    "    ],\n",
    "    'Data Operations': [\n",
    "        'DataFrame integration with insert_df()',\n",
    "        'Complex scientific queries with query_df()',\n",
    "        'Batch operations for efficiency',\n",
    "        'Data type handling and conversion'\n",
    "    ],\n",
    "    'Quality Assurance': [\n",
    "        'stx.db.delete_duplicates() - Duplicate detection',\n",
    "        'Data validation and quality checks',\n",
    "        'Integrity constraints and foreign keys',\n",
    "        'Error handling and rollback'\n",
    "    ],\n",
    "    'Analysis & Reporting': [\n",
    "        'Statistical analysis through SQL',\n",
    "        'Longitudinal data analysis',\n",
    "        'Performance metrics computation',\n",
    "        'Export to multiple formats'\n",
    "    ],\n",
    "    'Maintenance': [\n",
    "        'stx.db.inspect() - Database metadata',\n",
    "        'Performance optimization with indexes',\n",
    "        'Database vacuum and maintenance',\n",
    "        'Query plan analysis'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"SciTeX Database (db) Module - Key Utilities Summary\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "for category, utilities in summary.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for utility in utilities:\n",
    "        print(f\"  â€¢ {utility}\")\n",
    "\n",
    "print(f\"\\n{'='*58}\")\n",
    "print(\"Best Practices:\")\n",
    "print(\"  â€¢ Use SQLite3 for small-to-medium datasets and prototyping\")\n",
    "print(\"  â€¢ Implement proper schema design with foreign keys\")\n",
    "print(\"  â€¢ Regularly check data quality with automated validations\")\n",
    "print(\"  â€¢ Use transactions for data integrity in multi-step operations\")\n",
    "print(\"  â€¢ Create indexes for frequently queried columns\")\n",
    "print(\"  â€¢ Export results in multiple formats for different analyses\")\n",
    "print(\"  â€¢ Maintain comprehensive documentation of database schema\")\n",
    "\n",
    "print(f\"\\nDemo completed successfully! ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files and close database connections\n",
    "import shutil\n",
    "\n",
    "# Close any remaining database connections\n",
    "try:\n",
    "    sqlite_db.close()\n",
    "    print(\"Database connections closed\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Remove temporary directories\n",
    "for temp_path in ['./temp_db_demo']:\n",
    "    if Path(temp_path).exists():\n",
    "        shutil.rmtree(temp_path)\n",
    "        print(f\"Cleaned up: {temp_path}\")\n",
    "\n",
    "print(\"\\nNotebook cleanup completed.\")"
   ]
  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}