{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Context Management\n",
    "\n",
    "This comprehensive notebook demonstrates the SciTeX context module capabilities, covering context management, output suppression, and environment control utilities.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "### Output Control\n",
    "* Output suppression utilities\n",
    "* Quiet operation modes\n",
    "* Context managers for clean execution\n",
    "\n",
    "### Environment Management\n",
    "* Temporary state changes\n",
    "* Clean execution contexts\n",
    "* Resource management\n",
    "\n",
    "### Integration Examples\n",
    "* Scientific computation workflows\n",
    "* Data processing pipelines\n",
    "* Automated analysis systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect notebook name for output directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook name (for papermill compatibility)\n",
    "notebook_name = \"06_scitex_context\"\n",
    "if 'PAPERMILL_NOTEBOOK_NAME' in os.environ:\n",
    "    notebook_name = Path(os.environ['PAPERMILL_NOTEBOOK_NAME']).stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set up example data directory\n",
    "data_dir = Path(\"./context_examples\")\n",
    "data_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Output Suppression\n",
    "\n",
    "### 1.1 Suppress Output Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate basic output suppression\n",
    "\n",
    "# Normal output (visible)\n",
    "\n",
    "# Suppressed output (hidden)\n",
    "\n",
    "with scitex.context.suppress_output():\n",
    "    \n",
    "    # Even function calls that produce output\n",
    "    for i in range(3):\n",
    "        pass  # Loop body\n",
    "def noisy_function():\n",
    "    \"\"\"A function that produces lots of output.\"\"\"\n",
    "    for i in range(5):\n",
    "        pass  # Loop body\n",
    "        time.sleep(0.01)\n",
    "    return \"Function result\"\n",
    "\n",
    "# Run function normally (noisy)\n",
    "result1 = noisy_function()\n",
    "\n",
    "# Run function with suppressed output (quiet)\n",
    "with scitex.context.suppress_output():\n",
    "    result2 = noisy_function()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Quiet Operation Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate quiet operation mode\n",
    "\n",
    "# Define functions with verbose output\n",
    "def verbose_data_processing():\n",
    "    \"\"\"Simulate verbose data processing.\"\"\"\n",
    "    data = np.random.randn(1000, 50)\n",
    "    \n",
    "    normalized_data = (data - np.mean(data)) / np.std(data)\n",
    "    \n",
    "    correlations = np.corrcoef(normalized_data.T)\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eig(correlations)\n",
    "    \n",
    "    return {\n",
    "    'data': normalized_data,\n",
    "    'correlations': correlations,\n",
    "    'eigenvalues': eigenvalues,\n",
    "    'eigenvectors': eigenvectors\n",
    "    }\n",
    "\n",
    "def verbose_model_training():\n",
    "    \"\"\"Simulate verbose model training.\"\"\"\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        loss = 1.0 / (epoch + 1) + 0.1 * np.random.random()\n",
    "        accuracy = 1.0 - loss + 0.05 * np.random.random()\n",
    "        time.sleep(0.01)  # Simulate training time\n",
    "    \n",
    "    return {'final_loss': loss, 'final_accuracy': accuracy}\n",
    "\n",
    "# Test verbose operations\n",
    "data_results = verbose_data_processing()\n",
    "\n",
    "training_results = verbose_model_training()\n",
    "\n",
    "# Test quiet operations using scitex.context.quiet\n",
    "\n",
    "with scitex.context.quiet():\n",
    "    quiet_data_results = verbose_data_processing()\n",
    "\n",
    "with scitex.context.quiet():\n",
    "    quiet_training_results = verbose_model_training()\n",
    "\n",
    "# Verify results are identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Context Management\n",
    "\n",
    "### 2.1 Nested Context Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate nested context managers\n",
    "\n",
    "def multi_level_function():\n",
    "    \"\"\"Function with multiple levels of verbosity.\"\"\"\n",
    "    \n",
    "    def level_2_function():\n",
    "        for i in range(3):\n",
    "            pass  # Loop body\n",
    "        def level_3_function():\n",
    "            for j in range(5):\n",
    "                pass  # Loop body\n",
    "            return \"deep_result\"\n",
    "        \n",
    "        result = level_3_function()\n",
    "        return result\n",
    "    \n",
    "    result = level_2_function()\n",
    "    return result\n",
    "\n",
    "# Test normal execution\n",
    "result1 = multi_level_function()\n",
    "\n",
    "# Test single-level suppression\n",
    "with scitex.context.suppress_output():\n",
    "    result2 = multi_level_function()\n",
    "\n",
    "# Test nested suppression contexts\n",
    "\n",
    "def selective_suppression():\n",
    "    \n",
    "    with scitex.context.quiet():\n",
    "        \n",
    "        # Even more nested\n",
    "        with scitex.context.suppress_output():\n",
    "            for i in range(3):\n",
    "                pass  # Loop body\n",
    "    return \"selective_result\"\n",
    "\n",
    "result3 = selective_suppression()\n",
    "\n",
    "# Test context manager exception handling\n",
    "\n",
    "def function_with_error():\n",
    "    raise ValueError(\"Intentional error for testing\")\n",
    "\n",
    "# Test that context manager properly handles exceptions\n",
    "try:\n",
    "    with scitex.context.suppress_output():\n",
    "        function_with_error()\n",
    "except ValueError as e:\n",
    "    pass  # Exception handled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Warning and Error Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate warning and error suppression\n",
    "\n",
    "# Function that generates warnings\n",
    "def function_with_warnings():\n",
    "    \"\"\"Function that generates various warnings.\"\"\"\n",
    "    \n",
    "    # Generate numpy warnings\n",
    "    \n",
    "    # Division by zero warning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"always\")\n",
    "        result1 = np.array([1, 2, 3, 0]) / np.array([2, 0, 1, 0])  # Will generate warnings\n",
    "    \n",
    "    # Invalid value warning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"always\")\n",
    "        result2 = np.sqrt(np.array([-1, 4, -9, 16]))  # Will generate warnings\n",
    "    \n",
    "    # Overflow warning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"always\")\n",
    "        result3 = np.exp(np.array([700, 800, 900]))  # Will generate warnings\n",
    "    \n",
    "    \n",
    "    return result1, result2, result3\n",
    "\n",
    "# Test with warnings visible\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"always\")  # Show all warnings\n",
    "    results1 = function_with_warnings()\n",
    "\n",
    "# Test with both output and warnings suppressed\n",
    "with scitex.context.suppress_output():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
    "        results2 = function_with_warnings()\n",
    "\n",
    "\n",
    "# Verify results are still computed correctly\n",
    "\n",
    "# Test stderr suppression\n",
    "\n",
    "def function_with_stderr():\n",
    "    \"\"\"Function that writes to stderr.\"\"\"\n",
    "    sys.stderr.write(\"Writing to stderr\\n\")\n",
    "    sys.stderr.write(\"Another stderr message\\n\")\n",
    "    return \"stderr_test_result\"\n",
    "\n",
    "# Normal execution (stderr visible)\n",
    "result_normal = function_with_stderr()\n",
    "\n",
    "# Suppressed execution\n",
    "with scitex.context.suppress_output():\n",
    "    result_suppressed = function_with_stderr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Scientific Computing Applications\n",
    "\n",
    "### 3.1 Clean Data Processing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data processing pipelines\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"A data processor with verbose and quiet modes.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        self.processing_log = []\n",
    "    \n",
    "    def log(self, message):\n",
    "        \"\"\"Log a message if verbose mode is enabled.\"\"\"\n",
    "        self.processing_log.append(message)\n",
    "        if self.verbose:\n",
    "            pass  # Condition handled\n",
    "    \n",
    "    def load_data(self, shape=(1000, 50)):\n",
    "        \"\"\"Load synthetic data.\"\"\"\n",
    "        self.log(f\"Loading data with shape {shape}\")\n",
    "        data = np.random.randn(*shape)\n",
    "        \n",
    "        # Add some structure\n",
    "        data[:, :10] += np.sin(np.linspace(0, 2*np.pi, shape[0]))[:, np.newaxis]\n",
    "        data[:, 10:20] += np.cos(np.linspace(0, 4*np.pi, shape[0]))[:, np.newaxis]\n",
    "        \n",
    "        self.log(f\"Data loaded successfully\")\n",
    "        self.log(f\"Data statistics: mean={np.mean(data):.4f}, std={np.std(data):.4f}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def preprocess_data(self, data):\n",
    "        \"\"\"Preprocess the data.\"\"\"\n",
    "        self.log(\"Starting data preprocessing\")\n",
    "        \n",
    "        # Step 1: Remove outliers\n",
    "        self.log(\"Removing outliers (>3 std)\")\n",
    "        outlier_mask = np.abs(data) > 3 * np.std(data)\n",
    "        data_cleaned = data.copy()\n",
    "        data_cleaned[outlier_mask] = np.nan\n",
    "        outliers_removed = np.sum(outlier_mask)\n",
    "        self.log(f\"Removed {outliers_removed} outliers\")\n",
    "        \n",
    "        # Step 2: Interpolate missing values\n",
    "        self.log(\"Interpolating missing values\")\n",
    "        for col in range(data_cleaned.shape[1]):\n",
    "            mask = ~np.isnan(data_cleaned[:, col])\n",
    "            if np.sum(mask) > 0:\n",
    "                data_cleaned[~mask, col] = np.mean(data_cleaned[mask, col])\n",
    "        \n",
    "        # Step 3: Normalize\n",
    "        self.log(\"Normalizing data (z-score)\")\n",
    "        data_normalized = (data_cleaned - np.mean(data_cleaned, axis=0)) / np.std(data_cleaned, axis=0)\n",
    "        \n",
    "        self.log(\"Preprocessing completed\")\n",
    "        self.log(f\"Final data: mean={np.mean(data_normalized):.6f}, std={np.std(data_normalized):.6f}\")\n",
    "        \n",
    "        return data_normalized\n",
    "    \n",
    "    def analyze_data(self, data):\n",
    "        \"\"\"Analyze the preprocessed data.\"\"\"\n",
    "        self.log(\"Starting data analysis\")\n",
    "        \n",
    "        # Correlation analysis\n",
    "        self.log(\"Computing correlation matrix\")\n",
    "        correlation_matrix = np.corrcoef(data.T)\n",
    "        \n",
    "        # Principal component analysis\n",
    "        self.log(\"Performing PCA\")\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)\n",
    "        \n",
    "        # Sort by eigenvalue magnitude\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        # Compute explained variance\n",
    "        explained_variance = eigenvalues / np.sum(eigenvalues)\n",
    "        cumulative_variance = np.cumsum(explained_variance)\n",
    "        \n",
    "        self.log(f\"First 5 eigenvalues: {eigenvalues[:5]}\")\n",
    "        self.log(f\"Variance explained by first 5 PCs: {explained_variance[:5]}\")\n",
    "        self.log(f\"Cumulative variance (first 10 PCs): {cumulative_variance[9]:.4f}\")\n",
    "        \n",
    "        # Cluster analysis\n",
    "        self.log(\"Performing simple clustering\")\n",
    "        # Simple k-means-like clustering\n",
    "        n_clusters = 3\n",
    "        centroids = data[np.random.choice(data.shape[0], n_clusters, replace=False)]\n",
    "        \n",
    "        distances = np.sqrt(((data[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        cluster_sizes = [np.sum(labels == i) for i in range(n_clusters)]\n",
    "        self.log(f\"Cluster sizes: {cluster_sizes}\")\n",
    "        \n",
    "        self.log(\"Analysis completed\")\n",
    "        \n",
    "        return {\n",
    "        'correlation_matrix': correlation_matrix,\n",
    "        'eigenvalues': eigenvalues,\n",
    "        'eigenvectors': eigenvectors,\n",
    "        'explained_variance': explained_variance,\n",
    "        'cumulative_variance': cumulative_variance,\n",
    "        'cluster_labels': labels,\n",
    "        'cluster_sizes': cluster_sizes\n",
    "        }\n",
    "    \n",
    "    def run_pipeline(self, data_shape=(1000, 50)):\n",
    "        \"\"\"Run the complete data processing pipeline.\"\"\"\n",
    "        self.log(\"=\" * 50)\n",
    "        self.log(\"STARTING DATA PROCESSING PIPELINE\")\n",
    "        self.log(\"=\" * 50)\n",
    "        \n",
    "        # Load data\n",
    "        data = self.load_data(data_shape)\n",
    "        \n",
    "        # Preprocess\n",
    "        processed_data = self.preprocess_data(data)\n",
    "        \n",
    "        # Analyze\n",
    "        analysis_results = self.analyze_data(processed_data)\n",
    "        \n",
    "        self.log(\"=\" * 50)\n",
    "        self.log(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        self.log(\"=\" * 50)\n",
    "        \n",
    "        return {\n",
    "        'raw_data': data,\n",
    "        'processed_data': processed_data,\n",
    "        'analysis': analysis_results,\n",
    "        'log': self.processing_log\n",
    "        }\n",
    "\n",
    "# Test verbose pipeline\n",
    "verbose_processor = DataProcessor(verbose=True)\n",
    "verbose_results = verbose_processor.run_pipeline((500, 20))\n",
    "\n",
    "# Test quiet pipeline using context manager\n",
    "\n",
    "with scitex.context.quiet():\n",
    "    quiet_processor = DataProcessor(verbose=True)  # Still verbose, but output suppressed\n",
    "    quiet_results = quiet_processor.run_pipeline((500, 20))\n",
    "\n",
    "\n",
    "# Compare results\n",
    "\n",
    "# Show log comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Automated Analysis with Clean Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated analysis with clean output\n",
    "\n",
    "class AutomatedAnalyzer:\n",
    "    \"\"\"Automated analyzer that can run in quiet or verbose mode.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.analysis_history = []\n",
    "    \n",
    "    def analyze_dataset(self, dataset_name, data, quiet=False):\n",
    "        \"\"\"Analyze a dataset with optional quiet mode.\"\"\"\n",
    "        \n",
    "        def verbose_analysis():\n",
    "            \n",
    "            \n",
    "            # Basic statistics\n",
    "            \n",
    "            # Distribution analysis\n",
    "            percentiles = [5, 25, 50, 75, 95]\n",
    "            perc_values = np.percentile(data, percentiles)\n",
    "            for p, v in zip(percentiles, perc_values):\n",
    "        pass  # Processing p\n",
    "            # Correlation analysis\n",
    "            if data.ndim > 1 and data.shape[1] > 1:\n",
    "                corr_matrix = np.corrcoef(data.T)\n",
    "                \n",
    "                # Find highest correlations\n",
    "                mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "                correlations = corr_matrix[mask]\n",
    "                high_corr = correlations[np.abs(correlations) > 0.5]\n",
    "                \n",
    "                if len(high_corr) > 0:\n",
    "                    # Condition met\n",
    "            \n",
    "            # Outlier detection\n",
    "            z_scores = np.abs((data - np.mean(data)) / np.std(data))\n",
    "            outliers = z_scores > 3\n",
    "            n_outliers = np.sum(outliers)\n",
    "            outlier_percentage = (n_outliers / data.size) * 100\n",
    "            \n",
    "            \n",
    "            # Trend analysis\n",
    "            if data.ndim == 1 or (data.ndim == 2 and data.shape[1] == 1):\n",
    "                flat_data = data.flatten()\n",
    "                x = np.arange(len(flat_data))\n",
    "                slope, intercept = np.polyfit(x, flat_data, 1)\n",
    "            \n",
    "            \n",
    "            # Return analysis results\n",
    "            results = {\n",
    "                'dataset_name': dataset_name,\n",
    "                'shape': data.shape,\n",
    "                'basic_stats': {\n",
    "                'mean': np.mean(data),\n",
    "                'std': np.std(data),\n",
    "                'min': np.min(data),\n",
    "                'max': np.max(data)\n",
    "                },\n",
    "                'percentiles': dict(zip(percentiles, perc_values)),\n",
    "                'outliers': {\n",
    "                'count': n_outliers,\n",
    "                'percentage': outlier_percentage\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            if data.ndim > 1 and data.shape[1] > 1:\n",
    "                results['correlations'] = {\n",
    "                'matrix_shape': corr_matrix.shape,\n",
    "                'high_correlations': len(high_corr),\n",
    "                'max_correlation': np.max(np.abs(high_corr)) if len(high_corr) > 0 else 0\n",
    "                }\n",
    "            \n",
    "            if data.ndim == 1 or (data.ndim == 2 and data.shape[1] == 1):\n",
    "                results['trend'] = {\n",
    "                'slope': slope,\n",
    "                'direction': 'Increasing' if slope > 0 else 'Decreasing' if slope < 0 else 'Flat'\n",
    "                }\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        # Run analysis with or without output suppression\n",
    "        if quiet:\n",
    "            with scitex.context.suppress_output():\n",
    "                results = verbose_analysis()\n",
    "        else:\n",
    "            results = verbose_analysis()\n",
    "        \n",
    "        # Store in history\n",
    "        self.analysis_history.append(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_analysis(self, datasets, quiet=True):\n",
    "        \"\"\"Perform batch analysis on multiple datasets.\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        for name, data in datasets.items():\n",
    "            if not quiet:\n",
    "                # Condition met\n",
    "            \n",
    "            result = self.analyze_dataset(name, data, quiet=quiet)\n",
    "            results.append(result)\n",
    "            \n",
    "            if not quiet:\n",
    "                # Condition met\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_summary(self):\n",
    "        \"\"\"Generate a summary of all analyses.\"\"\"\n",
    "        if not self.analysis_history:\n",
    "            return\n",
    "        \n",
    "        \n",
    "        # Summary statistics\n",
    "        all_means = [r['basic_stats']['mean'] for r in self.analysis_history]\n",
    "        all_stds = [r['basic_stats']['std'] for r in self.analysis_history]\n",
    "        all_outlier_pcts = [r['outliers']['percentage'] for r in self.analysis_history]\n",
    "        \n",
    "        \n",
    "        # Dataset with highest/lowest variation\n",
    "        max_std_idx = np.argmax(all_stds)\n",
    "        min_std_idx = np.argmin(all_stds)\n",
    "        \n",
    "\n",
    "# Create test datasets\n",
    "test_datasets = {\n",
    "    'random_normal': np.random.randn(1000, 10),\n",
    "    'random_uniform': np.random.uniform(-1, 1, (800, 15)),\n",
    "    'structured_sine': np.sin(np.linspace(0, 4*np.pi, 500)).reshape(-1, 1),\n",
    "    'noisy_trend': np.linspace(0, 10, 1000) + 0.5 * np.random.randn(1000),\n",
    "    'sparse_data': np.zeros((200, 20)),\n",
    "}\n",
    "\n",
    "# Add some structure to sparse data\n",
    "test_datasets['sparse_data'][::10, ::5] = np.random.randn(20, 4)\n",
    "\n",
    "# Create analyzer\n",
    "analyzer = AutomatedAnalyzer()\n",
    "\n",
    "# Test individual analysis (verbose)\n",
    "individual_result = analyzer.analyze_dataset('test_normal', np.random.randn(100, 5), quiet=False)\n",
    "\n",
    "# Test batch analysis (quiet)\n",
    "batch_results = analyzer.batch_analysis(test_datasets, quiet=True)\n",
    "\n",
    "# Generate summary\n",
    "analyzer.generate_summary()\n",
    "\n",
    "# Test mixed mode\n",
    "\n",
    "with scitex.context.quiet():\n",
    "    mixed_result = analyzer.analyze_dataset('mixed_mode', np.random.exponential(2, (300, 8)), quiet=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Performance and Resource Management\n",
    "\n",
    "### 4.1 Performance Comparison with Context Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison with context managers\n",
    "\n",
    "import time\n",
    "\n",
    "def performance_heavy_function(n_iterations=100):\n",
    "    \"\"\"A function that does heavy computation with lots of output.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        pass  # Loop body\n",
    "        data = np.random.randn(100, 100)\n",
    "        \n",
    "        # Matrix operations\n",
    "        eigenvals = np.linalg.eigvals(data @ data.T)\n",
    "        \n",
    "        result = np.sum(eigenvals)\n",
    "        results.append(result)\n",
    "        \n",
    "        if i % 10 == 9:\n",
    "            pass  # Condition handled\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test performance with output\n",
    "start_time = time.time()\n",
    "results_with_output = performance_heavy_function(10)\n",
    "time_with_output = time.time() - start_time\n",
    "\n",
    "# Test performance without output\n",
    "start_time = time.time()\n",
    "with scitex.context.suppress_output():\n",
    "    results_without_output = performance_heavy_function(10)\n",
    "time_without_output = time.time() - start_time\n",
    "\n",
    "# Compare performance\n",
    "if time_with_output > time_without_output:\n",
    "    speedup = time_with_output / time_without_output\n",
    "else:\n",
    "    pass  # Else case\n",
    "\n",
    "# Verify results are identical\n",
    "results_match = np.allclose(results_with_output, results_without_output)\n",
    "\n",
    "# Memory usage test\n",
    "\n",
    "def memory_intensive_function():\n",
    "    \"\"\"Function that creates large objects and prints about them.\"\"\"\n",
    "    arrays = []\n",
    "    \n",
    "    for i in range(20):\n",
    "        pass  # Loop body\n",
    "        size = (i + 1) * 100\n",
    "        arr = np.random.randn(size, size)\n",
    "        arrays.append(arr)\n",
    "        \n",
    "        memory_usage = sum(a.nbytes for a in arrays) / (1024**2)  # MB\n",
    "        \n",
    "        if i % 5 == 4:\n",
    "            pass  # Condition handled\n",
    "    \n",
    "    total_memory = sum(a.nbytes for a in arrays) / (1024**2)\n",
    "    \n",
    "    return arrays\n",
    "\n",
    "# Test memory function with output\n",
    "start_time = time.time()\n",
    "arrays_with_output = memory_intensive_function()\n",
    "time_memory_with = time.time() - start_time\n",
    "\n",
    "# Clean up\n",
    "del arrays_with_output\n",
    "\n",
    "# Test memory function without output\n",
    "start_time = time.time()\n",
    "with scitex.context.suppress_output():\n",
    "    arrays_without_output = memory_intensive_function()\n",
    "time_memory_without = time.time() - start_time\n",
    "\n",
    "# Compare memory test performance\n",
    "memory_speedup = time_memory_with / time_memory_without if time_memory_without > 0 else 1\n",
    "\n",
    "# Clean up\n",
    "del arrays_without_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Resource Management and Context Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource management and context cleanup\n",
    "\n",
    "class ResourceManager:\n",
    "    \"\"\"Demonstrate resource management with context managers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.resources = []\n",
    "        self.resource_counter = 0\n",
    "    \n",
    "    def create_resource(self, name, size_mb=10):\n",
    "        \"\"\"Create a mock resource (large array).\"\"\"\n",
    "        self.resource_counter += 1\n",
    "        resource_id = f\"{name}_{self.resource_counter}\"\n",
    "        \n",
    "        # Create resource (large array)\n",
    "        elements = int(size_mb * 1024 * 1024 / 8)  # 8 bytes per float64\n",
    "        array_size = int(np.sqrt(elements))\n",
    "        resource_data = np.random.randn(array_size, array_size)\n",
    "        \n",
    "        resource = {\n",
    "        'id': resource_id,\n",
    "        'name': name,\n",
    "        'data': resource_data,\n",
    "        'size_mb': resource_data.nbytes / (1024**2),\n",
    "        'created_at': time.time()\n",
    "        }\n",
    "        \n",
    "        self.resources.append(resource)\n",
    "        \n",
    "        return resource\n",
    "    \n",
    "    def cleanup_resources(self):\n",
    "        \"\"\"Clean up all resources.\"\"\"\n",
    "        total_memory = sum(r['size_mb'] for r in self.resources)\n",
    "        count = len(self.resources)\n",
    "        \n",
    "        \n",
    "        for resource in self.resources:\n",
    "            del resource['data']  # Free the large array\n",
    "        \n",
    "        self.resources.clear()\n",
    "    \n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage.\"\"\"\n",
    "        total_mb = sum(r['size_mb'] for r in self.resources)\n",
    "        return total_mb\n",
    "    \n",
    "    def resource_intensive_operation(self, n_resources=5):\n",
    "        \"\"\"Perform a resource-intensive operation.\"\"\"\n",
    "        \n",
    "        for i in range(n_resources):\n",
    "            resource = self.create_resource(f\"data_array\", size_mb=20)\n",
    "            \n",
    "            # Simulate processing\n",
    "            \n",
    "            # Some computation\n",
    "            mean_value = np.mean(resource['data'])\n",
    "            std_value = np.std(resource['data'])\n",
    "            \n",
    "            \n",
    "            if i % 2 == 1:\n",
    "                pass  # Condition handled\n",
    "        \n",
    "        final_memory = self.get_memory_usage()\n",
    "        \n",
    "        return final_memory\n",
    "\n",
    "# Test resource management with output\n",
    "manager1 = ResourceManager()\n",
    "memory_used_1 = manager1.resource_intensive_operation(3)\n",
    "manager1.cleanup_resources()\n",
    "\n",
    "# Test resource management without output\n",
    "manager2 = ResourceManager()\n",
    "\n",
    "with scitex.context.suppress_output():\n",
    "    memory_used_2 = manager2.resource_intensive_operation(3)\n",
    "\n",
    "\n",
    "# Show current state\n",
    "\n",
    "# Clean up with output\n",
    "manager2.cleanup_resources()\n",
    "\n",
    "# Test context manager exception handling with resources\n",
    "\n",
    "class SafeResourceManager(ResourceManager):\n",
    "    \"\"\"Resource manager with automatic cleanup on exceptions.\"\"\"\n",
    "    \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if exc_type is not None:\n",
    "            pass  # Condition handled\n",
    "        else:\n",
    "            pass  # Else case\n",
    "        \n",
    "        self.cleanup_resources()\n",
    "        \n",
    "        # Don't suppress the exception\n",
    "        return False\n",
    "\n",
    "# Test normal operation\n",
    "with SafeResourceManager() as safe_manager:\n",
    "    safe_manager.create_resource(\"test_resource\", 5)\n",
    "    safe_manager.create_resource(\"another_resource\", 5)\n",
    "\n",
    "# Test exception handling\n",
    "try:\n",
    "    with SafeResourceManager() as safe_manager:\n",
    "        safe_manager.create_resource(\"test_resource\", 5)\n",
    "        \n",
    "        # Cause an intentional exception\n",
    "        raise ValueError(\"Intentional error for testing\")\n",
    "        \n",
    "except ValueError as e:\n",
    "    pass  # Exception handled\n",
    "\n",
    "# Test nested context managers\n",
    "\n",
    "with SafeResourceManager() as outer_manager:\n",
    "    outer_manager.create_resource(\"outer_resource\", 10)\n",
    "    \n",
    "    with scitex.context.suppress_output():\n",
    "        outer_manager.create_resource(\"quiet_resource_1\", 10)\n",
    "        outer_manager.create_resource(\"quiet_resource_2\", 10)\n",
    "        \n",
    "        # This output will be suppressed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This tutorial demonstrated the comprehensive context management capabilities of the SciTeX context module:\n",
    "\n",
    "### Key Features Covered:\n",
    "1. **Output Suppression**: `suppress_output()` for clean execution\n",
    "2. **Quiet Operations**: `quiet()` for silent processing\n",
    "3. **Context Management**: Proper resource handling and cleanup\n",
    "4. **Exception Safety**: Robust error handling with context managers\n",
    "5. **Performance Optimization**: Reduced overhead from suppressed output\n",
    "6. **Nested Contexts**: Complex workflow management\n",
    "7. **Resource Management**: Memory and resource cleanup\n",
    "8. **Scientific Applications**: Clean data processing pipelines\n",
    "\n",
    "### Best Practices:\n",
    "- Use **output suppression** for batch processing and automated workflows\n",
    "- Apply **quiet operations** when running repetitive analyses\n",
    "- Implement **proper exception handling** in context managers\n",
    "- Use **nested contexts** for complex processing pipelines\n",
    "- Apply **resource management** for memory-intensive operations\n",
    "- Use **context managers** for temporary state changes\n",
    "- Implement **clean interfaces** that can operate in silent mode\n",
    "- Consider **performance benefits** of suppressing verbose output\n",
    "\n",
    "### Recommended Workflows:\n",
    "1. **Batch Processing**: Use quiet mode for multiple dataset analysis\n",
    "2. **Automated Pipelines**: Suppress output during production runs\n",
    "3. **Interactive Development**: Use normal mode for debugging, quiet for final runs\n",
    "4. **Resource Management**: Implement context managers for cleanup\n",
    "5. **Performance Optimization**: Profile with and without output suppression\n",
    "\n",
    "### Context Manager Patterns:\n",
    "```python\n",
    "# Basic suppression\n",
    "with scitex.context.suppress_output():\n",
    "    noisy_function()\n",
    "\n",
    "# Quiet operations\n",
    "with scitex.context.quiet():\n",
    "    batch_process_data()\n",
    "\n",
    "# Resource management\n",
    "with ResourceManager() as manager:\n",
    "    manager.process_data()\n",
    "    # Automatic cleanup on exit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup = False  # Set to True to remove example files\n",
    "# Cleanup\n",
    "import shutil\n",
    "\n",
    "# cleanup = \"n\"  # input(\"Clean up example files? (y/n): \").lower().startswith('y')\n",
    "if cleanup:\n",
    "    shutil.rmtree(data_dir)\n",
    "else:\n",
    "    if data_dir.exists():\n",
    "        files = list(data_dir.rglob('*'))\n",
    "        \n",
    "        if files:\n",
    "            total_size = sum(f.stat().st_size for f in files if f.is_file())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}