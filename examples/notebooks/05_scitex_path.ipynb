{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Path Management Utilities\n",
    "\n",
    "This comprehensive notebook demonstrates the SciTeX path module capabilities, covering path manipulation, file system operations, and directory management.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "### Path Operations\n",
    "* Path cleaning and normalization\n",
    "* Directory and file finding\n",
    "* Path splitting and parsing\n",
    "* Current directory utilities\n",
    "\n",
    "### File System Navigation\n",
    "* Git repository detection\n",
    "* Module path resolution\n",
    "* Size calculations\n",
    "* Version management\n",
    "\n",
    "### Advanced Features\n",
    "* Smart path creation\n",
    "* Version incrementing\n",
    "* Latest file detection\n",
    "* Package data access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect notebook name for output directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook name (for papermill compatibility)\n",
    "notebook_name = \"05_scitex_path\"\n",
    "if 'PAPERMILL_NOTEBOOK_NAME' in os.environ:\n",
    "    notebook_name = Path(os.environ['PAPERMILL_NOTEBOOK_NAME']).stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Set up example data directory\n",
    "data_dir = Path(\"./path_examples\")\n",
    "data_dir.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Path Operations\n",
    "\n",
    "### 1.1 Current Path Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current path information\n",
    "\n",
    "try:\n",
    "    # Get this path (current file/notebook location)\n",
    "    this_path = scitex.path.this_path()\n",
    "    \n",
    "    # Alternative method\n",
    "    this_path_alt = scitex.path.get_this_path()\n",
    "    \n",
    "    # Current working directory\n",
    "    current_dir = Path.cwd()\n",
    "    \n",
    "    # Path relationships\n",
    "    if this_path:\n",
    "        pass  # Condition handled\n",
    "    \n",
    "except Exception as e:\n",
    "    pass  # Exception handled\n",
    "\n",
    "# Demonstrate path existence\n",
    "\n",
    "test_paths = [\n",
    "    \"../src\",\n",
    "    \"../src/scitex\",\n",
    "    \"../src/scitex/path\",\n",
    "    \"./path_examples\",\n",
    "    \"./nonexistent_directory\",\n",
    "    \"../requirements.txt\",\n",
    "    \"../nonexistent_file.txt\"\n",
    "]\n",
    "\n",
    "for path_str in test_paths:\n",
    "    path = Path(path_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Path Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path cleaning examples\n",
    "messy_paths = [\n",
    "    \"./data/../data/./file.txt\",\n",
    "    \"/home/user//double//slashes///file.txt\",\n",
    "    \"~/data/./current/./directory/file.txt\",\n",
    "    \"..\\\\windows\\\\style\\\\path\\\\file.txt\",\n",
    "    \"relative/path/with/../redundant/../elements/file.txt\",\n",
    "    \"/absolute/path/with/./current/./references/file.txt\"\n",
    "]\n",
    "\n",
    "\n",
    "for messy_path in messy_paths:\n",
    "    try:\n",
    "        cleaned = scitex.path.clean(messy_path)\n",
    "    except Exception as e:        pass  # Fixed incomplete except block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Path Splitting and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path splitting examples\n",
    "example_paths = [\n",
    "    \"/home/user/documents/project/data/file.csv\",\n",
    "    \"C:\\\\Users\\\\Name\\\\Documents\\\\project\\\\results.xlsx\",\n",
    "    \"../data/experiments/2024/experiment_001.json\",\n",
    "    \"./models/trained_model_v2.pkl\",\n",
    "    \"https://example.com/data/dataset.zip\"\n",
    "]\n",
    "\n",
    "\n",
    "for path_str in example_paths:\n",
    "    try:\n",
    "        split_result = scitex.path.split(path_str)\n",
    "        \n",
    "        # Also show pathlib parsing\n",
    "        path_obj = Path(path_str)\n",
    "    except Exception as e:        pass  # Fixed incomplete except block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: File and Directory Finding\n",
    "\n",
    "### 2.1 Directory and File Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test directory structure\n",
    "test_structure = {\n",
    "    \"project_root\": {\n",
    "    \"src\": {\n",
    "    \"module1\": [\"__init__.py\", \"main.py\", \"utils.py\"],\n",
    "    \"module2\": [\"__init__.py\", \"core.py\", \"helpers.py\"]\n",
    "    },\n",
    "    \"data\": {\n",
    "    \"raw\": [\"dataset1.csv\", \"dataset2.json\"],\n",
    "    \"processed\": [\"clean_data.pkl\", \"features.npy\"]\n",
    "    },\n",
    "    \"tests\": [\"test_module1.py\", \"test_module2.py\"],\n",
    "    \"docs\": [\"README.md\", \"tutorial.md\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_test_structure(base_path, structure):\n",
    "    \"\"\"Create a test directory structure.\"\"\"\n",
    "    for name, content in structure.items():\n",
    "        current_path = base_path / name\n",
    "        current_path.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        if isinstance(content, dict):\n",
    "            create_test_structure(current_path, content)\n",
    "        elif isinstance(content, list):\n",
    "            for filename in content:\n",
    "                file_path = current_path / filename\n",
    "                file_path.write_text(f\"# Content of {filename}\\nprint('Hello from {filename}')\")\n",
    "\n",
    "# Create the test structure\n",
    "test_root = data_dir / \"test_project\"\n",
    "create_test_structure(test_root, test_structure)\n",
    "\n",
    "\n",
    "# Function to print directory tree\n",
    "def print_tree(path, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    items = sorted(path.iterdir()) if path.is_dir() else []\n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"└── \" if is_last else \"├── \"\n",
    "        \n",
    "        if item.is_dir():\n",
    "            next_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "            print_tree(item, next_prefix, max_depth, current_depth + 1)\n",
    "\n",
    "print_tree(test_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory finding\n",
    "\n",
    "directories_to_find = ['src', 'data', 'tests', 'docs', 'nonexistent']\n",
    "\n",
    "for dir_name in directories_to_find:\n",
    "    try:\n",
    "        found_dir = scitex.path.find_dir(dir_name, str(test_root))\n",
    "    except Exception as e:\n",
    "        pass  # Fixed incomplete except block\n",
    "\n",
    "# File finding\n",
    "\n",
    "files_to_find = ['main.py', 'utils.py', 'dataset1.csv', 'README.md', 'nonexistent.txt']\n",
    "\n",
    "for file_name in files_to_find:\n",
    "    try:\n",
    "        found_file = scitex.path.find_file(file_name, str(test_root))\n",
    "    except Exception as e:\n",
    "        pass  # Fixed incomplete except block\n",
    "\n",
    "# Git root finding\n",
    "\n",
    "try:\n",
    "    # Try to find git root from current directory\n",
    "    git_root = scitex.path.find_git_root()\n",
    "    \n",
    "    if git_root:\n",
    "        git_path = Path(git_root)\n",
    "except Exception as e:\n",
    "    pass  # Fixed incomplete except block\n",
    "\n",
    "# Try from test directory (should not find git)\n",
    "try:\n",
    "    git_root_from_test = scitex.path.find_git_root(str(test_root))\n",
    "except Exception as e:    pass  # Fixed incomplete except block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Size Calculations and File Information\n",
    "\n",
    "### 3.1 File and Directory Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files of different sizes for testing\n",
    "size_test_dir = data_dir / \"size_tests\"\n",
    "size_test_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Create files with different content sizes\n",
    "file_contents = {\n",
    "    \"small.txt\": \"Small file content\",\n",
    "    \"medium.txt\": \"Medium file content\\n\" * 100,\n",
    "    \"large.txt\": \"Large file content with lots of text\\n\" * 1000,\n",
    "    \"binary.dat\": bytes(range(256)) * 100,  # Binary content\n",
    "    \"empty.txt\": \"\"\n",
    "}\n",
    "\n",
    "for filename, content in file_contents.items():\n",
    "    filepath = size_test_dir / filename\n",
    "    if isinstance(content, str):\n",
    "        filepath.write_text(content)\n",
    "    else:\n",
    "        filepath.write_bytes(content)\n",
    "\n",
    "\n",
    "for filename in file_contents.keys():\n",
    "    filepath = size_test_dir / filename\n",
    "    \n",
    "    try:\n",
    "        # Using scitex.path.getsize\n",
    "        scitex_size = scitex.path.getsize(str(filepath))\n",
    "        \n",
    "        # Using pathlib for comparison\n",
    "        pathlib_size = filepath.stat().st_size\n",
    "        \n",
    "        # Readable format\n",
    "        readable_size = scitex.str.readable_bytes(scitex_size)\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        pass  # Fixed incomplete except block\n",
    "\n",
    "# Directory size calculation\n",
    "\n",
    "directories_to_analyze = [size_test_dir, test_root, data_dir]\n",
    "\n",
    "for directory in directories_to_analyze:\n",
    "    try:\n",
    "        # Calculate total directory size\n",
    "        total_size = 0\n",
    "        file_count = 0\n",
    "        \n",
    "        for file_path in directory.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                size = scitex.path.getsize(str(file_path))\n",
    "                total_size += size\n",
    "                file_count += 1\n",
    "        \n",
    "        readable_total = scitex.str.readable_bytes(total_size)\n",
    "        \n",
    "        \n",
    "    except Exception as e:        pass  # Fixed incomplete except block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Version Management\n",
    "\n",
    "### 4.1 Version Incrementing and Latest File Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create versioned files for testing\n",
    "version_test_dir = data_dir / \"version_tests\"\n",
    "version_test_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Create files with version numbers\n",
    "versioned_files = [\n",
    "    \"experiment_v1.txt\",\n",
    "    \"experiment_v2.txt\",\n",
    "    \"experiment_v3.txt\",\n",
    "    \"model_v001.pkl\",\n",
    "    \"model_v002.pkl\",\n",
    "    \"model_v010.pkl\",\n",
    "    \"data_1.csv\",\n",
    "    \"data_2.csv\",\n",
    "    \"data_11.csv\",\n",
    "    \"results_2024_01.json\",\n",
    "    \"results_2024_02.json\",\n",
    "    \"results_2024_10.json\"\n",
    "]\n",
    "\n",
    "# Create the files with timestamps to test ordering\n",
    "import time\n",
    "\n",
    "for i, filename in enumerate(versioned_files):\n",
    "    filepath = version_test_dir / filename\n",
    "    filepath.write_text(f\"Content of {filename}\\nVersion: {i+1}\\nCreated: {time.time()}\")\n",
    "    time.sleep(0.01)  # Small delay to ensure different timestamps\n",
    "\n",
    "\n",
    "# List all files with their timestamps\n",
    "for filename in versioned_files:\n",
    "    filepath = version_test_dir / filename\n",
    "    if filepath.exists():\n",
    "        stat = filepath.stat()\n",
    "        mod_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(stat.st_mtime))\n",
    "\n",
    "# Test version incrementing\n",
    "\n",
    "base_names = [\n",
    "    \"experiment_v3.txt\",\n",
    "    \"model_v010.pkl\",\n",
    "    \"data_11.csv\",\n",
    "    \"results_2024_10.json\",\n",
    "    \"new_file.txt\"  # File that doesn't exist yet\n",
    "]\n",
    "\n",
    "for base_name in base_names:\n",
    "    try:\n",
    "        base_path = version_test_dir / base_name\n",
    "        incremented = scitex.path.increment_version(str(base_path))\n",
    "    except Exception as e:\n",
    "        pass  # Fixed incomplete except block\n",
    "\n",
    "# Test finding latest files\n",
    "\n",
    "patterns = [\n",
    "    \"experiment_v*.txt\",\n",
    "    \"model_v*.pkl\",\n",
    "    \"data_*.csv\",\n",
    "    \"results_*.json\"\n",
    "]\n",
    "\n",
    "for pattern in patterns:\n",
    "    try:\n",
    "        latest = scitex.path.find_latest(pattern, str(version_test_dir))\n",
    "    except Exception as e:        pass  # Fixed incomplete except block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Smart Path Creation and Management\n",
    "\n",
    "### 5.1 Smart Path (spath) Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart path creation and management\n",
    "spath_test_dir = data_dir / \"spath_tests\"\n",
    "spath_test_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "# Test smart path creation\n",
    "test_scenarios = [\n",
    "    {\n",
    "    'description': 'Simple file creation',\n",
    "    'base': 'simple_file.txt',\n",
    "    'content': 'Simple content'\n",
    "    },\n",
    "    {\n",
    "    'description': 'File with timestamp',\n",
    "    'base': 'timestamped_file.log',\n",
    "    'content': 'Log entry with timestamp'\n",
    "    },\n",
    "    {\n",
    "    'description': 'Data file with metadata',\n",
    "    'base': 'data_export.csv',\n",
    "    'content': 'col1,col2,col3\\n1,2,3\\n4,5,6'\n",
    "    },\n",
    "    {\n",
    "    'description': 'Configuration file',\n",
    "    'base': 'config.json',\n",
    "    'content': '{\"setting1\": \"value1\", \"setting2\": 42}'\n",
    "    }\n",
    "]\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    try:\n",
    "        \n",
    "        # Create smart path\n",
    "        base_path = spath_test_dir / scenario['base']\n",
    "        spath = scitex.path.mk_spath(str(base_path))\n",
    "        \n",
    "        \n",
    "        # Write content to the smart path\n",
    "        with open(spath, 'w') as f:\n",
    "            f.write(scenario['content'])\n",
    "        \n",
    "        \n",
    "        # Test getting the spath\n",
    "        retrieved_spath = scitex.path.get_spath(str(base_path))\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass  # Fixed incomplete except block\n",
    "\n",
    "# Demonstrate spath behavior with existing files\n",
    "\n",
    "# Create the same file multiple times to see versioning\n",
    "base_name = \"repeated_file.txt\"\n",
    "base_path = spath_test_dir / base_name\n",
    "\n",
    "for i in range(3):\n",
    "    try:\n",
    "        spath = scitex.path.mk_spath(str(base_path))\n",
    "        content = f\"Content version {i+1}\\nCreated at iteration {i+1}\"\n",
    "        \n",
    "        with open(spath, 'w') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        pass  # Fixed incomplete except block\n",
    "\n",
    "# List all files created\n",
    "for file_path in sorted(spath_test_dir.iterdir()):\n",
    "    if file_path.is_file():\n",
    "        size = file_path.stat().st_size\n",
    "        readable_size = scitex.str.readable_bytes(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Package Data and Module Paths\n",
    "\n",
    "### 6.1 Module Path Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module path resolution\n",
    "\n",
    "# Test with various packages and modules\n",
    "modules_to_test = [\n",
    "    'scitex',\n",
    "    'scitex.path',\n",
    "    'scitex.str',\n",
    "    'scitex.gen',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'pathlib',\n",
    "    'os',\n",
    "    'sys'\n",
    "]\n",
    "\n",
    "for module_name in modules_to_test:\n",
    "    try:\n",
    "        # Try to get module path using scitex\n",
    "        module_path = scitex.path.get_data_path_from_a_package(module_name)\n",
    "        \n",
    "        if module_path and Path(module_path).exists():\n",
    "            path_obj = Path(module_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass  # Fixed incomplete except block\n",
    "\n",
    "# Alternative method using importlib for comparison\n",
    "\n",
    "import importlib\n",
    "import importlib.util\n",
    "\n",
    "for module_name in ['scitex', 'numpy', 'pandas']:\n",
    "    try:\n",
    "        # Using importlib\n",
    "        spec = importlib.util.find_spec(module_name)\n",
    "        if spec and spec.origin:\n",
    "            importlib_path = Path(spec.origin).parent\n",
    "        else:\n",
    "            pass  # Fixed incomplete block\n",
    "            \n",
    "        # Using scitex for comparison\n",
    "        scitex_path = scitex.path.get_data_path_from_a_package(module_name)\n",
    "        \n",
    "        # Check if paths match\n",
    "        if spec and spec.origin and scitex_path:\n",
    "            match = str(importlib_path) == str(scitex_path)\n",
    "        \n",
    "    except Exception as e:        pass  # Fixed incomplete except block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Practical Applications\n",
    "\n",
    "### 7.1 Project Organization Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive project organization tool\n",
    "class ProjectOrganizer:\n",
    "    def __init__(self, project_root):\n",
    "        self.project_root = Path(project_root)\n",
    "        self.analysis_results = {}\n",
    "    \n",
    "    def analyze_project_structure(self):\n",
    "        \"\"\"Analyze the project directory structure.\"\"\"\n",
    "        \n",
    "        if not self.project_root.exists():\n",
    "            return\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_files = 0\n",
    "        total_dirs = 0\n",
    "        total_size = 0\n",
    "        file_types = {}\n",
    "        \n",
    "        for item in self.project_root.rglob('*'):\n",
    "            if item.is_file():\n",
    "                total_files += 1\n",
    "                try:\n",
    "                    size = scitex.path.getsize(str(item))\n",
    "                    total_size += size\n",
    "                    \n",
    "                    # Track file types\n",
    "                    suffix = item.suffix.lower() or 'no_extension'\n",
    "                    if suffix not in file_types:\n",
    "                        file_types[suffix] = {'count': 0, 'size': 0}\n",
    "                    file_types[suffix]['count'] += 1\n",
    "                    file_types[suffix]['size'] += size\n",
    "                except:\n",
    "                    pass\n",
    "            elif item.is_dir():\n",
    "                total_dirs += 1\n",
    "        \n",
    "        self.analysis_results = {\n",
    "            'total_files': total_files,\n",
    "            'total_dirs': total_dirs,\n",
    "            'total_size': total_size,\n",
    "            'file_types': file_types\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        \n",
    "        # File type breakdown\n",
    "        for suffix, info in sorted(file_types.items(), key=lambda x: x[1]['size'], reverse=True):\n",
    "            readable_size = scitex.str.readable_bytes(info['size'])\n",
    "    \n",
    "    def find_large_files(self, threshold_mb=1):\n",
    "        \"\"\"Find files larger than threshold.\"\"\"\n",
    "        threshold_bytes = threshold_mb * 1024 * 1024\n",
    "        large_files = []\n",
    "        \n",
    "        \n",
    "        for item in self.project_root.rglob('*'):\n",
    "            if item.is_file():\n",
    "                try:\n",
    "                    size = scitex.path.getsize(str(item))\n",
    "                    if size > threshold_bytes:\n",
    "                        relative_path = item.relative_to(self.project_root)\n",
    "                        readable_size = scitex.str.readable_bytes(size)\n",
    "                        large_files.append((str(relative_path), size, readable_size))\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Sort by size (largest first)\n",
    "        large_files.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if large_files:\n",
    "            for filepath, size, readable_size in large_files:\n",
    "                # Loop body\n",
    "        else:\n",
    "            pass  # Fixed incomplete block\n",
    "        \n",
    "        return large_files\n",
    "    \n",
    "    def find_duplicate_names(self):\n",
    "        \"\"\"Find files with duplicate names (potentially confusing).\"\"\"\n",
    "        name_map = {}\n",
    "        \n",
    "        for item in self.project_root.rglob('*'):\n",
    "            if item.is_file():\n",
    "                name = item.name\n",
    "                if name not in name_map:\n",
    "                    name_map[name] = []\n",
    "                name_map[name].append(item.relative_to(self.project_root))\n",
    "        \n",
    "        duplicates = {name: paths for name, paths in name_map.items() if len(paths) > 1}\n",
    "        \n",
    "        \n",
    "        if duplicates:\n",
    "            for name, paths in duplicates.items():\n",
    "                for path in paths:\n",
    "                    # Process path\n",
    "        else:\n",
    "            pass  # Fixed incomplete block\n",
    "        \n",
    "        return duplicates\n",
    "    \n",
    "    def suggest_cleanup(self):\n",
    "        \"\"\"Suggest cleanup actions.\"\"\"\n",
    "        \n",
    "        suggestions = []\n",
    "        \n",
    "        # Check for common temporary files\n",
    "        temp_patterns = ['*.tmp', '*.temp', '*~', '*.bak', '*.log']\n",
    "        temp_files = []\n",
    "        \n",
    "        for pattern in temp_patterns:\n",
    "            temp_files.extend(self.project_root.rglob(pattern))\n",
    "        \n",
    "        if temp_files:\n",
    "            total_temp_size = sum(scitex.path.getsize(str(f)) for f in temp_files if f.is_file())\n",
    "            suggestions.append(f\"Remove {len(temp_files)} temporary files (saves {scitex.str.readable_bytes(total_temp_size)})\")\n",
    "        \n",
    "        # Check for large files\n",
    "        large_files = self.find_large_files(5)  # Files > 5MB\n",
    "        if large_files:\n",
    "            suggestions.append(f\"Review {len(large_files)} large files (consider compression or archiving)\")\n",
    "        \n",
    "        # Check file type distribution\n",
    "        if self.analysis_results:\n",
    "            file_types = self.analysis_results['file_types']\n",
    "            if '.log' in file_types and file_types['.log']['count'] > 10:\n",
    "                suggestions.append(f\"Archive or clean {file_types['.log']['count']} log files\")\n",
    "        \n",
    "        if suggestions:\n",
    "            for i, suggestion in enumerate(suggestions, 1):\n",
    "        pass  # Processing i\n",
    "        else:\n",
    "            pass  # Fixed incomplete block\n",
    "\n",
    "# Test the project organizer\n",
    "organizer = ProjectOrganizer(test_root)\n",
    "organizer.analyze_project_structure()\n",
    "organizer.find_large_files(0.001)  # Very small threshold for demo\n",
    "organizer.find_duplicate_names()\n",
    "organizer.suggest_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Backup and Version Management System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a backup and version management system\n",
    "class BackupManager:\n",
    "    def __init__(self, source_dir, backup_dir):\n",
    "        self.source_dir = Path(source_dir)\n",
    "        self.backup_dir = Path(backup_dir)\n",
    "        self.backup_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    def create_backup(self, description=\"\"):\n",
    "        \"\"\"Create a timestamped backup.\"\"\"\n",
    "        import datetime\n",
    "        \n",
    "        # Create timestamp\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Create backup name\n",
    "        backup_name = f\"backup_{timestamp}\"\n",
    "        if description:\n",
    "            safe_desc = \"\".join(c for c in description if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "            safe_desc = safe_desc.replace(' ', '_')\n",
    "            backup_name += f\"_{safe_desc}\"\n",
    "        \n",
    "        backup_path = self.backup_dir / backup_name\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Copy directory structure\n",
    "            shutil.copytree(self.source_dir, backup_path)\n",
    "            \n",
    "            # Calculate backup size\n",
    "            backup_size = sum(\n",
    "            scitex.path.getsize(str(f))\n",
    "            for f in backup_path.rglob('*')\n",
    "            if f.is_file()\n",
    "            )\n",
    "            \n",
    "            \n",
    "            return backup_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def list_backups(self):\n",
    "        \"\"\"List all available backups.\"\"\"\n",
    "        backups = []\n",
    "        \n",
    "        for item in self.backup_dir.iterdir():\n",
    "            if item.is_dir() and item.name.startswith('backup_'):\n",
    "                # Get backup info\n",
    "                stat = item.stat()\n",
    "                created = datetime.datetime.fromtimestamp(stat.st_ctime)\n",
    "                \n",
    "                # Calculate size\n",
    "                size = sum(\n",
    "                scitex.path.getsize(str(f))\n",
    "                for f in item.rglob('*')\n",
    "                if f.is_file()\n",
    "                )\n",
    "                \n",
    "                backups.append({\n",
    "                'name': item.name,\n",
    "                'path': item,\n",
    "                'created': created,\n",
    "                'size': size\n",
    "                })\n",
    "        \n",
    "        # Sort by creation time (newest first)\n",
    "        backups.sort(key=lambda x: x['created'], reverse=True)\n",
    "        \n",
    "        \n",
    "        if backups:\n",
    "            for backup in backups:\n",
    "                created_str = backup['created'].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                size_str = scitex.str.readable_bytes(backup['size'])\n",
    "        else:\n",
    "            pass  # Fixed incomplete block\n",
    "        \n",
    "        return backups\n",
    "    \n",
    "    def cleanup_old_backups(self, keep_count=5):\n",
    "        \"\"\"Remove old backups, keeping only the most recent ones.\"\"\"\n",
    "        backups = self.list_backups()\n",
    "        \n",
    "        if len(backups) <= keep_count:\n",
    "            return\n",
    "        \n",
    "        backups_to_remove = backups[keep_count:]\n",
    "        total_freed = 0\n",
    "        \n",
    "        \n",
    "        for backup in backups_to_remove:\n",
    "            try:\n",
    "                shutil.rmtree(backup['path'])\n",
    "                total_freed += backup['size']\n",
    "            except Exception as e:\n",
    "                pass  # Fixed incomplete except block\n",
    "        \n",
    "\n",
    "# Test the backup manager\n",
    "backup_dir = data_dir / \"backups\"\n",
    "backup_manager = BackupManager(test_root, backup_dir)\n",
    "\n",
    "# Create a few backups\n",
    "backup_manager.create_backup(\"initial_state\")\n",
    "time.sleep(1)  # Ensure different timestamps\n",
    "backup_manager.create_backup(\"after_modifications\")\n",
    "time.sleep(1)\n",
    "backup_manager.create_backup(\"final_version\")\n",
    "\n",
    "# List all backups\n",
    "backup_manager.list_backups()\n",
    "\n",
    "# Test cleanup (keep only 2 most recent)\n",
    "backup_manager.cleanup_old_backups(keep_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This tutorial demonstrated the comprehensive path management capabilities of the SciTeX path module:\n",
    "\n",
    "### Key Features Covered:\n",
    "1. **Path Operations**: `this_path()`, `clean()`, `split()` for basic path handling\n",
    "2. **File Finding**: `find_file()`, `find_dir()`, `find_git_root()` for navigation\n",
    "3. **Size Calculations**: `getsize()` for file and directory size analysis\n",
    "4. **Version Management**: `increment_version()`, `find_latest()` for file versioning\n",
    "5. **Smart Paths**: `mk_spath()`, `get_spath()` for intelligent path creation\n",
    "6. **Module Resolution**: `get_data_path_from_a_package()` for package paths\n",
    "7. **Project Organization**: Comprehensive directory analysis and cleanup\n",
    "8. **Backup Management**: Automated backup creation and maintenance\n",
    "\n",
    "### Best Practices:\n",
    "- Use **path cleaning** functions to normalize paths across platforms\n",
    "- Apply **smart path creation** to avoid overwriting important files\n",
    "- Implement **version management** for iterative development\n",
    "- Use **file finding** utilities for robust project navigation\n",
    "- Apply **size analysis** for storage optimization\n",
    "- Create **backup systems** for important project data\n",
    "- Use **git root detection** for repository-aware operations\n",
    "- Implement **project organization** tools for maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup = False\n",
    "# Cleanup\n",
    "import shutil\n",
    "\n",
    "# cleanup = \"n\"  # input(\"Clean up example files? (y/n): \").lower().startswith('y')\n",
    "if cleanup:\n",
    "    shutil.rmtree(data_dir)\n",
    "else:\n",
    "    total_size = sum(f.stat().st_size for f in data_dir.rglob('*') if f.is_file())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}