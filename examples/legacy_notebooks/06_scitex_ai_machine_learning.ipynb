{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX AI - Machine Learning and AI Workflows\n",
    "\n",
    "This notebook demonstrates the AI and machine learning capabilities of SciTeX's AI module.\n",
    "\n",
    "The `scitex.ai` module provides:\n",
    "- Classification workflows with comprehensive reporting\n",
    "- Feature extraction and selection\n",
    "- Model evaluation and validation\n",
    "- Hyperparameter optimization\n",
    "- Neural network utilities\n",
    "- Generative AI integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scitex as stx\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup\n",
    "np.random.seed(42)\n",
    "print(f\"SciTeX version: {stx.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification with SciTeX AI\n",
    "\n",
    "Comprehensive classification workflow with automatic reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=3,\n",
    "    n_classes=3,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create feature names\n",
    "feature_names = [f'feature_{i+1}' for i in range(X.shape[1])]\n",
    "class_names = ['Class_A', 'Class_B', 'Class_C']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SciTeX Classification Reporter\n",
    "reporter = stx.ai.ClassificationReporter(\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "# Train multiple models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Scale features for SVM and Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Training and Evaluating Models:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Use scaled data for SVM and Logistic Regression\n",
    "    if name in ['SVM', 'Logistic Regression']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_proba = model.predict_proba(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    report = reporter.evaluate(y_test, y_pred, y_proba)\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_proba,\n",
    "        'report': report\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {report['accuracy']:.3f}\")\n",
    "    print(f\"  F1-score (macro): {report['f1_macro']:.3f}\")\n",
    "    print(f\"  AUC (macro): {report['auc_macro']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification results\n",
    "fig, axes = stx.plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot confusion matrices\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    ax = axes[0, i]\n",
    "    cm = result['report']['confusion_matrix']\n",
    "    \n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    ax.set_xyt('Predicted Label', 'True Label', f'{name}\\nConfusion Matrix')\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for row in range(cm.shape[0]):\n",
    "        for col in range(cm.shape[1]):\n",
    "            ax.text(col, row, format(cm[row, col], 'd'),\n",
    "                   ha=\"center\", va=\"center\",\n",
    "                   color=\"white\" if cm[row, col] > thresh else \"black\")\n",
    "    \n",
    "    ax.set_xticks(range(len(class_names)))\n",
    "    ax.set_yticks(range(len(class_names)))\n",
    "    ax.set_xticklabels(class_names)\n",
    "    ax.set_yticklabels(class_names)\n",
    "\n",
    "# Plot ROC curves\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    ax = axes[1, i]\n",
    "    \n",
    "    # Plot ROC curve for each class\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    for class_idx in range(len(class_names)):\n",
    "        fpr = result['report']['roc_curves'][class_idx]['fpr']\n",
    "        tpr = result['report']['roc_curves'][class_idx]['tpr']\n",
    "        auc_score = result['report']['roc_curves'][class_idx]['auc']\n",
    "        \n",
    "        ax.plot(fpr, tpr, color=colors[class_idx], \n",
    "                label=f'{class_names[class_idx]} (AUC = {auc_score:.3f})')\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax.set_xyt('False Positive Rate', 'True Positive Rate', \n",
    "               f'{name}\\nROC Curves')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, \"./figures/classification_evaluation.png\", symlink_from_cwd=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Importance and Selection\n",
    "\n",
    "Analyze feature importance and perform feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from Random Forest\n",
    "rf_model = results['Random Forest']['model']\n",
    "feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feature_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(\"=\" * 35)\n",
    "print(feature_df.head(10))\n",
    "\n",
    "# Select top features\n",
    "top_features = feature_df.head(10)['feature'].tolist()\n",
    "top_feature_indices = [feature_names.index(feat) for feat in top_features]\n",
    "\n",
    "print(f\"\\nSelected {len(top_features)} most important features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain models with selected features\n",
    "X_train_selected = X_train[:, top_feature_indices]\n",
    "X_test_selected = X_test[:, top_feature_indices]\n",
    "X_train_selected_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_selected_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "print(\"Performance Comparison: All Features vs Selected Features\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<20} {'All Features':<15} {'Selected Features':<15} {'Difference':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "selected_results = {}\n",
    "\n",
    "for name, model_info in results.items():\n",
    "    # Clone and retrain model with selected features\n",
    "    model = type(model_info['model'])(**model_info['model'].get_params())\n",
    "    \n",
    "    if name in ['SVM', 'Logistic Regression']:\n",
    "        model.fit(X_train_selected_scaled, y_train)\n",
    "        y_pred_selected = model.predict(X_test_selected_scaled)\n",
    "        y_proba_selected = model.predict_proba(X_test_selected_scaled)\n",
    "    else:\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred_selected = model.predict(X_test_selected)\n",
    "        y_proba_selected = model.predict_proba(X_test_selected)\n",
    "    \n",
    "    # Evaluate with selected features\n",
    "    report_selected = reporter.evaluate(y_test, y_pred_selected, y_proba_selected)\n",
    "    \n",
    "    # Compare performance\n",
    "    original_acc = model_info['report']['accuracy']\n",
    "    selected_acc = report_selected['accuracy']\n",
    "    difference = selected_acc - original_acc\n",
    "    \n",
    "    print(f\"{name:<20} {original_acc:<15.3f} {selected_acc:<15.3f} {difference:<+10.3f}\")\n",
    "    \n",
    "    selected_results[name] = {\n",
    "        'model': model,\n",
    "        'report': report_selected\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance and selection results\n",
    "fig, axes = stx.plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Feature importance plot\n",
    "ax1 = axes[0, 0]\n",
    "top_15 = feature_df.head(15)\n",
    "bars = ax1.barh(range(len(top_15)), top_15['importance'], color='skyblue')\n",
    "ax1.set_yticks(range(len(top_15)))\n",
    "ax1.set_yticklabels(top_15['feature'])\n",
    "ax1.set_xyt('Feature Importance', '', 'Top 15 Feature Importance')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Performance comparison\n",
    "ax2 = axes[0, 1]\n",
    "model_names = list(results.keys())\n",
    "all_features_acc = [results[name]['report']['accuracy'] for name in model_names]\n",
    "selected_features_acc = [selected_results[name]['report']['accuracy'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, all_features_acc, width, label='All Features', alpha=0.7)\n",
    "bars2 = ax2.bar(x + width/2, selected_features_acc, width, label='Selected Features', alpha=0.7)\n",
    "\n",
    "ax2.set_xyt('Models', 'Accuracy', 'Performance: All vs Selected Features')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([name.replace(' ', '\\n') for name in model_names])\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature correlation heatmap (top features)\n",
    "ax3 = axes[1, 0]\n",
    "top_feature_data = X[:, top_feature_indices[:10]]  # Top 10 for readability\n",
    "correlation_matrix = np.corrcoef(top_feature_data.T)\n",
    "\n",
    "im = ax3.imshow(correlation_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax3.set_xyt('Features', 'Features', 'Feature Correlation Matrix')\n",
    "ax3.set_xticks(range(10))\n",
    "ax3.set_yticks(range(10))\n",
    "ax3.set_xticklabels([f'F{i+1}' for i in range(10)], rotation=45)\n",
    "ax3.set_yticklabels([f'F{i+1}' for i in range(10)])\n",
    "\n",
    "# Feature distribution by class\n",
    "ax4 = axes[1, 1]\n",
    "# Select the most important feature for visualization\n",
    "most_important_idx = top_feature_indices[0]\n",
    "most_important_data = X[:, most_important_idx]\n",
    "\n",
    "for class_idx in range(len(class_names)):\n",
    "    class_data = most_important_data[y == class_idx]\n",
    "    ax4.hist(class_data, alpha=0.6, label=class_names[class_idx], bins=20)\n",
    "\n",
    "ax4.set_xyt(f'{feature_names[most_important_idx]}', 'Frequency', \n",
    "            'Most Important Feature Distribution')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, \"./figures/feature_analysis.png\", symlink_from_cwd=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Optimization\n",
    "\n",
    "Optimize model hyperparameters using grid search and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, validation_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define parameter grids for optimization\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter Optimization:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "optimized_models = {}\n",
    "\n",
    "for model_name, param_grid in param_grids.items():\n",
    "    print(f\"\\nOptimizing {model_name}...\")\n",
    "    \n",
    "    # Initialize base model\n",
    "    if model_name == 'Random Forest':\n",
    "        base_model = RandomForestClassifier(random_state=42)\n",
    "        X_train_opt = X_train_selected\n",
    "        X_test_opt = X_test_selected\n",
    "    else:  # SVM\n",
    "        base_model = SVC(probability=True, random_state=42)\n",
    "        X_train_opt = X_train_selected_scaled\n",
    "        X_test_opt = X_test_selected_scaled\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        base_model, param_grid, \n",
    "        cv=5, scoring='accuracy', \n",
    "        n_jobs=-1, verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_opt, y_train)\n",
    "    \n",
    "    # Get best model and evaluate\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_opt = best_model.predict(X_test_opt)\n",
    "    y_proba_opt = best_model.predict_proba(X_test_opt)\n",
    "    \n",
    "    # Generate report\n",
    "    report_opt = reporter.evaluate(y_test, y_pred_opt, y_proba_opt)\n",
    "    \n",
    "    optimized_models[model_name] = {\n",
    "        'model': best_model,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'test_accuracy': report_opt['accuracy'],\n",
    "        'report': report_opt\n",
    "    }\n",
    "    \n",
    "    print(f\"  Best CV Score: {grid_search.best_score_:.3f}\")\n",
    "    print(f\"  Test Accuracy: {report_opt['accuracy']:.3f}\")\n",
    "    print(f\"  Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    baseline_acc = selected_results[model_name]['report']['accuracy']\n",
    "    improvement = report_opt['accuracy'] - baseline_acc\n",
    "    print(f\"  Improvement: {improvement:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curves for key hyperparameters\n",
    "fig, axes = stx.plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Random Forest: n_estimators validation curve\n",
    "ax1 = axes[0]\n",
    "n_estimators_range = [10, 50, 100, 150, 200, 250, 300]\n",
    "train_scores, val_scores = validation_curve(\n",
    "    RandomForestClassifier(random_state=42, max_depth=20),\n",
    "    X_train_selected, y_train,\n",
    "    param_name='n_estimators',\n",
    "    param_range=n_estimators_range,\n",
    "    cv=5, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "ax1.plot(n_estimators_range, train_scores_mean, 'o-', color='blue', label='Training score')\n",
    "ax1.fill_between(n_estimators_range, \n",
    "                train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, \n",
    "                alpha=0.1, color='blue')\n",
    "\n",
    "ax1.plot(n_estimators_range, val_scores_mean, 'o-', color='red', label='Cross-validation score')\n",
    "ax1.fill_between(n_estimators_range, \n",
    "                val_scores_mean - val_scores_std,\n",
    "                val_scores_mean + val_scores_std, \n",
    "                alpha=0.1, color='red')\n",
    "\n",
    "ax1.set_xyt('Number of Estimators', 'Accuracy', 'Random Forest: Validation Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# SVM: C parameter validation curve\n",
    "ax2 = axes[1]\n",
    "C_range = [0.01, 0.1, 1, 10, 100]\n",
    "train_scores, val_scores = validation_curve(\n",
    "    SVC(random_state=42, gamma='scale'),\n",
    "    X_train_selected_scaled, y_train,\n",
    "    param_name='C',\n",
    "    param_range=C_range,\n",
    "    cv=5, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "ax2.semilogx(C_range, train_scores_mean, 'o-', color='blue', label='Training score')\n",
    "ax2.fill_between(C_range, \n",
    "                train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, \n",
    "                alpha=0.1, color='blue')\n",
    "\n",
    "ax2.semilogx(C_range, val_scores_mean, 'o-', color='red', label='Cross-validation score')\n",
    "ax2.fill_between(C_range, \n",
    "                val_scores_mean - val_scores_std,\n",
    "                val_scores_mean + val_scores_std, \n",
    "                alpha=0.1, color='red')\n",
    "\n",
    "ax2.set_xyt('C Parameter', 'Accuracy', 'SVM: Validation Curve')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, \"./figures/hyperparameter_optimization.png\", symlink_from_cwd=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network with SciTeX\n",
    "\n",
    "Simple neural network using SciTeX's neural network utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network implementation\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Train neural network with different architectures\n",
    "nn_architectures = {\n",
    "    'Small NN': (50,),\n",
    "    'Medium NN': (100, 50),\n",
    "    'Large NN': (200, 100, 50)\n",
    "}\n",
    "\n",
    "print(\"Neural Network Training:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "nn_results = {}\n",
    "\n",
    "for name, hidden_layers in nn_architectures.items():\n",
    "    print(f\"\\nTraining {name} {hidden_layers}...\")\n",
    "    \n",
    "    # Initialize and train neural network\n",
    "    nn_model = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layers,\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    \n",
    "    nn_model.fit(X_train_selected_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_nn = nn_model.predict(X_test_selected_scaled)\n",
    "    y_proba_nn = nn_model.predict_proba(X_test_selected_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    report_nn = reporter.evaluate(y_test, y_pred_nn, y_proba_nn)\n",
    "    \n",
    "    nn_results[name] = {\n",
    "        'model': nn_model,\n",
    "        'architecture': hidden_layers,\n",
    "        'n_params': sum([layer.size for layer in nn_model.coefs_]) + sum([layer.size for layer in nn_model.intercepts_]),\n",
    "        'n_iter': nn_model.n_iter_,\n",
    "        'loss_curve': nn_model.loss_curve_,\n",
    "        'report': report_nn\n",
    "    }\n",
    "    \n",
    "    print(f\"  Architecture: {hidden_layers}\")\n",
    "    print(f\"  Parameters: {nn_results[name]['n_params']}\")\n",
    "    print(f\"  Iterations: {nn_results[name]['n_iter']}\")\n",
    "    print(f\"  Test Accuracy: {report_nn['accuracy']:.3f}\")\n",
    "    print(f\"  F1-score: {report_nn['f1_macro']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize neural network training and performance\n",
    "fig, axes = stx.plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Loss curves during training\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, (name, result) in enumerate(nn_results.items()):\n",
    "    ax1.plot(result['loss_curve'], color=colors[i], label=name)\n",
    "\n",
    "ax1.set_xyt('Iteration', 'Loss', 'Neural Network Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Performance comparison\n",
    "ax2 = axes[0, 1]\n",
    "nn_names = list(nn_results.keys())\n",
    "nn_accuracies = [nn_results[name]['report']['accuracy'] for name in nn_names]\n",
    "nn_f1_scores = [nn_results[name]['report']['f1_macro'] for name in nn_names]\n",
    "\n",
    "x = np.arange(len(nn_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, nn_accuracies, width, label='Accuracy', alpha=0.7)\n",
    "bars2 = ax2.bar(x + width/2, nn_f1_scores, width, label='F1-score', alpha=0.7)\n",
    "\n",
    "ax2.set_xyt('Neural Network', 'Score', 'Neural Network Performance')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(nn_names)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Model complexity vs performance\n",
    "ax3 = axes[1, 0]\n",
    "nn_params = [nn_results[name]['n_params'] for name in nn_names]\n",
    "\n",
    "ax3.scatter(nn_params, nn_accuracies, s=100, alpha=0.7, color='purple')\n",
    "for i, name in enumerate(nn_names):\n",
    "    ax3.annotate(name, (nn_params[i], nn_accuracies[i]), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "ax3.set_xyt('Number of Parameters', 'Test Accuracy', 'Model Complexity vs Performance')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Final model comparison (all models)\n",
    "ax4 = axes[1, 1]\n",
    "all_model_names = list(optimized_models.keys()) + list(nn_results.keys())\n",
    "all_accuracies = ([optimized_models[name]['test_accuracy'] for name in optimized_models.keys()] +\n",
    "                 [nn_results[name]['report']['accuracy'] for name in nn_results.keys()])\n",
    "\n",
    "colors = ['lightblue', 'lightcoral', 'lightgreen', 'lightyellow', 'lightpink']\n",
    "bars = ax4.bar(range(len(all_model_names)), all_accuracies, \n",
    "               color=colors[:len(all_model_names)], alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, all_accuracies):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax4.set_xyt('Models', 'Test Accuracy', 'Final Model Comparison')\n",
    "ax4.set_xticks(range(len(all_model_names)))\n",
    "ax4.set_xticklabels([name.replace(' ', '\\n') for name in all_model_names], rotation=0)\n",
    "ax4.set_ylim(0.8, max(all_accuracies) + 0.05)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, \"./figures/neural_network_analysis.png\", symlink_from_cwd=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning Curves and Model Diagnostics\n",
    "\n",
    "Analyze learning behavior and diagnose potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curves for best models\n",
    "def plot_learning_curve(model, X, y, title, ax):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, cv=5, n_jobs=-1, \n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    ax.plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training score')\n",
    "    ax.fill_between(train_sizes, \n",
    "                    train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std, \n",
    "                    alpha=0.1, color='blue')\n",
    "    \n",
    "    ax.plot(train_sizes, val_scores_mean, 'o-', color='red', label='Cross-validation score')\n",
    "    ax.fill_between(train_sizes, \n",
    "                    val_scores_mean - val_scores_std,\n",
    "                    val_scores_mean + val_scores_std, \n",
    "                    alpha=0.1, color='red')\n",
    "    \n",
    "    ax.set_xyt('Training Set Size', 'Accuracy', title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return train_sizes, train_scores_mean, val_scores_mean\n",
    "\n",
    "# Plot learning curves for best models\n",
    "fig, axes = stx.plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Random Forest learning curve\n",
    "rf_model = optimized_models['Random Forest']['model']\n",
    "plot_learning_curve(rf_model, X_train_selected, y_train, \n",
    "                   'Random Forest Learning Curve', axes[0, 0])\n",
    "\n",
    "# SVM learning curve\n",
    "svm_model = optimized_models['SVM']['model']\n",
    "plot_learning_curve(svm_model, X_train_selected_scaled, y_train, \n",
    "                   'SVM Learning Curve', axes[0, 1])\n",
    "\n",
    "# Neural Network learning curve\n",
    "best_nn = nn_results['Medium NN']['model']\n",
    "plot_learning_curve(best_nn, X_train_selected_scaled, y_train, \n",
    "                   'Neural Network Learning Curve', axes[1, 0])\n",
    "\n",
    "# Bias-Variance Analysis\n",
    "ax = axes[1, 1]\n",
    "# Simulate bias-variance tradeoff\n",
    "complexities = [1, 2, 5, 10, 20, 50, 100]\n",
    "bias_errors = [0.15, 0.12, 0.08, 0.05, 0.03, 0.02, 0.015]  # Decreasing with complexity\n",
    "variance_errors = [0.01, 0.015, 0.025, 0.04, 0.08, 0.15, 0.25]  # Increasing with complexity\n",
    "total_errors = [b + v for b, v in zip(bias_errors, variance_errors)]\n",
    "\n",
    "ax.plot(complexities, bias_errors, 'o-', label='BiasÂ²', color='blue')\n",
    "ax.plot(complexities, variance_errors, 's-', label='Variance', color='red')\n",
    "ax.plot(complexities, total_errors, '^-', label='Total Error', color='green', linewidth=2)\n",
    "\n",
    "ax.set_xyt('Model Complexity', 'Error', 'Bias-Variance Tradeoff')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, \"./figures/learning_curves_diagnostics.png\", symlink_from_cwd=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Interpretability and Explainability\n",
    "\n",
    "Understand what the models have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from different models\n",
    "print(\"Model Interpretability Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_importance = optimized_models['Random Forest']['model'].feature_importances_\n",
    "\n",
    "# Logistic Regression coefficients (using all features for comparison)\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_selected_scaled, y_train)\n",
    "lr_coefs = np.abs(lr_model.coef_).mean(axis=0)  # Average absolute coefficients across classes\n",
    "\n",
    "# Create feature importance comparison\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': [feature_names[i] for i in top_feature_indices],\n",
    "    'rf_importance': rf_importance,\n",
    "    'lr_importance': lr_coefs / lr_coefs.max()  # Normalize\n",
    "})\n",
    "\n",
    "print(\"\\nFeature Importance Comparison:\")\n",
    "print(importance_df.sort_values('rf_importance', ascending=False).head(10))\n",
    "\n",
    "# Prediction confidence analysis\n",
    "best_model = optimized_models['Random Forest']['model']\n",
    "y_proba_best = best_model.predict_proba(X_test_selected)\n",
    "prediction_confidence = np.max(y_proba_best, axis=1)\n",
    "predictions = np.argmax(y_proba_best, axis=1)\n",
    "correct_predictions = (predictions == y_test)\n",
    "\n",
    "print(f\"\\nPrediction Confidence Analysis:\")\n",
    "print(f\"Mean confidence: {np.mean(prediction_confidence):.3f}\")\n",
    "print(f\"Confidence for correct predictions: {np.mean(prediction_confidence[correct_predictions]):.3f}\")\n",
    "print(f\"Confidence for incorrect predictions: {np.mean(prediction_confidence[~correct_predictions]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model interpretability\n",
    "fig, axes = stx.plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Feature importance comparison\n",
    "ax1 = axes[0, 0]\n",
    "x = np.arange(len(importance_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, importance_df['rf_importance'], width, \n",
    "                label='Random Forest', alpha=0.7)\n",
    "bars2 = ax1.bar(x + width/2, importance_df['lr_importance'], width, \n",
    "                label='Logistic Regression', alpha=0.7)\n",
    "\n",
    "ax1.set_xyt('Features', 'Importance', 'Feature Importance Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f'F{i+1}' for i in range(len(importance_df))], rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction confidence distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(prediction_confidence[correct_predictions], alpha=0.7, \n",
    "         label='Correct Predictions', bins=20, color='green')\n",
    "ax2.hist(prediction_confidence[~correct_predictions], alpha=0.7, \n",
    "         label='Incorrect Predictions', bins=20, color='red')\n",
    "\n",
    "ax2.set_xyt('Prediction Confidence', 'Frequency', 'Confidence Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Class-wise prediction confidence\n",
    "ax3 = axes[1, 0]\n",
    "for class_idx in range(len(class_names)):\n",
    "    class_mask = (y_test == class_idx)\n",
    "    class_confidence = prediction_confidence[class_mask]\n",
    "    ax3.hist(class_confidence, alpha=0.6, label=class_names[class_idx], bins=15)\n",
    "\n",
    "ax3.set_xyt('Prediction Confidence', 'Frequency', 'Confidence by True Class')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundary visualization (2D projection)\n",
    "ax4 = axes[1, 1]\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Project to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_test_2d = pca.fit_transform(X_test_selected)\n",
    "\n",
    "# Plot test points colored by true class\n",
    "colors = ['blue', 'red', 'green']\n",
    "for class_idx in range(len(class_names)):\n",
    "    mask = (y_test == class_idx)\n",
    "    ax4.scatter(X_test_2d[mask, 0], X_test_2d[mask, 1], \n",
    "               c=colors[class_idx], label=class_names[class_idx], \n",
    "               alpha=0.6, s=30)\n",
    "\n",
    "# Mark misclassified points\n",
    "misclassified = ~correct_predictions\n",
    "ax4.scatter(X_test_2d[misclassified, 0], X_test_2d[misclassified, 1], \n",
    "           marker='x', s=100, c='black', label='Misclassified')\n",
    "\n",
    "ax4.set_xyt('First Principal Component', 'Second Principal Component', \n",
    "            'Test Set Visualization (PCA)')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, \"./figures/model_interpretability.png\", symlink_from_cwd=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the comprehensive machine learning capabilities of SciTeX's AI module:\n",
    "\n",
    "### Key Features Covered:\n",
    "\n",
    "1. **Classification Workflows**: Complete pipeline with SciTeX ClassificationReporter\n",
    "2. **Feature Analysis**: Importance ranking, selection, and correlation analysis\n",
    "3. **Hyperparameter Optimization**: Grid search and validation curves\n",
    "4. **Neural Networks**: Multiple architectures with training analysis\n",
    "5. **Learning Diagnostics**: Learning curves and bias-variance analysis\n",
    "6. **Model Interpretability**: Feature importance and prediction confidence\n",
    "\n",
    "### SciTeX AI Advantages:\n",
    "\n",
    "- **Automated Reporting**: Comprehensive evaluation with ROC curves, confusion matrices\n",
    "- **Feature Management**: Built-in feature selection and importance analysis\n",
    "- **Visualization Integration**: Seamless plotting with data export\n",
    "- **Best Practices**: Proper validation, scaling, and diagnostic workflows\n",
    "- **Extensibility**: Easy integration with scikit-learn and other ML libraries\n",
    "\n",
    "### Best Practices Demonstrated:\n",
    "\n",
    "- Always split data properly and use cross-validation\n",
    "- Scale features for algorithms that need it (SVM, Neural Networks)\n",
    "- Perform feature selection to reduce overfitting\n",
    "- Optimize hyperparameters systematically\n",
    "- Analyze learning curves to diagnose bias/variance\n",
    "- Evaluate model confidence and interpretability\n",
    "\n",
    "### Next Steps:\n",
    "- Explore `scitex.nn` for advanced neural network architectures\n",
    "- Use `scitex.dsp` for feature extraction from signal data\n",
    "- Check `scitex.stats` for statistical validation of ML results\n",
    "- Try `scitex.ai.genai` for LLM integration in ML workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive ML results\n",
    "ml_results = {\n",
    "    'dataset_info': {\n",
    "        'n_samples': len(X),\n",
    "        'n_features': X.shape[1],\n",
    "        'n_classes': len(class_names),\n",
    "        'class_names': class_names,\n",
    "        'feature_names': feature_names\n",
    "    },\n",
    "    'feature_selection': {\n",
    "        'selected_features': top_features,\n",
    "        'feature_importance': feature_df.to_dict('records')\n",
    "    },\n",
    "    'model_performance': {\n",
    "        'baseline': {name: result['report']['accuracy'] for name, result in results.items()},\n",
    "        'optimized': {name: result['test_accuracy'] for name, result in optimized_models.items()},\n",
    "        'neural_networks': {name: result['report']['accuracy'] for name, result in nn_results.items()}\n",
    "    },\n",
    "    'best_models': {\n",
    "        'random_forest': {\n",
    "            'params': optimized_models['Random Forest']['best_params'],\n",
    "            'accuracy': optimized_models['Random Forest']['test_accuracy']\n",
    "        },\n",
    "        'svm': {\n",
    "            'params': optimized_models['SVM']['best_params'],\n",
    "            'accuracy': optimized_models['SVM']['test_accuracy']\n",
    "        }\n",
    "    },\n",
    "    'interpretability': {\n",
    "        'feature_importance_comparison': importance_df.to_dict('records'),\n",
    "        'prediction_confidence': {\n",
    "            'mean_confidence': float(np.mean(prediction_confidence)),\n",
    "            'correct_confidence': float(np.mean(prediction_confidence[correct_predictions])),\n",
    "            'incorrect_confidence': float(np.mean(prediction_confidence[~correct_predictions]))\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "stx.io.save(ml_results, \"./data/ml_analysis_results.json\", symlink_from_cwd=True)\n",
    "stx.io.save(X_test_selected, \"./data/test_features.npy\", symlink_from_cwd=True)\n",
    "stx.io.save(y_test, \"./data/test_labels.npy\", symlink_from_cwd=True)\n",
    "\n",
    "print(\"\\nâœ… Machine Learning analysis complete!\")\n",
    "print(\"ðŸ¤– Results saved to ./data/ml_analysis_results.json\")\n",
    "print(\"ðŸ“ˆ Figures saved to ./figures/\")\n",
    "print(\"ðŸ”¬ Test data saved for further analysis\")\n",
    "\n",
    "# Final summary\n",
    "best_accuracy = max([result['test_accuracy'] for result in optimized_models.values()] + \n",
    "                   [result['report']['accuracy'] for result in nn_results.values()])\n",
    "print(f\"\\nðŸ† Best model accuracy: {best_accuracy:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}